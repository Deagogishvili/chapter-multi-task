{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78bf4c1f-c19c-4092-9cae-5900ea389bb0",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "557b895b-8550-4a87-8c4b-3babe4a9daf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os \n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421b26ee-61ac-4e95-9909-19e2d883c000",
   "metadata": {},
   "source": [
    "# Define paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f65dbce-5daa-4254-9567-18563412ea40",
   "metadata": {},
   "source": [
    "Google drive = https://drive.google.com/drive/folders/1NcerEtJUn6eULDLdu2l-WPdzvTTw6mFE?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561f1e32-8b46-4324-b41a-5120773d35f8",
   "metadata": {},
   "source": [
    "# Imort data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "91e4dc54-3031-4cce-8aa0-2bc22512c681",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.dirname(os.getcwd()) + '/data/'\n",
    "expression_file=pd.read_csv(data_path + 'expression/expression.csv')\n",
    "expression=dict(zip(list(expression_file.PDB_ID),list(expression_file.Expression)))\n",
    "species=dict(zip(list(expression_file.PDB_ID),list(expression_file.Species)))\n",
    "patches = os.listdir(data_path + 'patches/raw')\n",
    "\n",
    "AA = ['A','C','D','E','F','G','H','I','K','L','M','N','P','Q','R','S','T','V','W','Y']\n",
    "HYDR = ['A','C','F','I','L','M','V','W','Y']\n",
    "SPEC = dict(zip(list(expression_file.Species.value_counts().index[:10]),range(0,10)))\n",
    "EXP = np.percentile(expression_file[~expression_file.Expression.isnull()].Expression, np.arange(0,100,10)).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "27c36a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_match_percentage(list1, list2): \n",
    "    if len(list1) != len(list2):\n",
    "        return 0   \n",
    "    match_count = 0\n",
    "    for aa1, aa2 in zip(list1, list2):\n",
    "        if aa1 == aa2:\n",
    "            match_count += 1\n",
    "    \n",
    "    match_percentage = (match_count / len(list1))\n",
    "    return match_percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "58105d5c-1993-4278-8b55-2a0999bb9b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ASA(data, list_data, agg_data=None):\n",
    "    new_data=np.zeros((len(data),len(data[0]),95))\n",
    "    count = 0\n",
    "    print(new_data.shape, 'initial')\n",
    "    for i in tqdm(range(len(list_data))):\n",
    "        #Column len(AA)\n",
    "        column=np.zeros((len(data[i]),27))\n",
    "        mask = np.count_nonzero(data[i,:,50])\n",
    "        \n",
    "        #Column 0 equals TASA = sum(RSA*ASAmax)\n",
    "        tasa=sum(data[i,:,53]*data[i,:,50])\n",
    "        hydr=[bool(AA[k] in HYDR) for k in np.argmax(data[i,:,:20],axis=-1)]\n",
    "        #Column 1 equals THSA = sum(RSA*ASAmax*hydr_mask)\n",
    "        thsa=sum(data[i,:,53]*data[i,:,50]*hydr)\n",
    "        #Column 2 equals RHSA = sum(RSA*ASAmax*hydr_mask)/sum(RSA*ASAmax)\n",
    "        rhsa=thsa/tasa\n",
    "        column[0,0]=round(tasa,1)\n",
    "        column[0,1]=round(thsa,1)\n",
    "        column[0,2]=round(rhsa,5)\n",
    "\n",
    "        #Column 3 equals Patch Size\n",
    "        id_patch=list_data[i].replace('-','').upper()\n",
    "        if id_patch+'.csv' in patches: \n",
    "            patch_info = pd.read_csv(os.path.join(data_path + 'patches','raw',f'{id_patch}.csv'))\n",
    "                        \n",
    "            data_fasta = \"\".join([AA[k] for k in np.argmax(data[i,:mask,:20],axis=-1)])\n",
    "            \n",
    "            min_index = min(patch_info['pdb_index'])\n",
    "            patch_info['pdb_index'] = patch_info['pdb_index'] - min_index\n",
    "            max_index = max(patch_info['pdb_index'])\n",
    "            pdb_id_AA_dict = dict(zip(patch_info['pdb_index'], patch_info['amino_acid']))\n",
    "            \n",
    "            patch_fasta = \"\".join([pdb_id_AA_dict.get(i, \"_\") for i in range(max_index)])\n",
    "            \n",
    "            s = SequenceMatcher(None, data_fasta, patch_fasta)\n",
    "            matches = s.get_matching_blocks()\n",
    "            if matches[0].a == 0:\n",
    "                patch_fasta = patch_fasta[matches[0].b:]\n",
    "                patch_info['pdb_index'] = patch_info['pdb_index'] + matches[0].b\n",
    "            if matches[0].b == 0:\n",
    "                patch_fasta = \"_\"*matches[0].a + patch_fasta\n",
    "                patch_info['pdb_index'] = patch_info['pdb_index'] + matches[0].a\n",
    "\n",
    "            pdb_id_AA_dict = dict(zip(patch_info['pdb_index'], patch_info['amino_acid']))\n",
    "            patch_fasta = \"\".join([pdb_id_AA_dict.get(i, \"_\") for i in range(len(data_fasta))])\n",
    "\n",
    "            skip_files = list_data[i] in [\"1dts-A\", \"1rlr-A\", \"4pfk-A\", \"1u2z-B\"]\n",
    "            \n",
    "            if calculate_match_percentage(data_fasta, patch_fasta) > 0.95 and not skip_files:                \n",
    "                patch_place = np.zeros((len(data[i])))\n",
    "                patch_place[patch_info['pdb_index']] = patch_info['patch_size']\n",
    "                patch_place[np.isnan(patch_place)] = 0\n",
    "                column[:,4]=np.where(patch_place>0,1,0)\n",
    "                if not((patch_place==0).all()):\n",
    "                    column[0,3]=round(max(patch_place),1)\n",
    "                    column[:,5]=np.where(patch_place==max(patch_place),1,0)\n",
    "                count += 1  \n",
    "            # elif calculate_match_percentage(data_fasta, patch_fasta) > 0.9:      \n",
    "            #     print(data_fasta) \n",
    "            #     print(patch_fasta)\n",
    "            #     print(id_patch)\n",
    "            #     print('*******')\n",
    "\n",
    "        #Column 6 equals Species\n",
    "        if list_data[i] in species.keys() and species[list_data[i]] in SPEC.keys():\n",
    "            column[0,6+SPEC[species[list_data[i]]]]=1\n",
    "\n",
    "        # Columns 17 equals expression\n",
    "        if list_data[i] in expression.keys() and not(np.isnan(expression[list_data[i]])):\n",
    "            expression_value=round(expression[list_data[i]],1)  \n",
    "            column[0,16]=expression_value\n",
    "            rk=0\n",
    "            for k in EXP[1:]:\n",
    "                if expression_value>=k:\n",
    "                    rk+=1\n",
    "            column[0,17+rk]=1    \n",
    "        new_data[i]=np.c_[data[i],column]        \n",
    "    \n",
    "    print(new_data.shape, 'final')\n",
    "    print(count)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6da19142-865b-4123-a7d9-ab37c914fd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10848, 1632, 95) initial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10848/10848 [00:16<00:00, 643.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10848, 1632, 95) final\n",
      "9990\n",
      "(21, 1494, 95) initial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21/21 [00:00<00:00, 686.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21, 1494, 95) final\n",
      "20\n",
      "(513, 874, 95) initial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 513/513 [00:00<00:00, 923.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(513, 874, 95) final\n",
      "470\n",
      "(115, 1111, 95) initial\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 115/115 [00:00<00:00, 841.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115, 1111, 95) final\n",
      "113\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    name_train = np.load(f\"{os.path.dirname(os.getcwd())}/data/source_dataset/Train_HHblits.npz\")['pdbids']\n",
    "    name_casp = np.load(f\"{os.path.dirname(os.getcwd())}/data/source_dataset/CASP12_HHblits.npz\")['pdbids']\n",
    "    name_cb = np.load(f\"{os.path.dirname(os.getcwd())}/data/source_dataset/CB513_HHblits.npz\")['pdbids']\n",
    "    name_ts = np.load(f\"{os.path.dirname(os.getcwd())}/data/source_dataset/TS115_HHblits.npz\")['pdbids']\n",
    "    data_train = np.load(f\"{os.path.dirname(os.getcwd())}/data/source_dataset/Train_HHblits.npz\")['data']\n",
    "    data_casp = np.load(f\"{os.path.dirname(os.getcwd())}/data/source_dataset/CASP12_HHblits.npz\")['data']\n",
    "    data_cb = np.load(f\"{os.path.dirname(os.getcwd())}/data/source_dataset/CB513_HHblits.npz\")['data']\n",
    "    data_ts = np.load(f\"{os.path.dirname(os.getcwd())}/data/source_dataset/TS115_HHblits.npz\")['data']\n",
    "    list_train = get_ASA(data_train,name_train)\n",
    "    list_casp = get_ASA(data_casp,name_casp)\n",
    "    list_cb = get_ASA(data_cb,name_cb)\n",
    "    list_ts = get_ASA(data_ts,name_ts)\n",
    "    np.savez_compressed(f\"{os.path.dirname(os.getcwd())}/data/extended/Train_HHblits_extended.npz\",pdbids=name_train,data=list_train)\n",
    "    np.savez_compressed(f\"{os.path.dirname(os.getcwd())}/data/extended/CASP12_HHblits_extended.npz\",pdbids=name_casp,data=list_casp)\n",
    "    np.savez_compressed(f\"{os.path.dirname(os.getcwd())}/data/extended/CB513_HHblits_extended.npz\",pdbids=name_cb,data=list_cb)\n",
    "    np.savez_compressed(f\"{os.path.dirname(os.getcwd())}/data/extended/TS115_HHblits_extended.npz\",pdbids=name_ts,data=list_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86658eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10848 10848\n",
      "21 21\n",
      "513 434\n",
      "115 115\n"
     ]
    }
   ],
   "source": [
    "print(len(name_train), len(set(name_train)))\n",
    "print(len(name_casp), len(set(name_casp)))\n",
    "print(len(name_cb), len(set(name_cb)))\n",
    "print(len(name_ts), len(set(name_ts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e9ba04-ca12-4112-b47b-604f94641995",
   "metadata": {},
   "source": [
    "# Only keep LHP global and local features and remove the proteins that are not annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e4c7de06-8338-4a52-ae2b-15850975ee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def LHP_only(dataset_path, save_path):\n",
    "    # Load dataset\n",
    "    data = np.load(dataset_path, allow_pickle=True)\n",
    "    pdbids = data['pdbids']\n",
    "    dataset = data['data']\n",
    "    \n",
    "    # Define indices of columns with LHP annotations\n",
    "    lhp_indices = [71, 72, 73]\n",
    "    \n",
    "    # Initialize a list to store filtered data\n",
    "    filtered_data = []\n",
    "    indices_with_lhp = []\n",
    "    \n",
    "    # Iterate through each entry in the dataset\n",
    "    for i in range(len(dataset)):\n",
    "        # Check if any of the LHP columns have annotations\n",
    "        if any(dataset[i, :, idx].any() for idx in lhp_indices):\n",
    "            # If LHP annotation is present, keep the row\n",
    "            filtered_data.append(dataset[i])\n",
    "            indices_with_lhp.append(i)\n",
    "    \n",
    "    # Stack filtered data\n",
    "    filtered_data = np.stack(filtered_data)\n",
    "    \n",
    "    # Save filtered dataset\n",
    "    np.savez_compressed(save_path, pdbids=pdbids[indices_with_lhp], data=filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1cb6bab2-fd39-4662-bb81-ef5b38e4abd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LHP_only(f\"{os.path.dirname(os.getcwd())}/data/extended/Train_HHblits_extended.npz\", \n",
    "                              f\"{os.path.dirname(os.getcwd())}/data/extended/Train_LHP.npz\")\n",
    "LHP_only(f\"{os.path.dirname(os.getcwd())}/data/extended/CASP12_HHblits_extended.npz\", \n",
    "                              f\"{os.path.dirname(os.getcwd())}/data/extended/CASP12_LHP.npz\")\n",
    "LHP_only(f\"{os.path.dirname(os.getcwd())}/data/extended/CB513_HHblits_extended.npz\", \n",
    "                              f\"{os.path.dirname(os.getcwd())}/data/extended/CB513_LHP.npz\")\n",
    "LHP_only(f\"{os.path.dirname(os.getcwd())}/data/extended/TS115_HHblits_extended.npz\", \n",
    "                              f\"{os.path.dirname(os.getcwd())}/data/extended/TS115_LHP.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "82c6add2-5adc-48d7-a2a3-3dbb4c04392f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CASP12_HHblits_extended = np.load(f\"{os.path.dirname(os.getcwd())}/data/extended/CASP12_HHblits_extended.npz\")\n",
    "Train_LHP = np.load(f\"{os.path.dirname(os.getcwd())}/data/extended/Train_LHP.npz\")\n",
    "CASP12_LHP = np.load(f\"{os.path.dirname(os.getcwd())}/data/extended/CASP12_LHP.npz\")\n",
    "CB513_LHP = np.load(f\"{os.path.dirname(os.getcwd())}/data/extended/CB513_LHP.npz\")\n",
    "TS115_LHP = np.load(f\"{os.path.dirname(os.getcwd())}/data/extended/TS115_LHP.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "46c1a993-a7a7-49bf-adfc-d6e363700b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9990, 1632, 95)\n",
      "(20, 1494, 95)\n",
      "(470, 874, 95)\n",
      "(113, 1111, 95)\n"
     ]
    }
   ],
   "source": [
    "print(Train_LHP['data'].shape)\n",
    "print(CASP12_LHP['data'].shape)\n",
    "print(CB513_LHP['data'].shape)\n",
    "print(TS115_LHP['data'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7b5c8baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 1242\n",
      "Y: 3898\n",
      "I: 4745\n",
      "A: 8527\n",
      "L: 7372\n",
      "V: 6105\n",
      "W: 1513\n",
      "F: 3549\n",
      "M: 1947\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "data_test_hydr = list_cb\n",
    "\n",
    "all_residues = []\n",
    "for i in range(len(data_test_hydr)):\n",
    "    mask = np.count_nonzero(data_test_hydr[i,:,50])\n",
    "    data_fasta = [AA[k] for k in np.argmax(data_test_hydr[i,:mask,:20],axis=-1)]\n",
    "    list_aa_in_hydr = pd.DataFrame({'AA': data_fasta, 'patch':list(data_test_hydr[i, :mask, 72])})\n",
    "    all_residues += list(list_aa_in_hydr[list_aa_in_hydr[\"patch\"] == 1]['AA'])\n",
    "    if 'P' in list(list_aa_in_hydr[list_aa_in_hydr[\"patch\"] == 1]['AA']):\n",
    "        print(list_aa_in_hydr)\n",
    "        print(name_train[i])\n",
    "        \n",
    "HYDR = ['A','C','F','I','L','M','V','W','Y']\n",
    "for item, count in Counter(all_residues).items():\n",
    "    print(f\"{item}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4379b771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
