Requirement already satisfied: click in /scistor/informatica/dgi460/anaconda3/envs/dl2022/lib/python3.10/site-packages (8.1.3)
Requirement already satisfied: fair-esm in /scistor/informatica/dgi460/anaconda3/envs/dl2022/lib/python3.10/site-packages (2.0.0)
Processing /scistor/informatica/dgi460/PROT/PROT
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Building wheels for collected packages: PROT
  Building wheel for PROT (setup.py): started
  Building wheel for PROT (setup.py): finished with status 'done'
  Created wheel for PROT: filename=PROT-0.0.1-py3-none-any.whl size=75426 sha256=f1cf44cb8fdbbacee49fcbfb81aabf7a9552a2648de88e3cd0eee03d4dfae099
  Stored in directory: /scratch/666237/pip-ephem-wheel-cache-f5djtcj6/wheels/f3/f3/d5/e95d019bbe728e863c8658a0c362e103b5264b28afecea62b9
Successfully built PROT
Installing collected packages: PROT
  Attempting uninstall: PROT
    Found existing installation: PROT 0.0.1
    Uninstalling PROT-0.0.1:
      Successfully uninstalled PROT-0.0.1
Successfully installed PROT-0.0.1
2024-02-04 18:00:35,083 - PROT.PROT.main - INFO - Building: PROT.models.ESM2_original_extended
FINETUNING CHOSEN: LoRA
Gradient Checkpointing 2
2024-02-04 18:00:47,273 - PROT.PROT.models.ESM2_original_extended.model - INFO - <init>: 
ESM2_original_extended(
  (embedding): ESM2Embedding(
    (model): ESM2(
      (embed_tokens): Embedding(33, 1280, padding_idx=1)
      (layers): ModuleList(
        (0): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (12): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (13): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (14): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (15): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (16): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (17): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (18): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (19): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (20): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (21): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (22): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (23): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (24): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (25): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (26): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (27): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (28): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (29): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (30): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (31): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (32): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (contact_head): ContactPredictionHead(
        (regression): Linear(in_features=660, out_features=1, bias=True)
        (activation): Sigmoid()
      )
      (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (lm_head): RobertaLMHead(
        (dense): Linear(in_features=1280, out_features=1280, bias=True)
        (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (conv): ModuleList(
    (0): Sequential(
      (0): Dropout(p=0.5, inplace=False)
      (1): Conv1d(1280, 32, kernel_size=(129,), stride=(1,), padding=(64,))
      (2): ReLU()
    )
    (1): Sequential(
      (0): Dropout(p=0.5, inplace=False)
      (1): Conv1d(1280, 32, kernel_size=(257,), stride=(1,), padding=(128,))
      (2): ReLU()
    )
  )
  (batch_norm): BatchNorm1d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (lstm): LSTM(1344, 1024, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)
  (lstm_dropout_layer): Dropout(p=0.5, inplace=False)
  (ss8): Sequential(
    (0): Linear(in_features=2048, out_features=8, bias=True)
  )
  (ss3): Sequential(
    (0): Linear(in_features=2048, out_features=3, bias=True)
  )
  (disorder): Sequential(
    (0): Linear(in_features=2048, out_features=2, bias=True)
  )
  (rsa): Sequential(
    (0): Linear(in_features=2048, out_features=1, bias=True)
    (1): Sigmoid()
  )
  (phi): Sequential(
    (0): Linear(in_features=2048, out_features=2, bias=True)
    (1): Tanh()
  )
  (psi): Sequential(
    (0): Linear(in_features=2048, out_features=2, bias=True)
    (1): Tanh()
  )
  (tasa): Sequential(
    (0): Linear(in_features=2048, out_features=1, bias=True)
  )
  (thsa): Sequential(
    (0): Linear(in_features=2048, out_features=1, bias=True)
  )
  (lhp): Sequential(
    (0): Linear(in_features=2048, out_features=1, bias=True)
  )
  (hp_loc): Sequential(
    (0): Linear(in_features=2048, out_features=2, bias=True)
  )
  (lhp_loc): Sequential(
    (0): Linear(in_features=2048, out_features=2, bias=True)
  )
  (species): Sequential(
    (0): Linear(in_features=2048, out_features=10, bias=True)
  )
  (expression): Sequential(
    (0): Linear(in_features=2048, out_features=2, bias=True)
  )
  (aggregation): Sequential(
    (0): Linear(in_features=2048, out_features=2, bias=True)
  )
)
Trainable parameters: 61504231
2024-02-04 18:00:47,283 - PROT.PROT.main - INFO - Using devices [0] of available devices [0]
Using devices [0] of available devices [0]
2024-02-04 18:01:00,544 - PROT.PROT.main - INFO - Building: torch.optim.Adam
2024-02-04 18:01:00,546 - PROT.PROT.main - INFO - Building: PROT.data_loader.augmentation.sparse_token
FINETUNING CHOSEN: no finetuning
Gradient Checkpointing None
2024-02-04 18:01:08,034 - PROT.PROT.main - INFO - Building: PROT.data_loader.data_loaders.NSPDataLoader
2024-02-04 18:05:11,184 - PROT.PROT.main - INFO - Getting loss and metric function handles
2024-02-04 18:05:11,185 - PROT.PROT.main - INFO - Initialising trainer
GRADIENT ACCUMULATION 6
2024-02-04 18:05:11,197 - PROT.PROT.base.base_trainer - INFO - Starting training...
Multi Task Loss
SS8 : 1 // SS3: 5 // DIS: 5 // RSA: 100 // PHI: 5 // PSI: 5
2024-02-04 18:05:19,838 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [0/10308 (0%)] Loss: 17.523422
2024-02-04 18:09:43,447 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [900/10308 (9%)] Loss: 8.051588
2024-02-04 18:13:59,882 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [1800/10308 (17%)] Loss: 7.895588
2024-02-04 18:18:17,207 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [2700/10308 (26%)] Loss: 7.333611
2024-02-04 18:22:42,063 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [3600/10308 (35%)] Loss: 8.163786
2024-02-04 18:27:01,007 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [4500/10308 (44%)] Loss: 6.744476
2024-02-04 18:31:17,507 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [5400/10308 (52%)] Loss: 7.743315
2024-02-04 18:35:36,397 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [6300/10308 (61%)] Loss: 9.359033
2024-02-04 18:39:54,130 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [7200/10308 (70%)] Loss: 6.557192
2024-02-04 18:44:17,465 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [8100/10308 (79%)] Loss: 7.555952
2024-02-04 18:48:32,765 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [9000/10308 (87%)] Loss: 7.488065
2024-02-04 18:52:52,010 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [9900/10308 (96%)] Loss: 6.675498
2024-02-04 18:56:09,212 - PROT.PROT.base.base_trainer - INFO - epoch          : 0
2024-02-04 18:56:09,213 - PROT.PROT.base.base_trainer - INFO - loss           : 7.539672561370121
2024-02-04 18:56:09,214 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.6789512590815624
2024-02-04 18:56:09,216 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8104840119679769
2024-02-04 18:56:09,217 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.4555217094408969
2024-02-04 18:56:09,218 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.5231580001612505
2024-02-04 18:56:09,220 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.7209473732703676
2024-02-04 18:56:09,221 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.748436643431584
2024-02-04 18:56:09,222 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 25.134789784749348
2024-02-04 18:56:09,223 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 32.18378512064616
2024-02-04 18:56:09,224 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.082064388422949
2024-02-04 18:56:09,226 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7602521517619876
2024-02-04 18:56:09,227 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8696553743413453
2024-02-04 18:56:09,228 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6051523468407332
2024-02-04 18:56:09,229 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.46369410838890557
2024-02-04 18:56:09,231 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8124748765322555
2024-02-04 18:56:09,232 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8251952607033437
2024-02-04 18:56:09,233 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.520358639889537
2024-02-04 18:56:09,234 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.174839519486657
2024-02-04 18:56:31,976 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0204-180511/checkpoints/checkpoint-epoch0.pth ...
2024-02-04 18:57:02,457 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2/0204-180511/checkpoints/model_best.pth
2024-02-04 18:57:05,077 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [0/10308 (0%)] Loss: 6.647788
2024-02-04 19:01:23,539 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [900/10308 (9%)] Loss: 6.821314
2024-02-04 19:05:38,764 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [1800/10308 (17%)] Loss: 6.986051
2024-02-04 19:10:02,342 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [2700/10308 (26%)] Loss: 6.810759
2024-02-04 19:14:23,898 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [3600/10308 (35%)] Loss: 7.401608
2024-02-04 19:18:50,615 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [4500/10308 (44%)] Loss: 6.585504
2024-02-04 19:22:54,397 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [5400/10308 (52%)] Loss: 9.225356
2024-02-04 19:27:20,167 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [6300/10308 (61%)] Loss: 11.014236
2024-02-04 19:31:45,338 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [7200/10308 (70%)] Loss: 6.668110
2024-02-04 19:36:09,184 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [8100/10308 (79%)] Loss: 7.416069
2024-02-04 19:40:30,699 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [9000/10308 (87%)] Loss: 6.135871
2024-02-04 19:44:45,687 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [9900/10308 (96%)] Loss: 6.777107
2024-02-04 19:47:48,897 - PROT.PROT.base.base_trainer - INFO - epoch          : 1
2024-02-04 19:47:48,901 - PROT.PROT.base.base_trainer - INFO - loss           : 7.097569283249076
2024-02-04 19:47:48,902 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7363964319229126
2024-02-04 19:47:48,903 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8501580506563187
2024-02-04 19:47:48,904 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.6212154142558575
2024-02-04 19:47:48,906 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.41047287235657376
2024-02-04 19:47:48,907 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.7837377488613129
2024-02-04 19:47:48,908 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.7982568194468816
2024-02-04 19:47:48,909 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 17.207842508951824
2024-02-04 19:47:48,910 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 25.735682646433514
2024-02-04 19:47:48,912 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.050405533991177
2024-02-04 19:47:48,913 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7643359956706142
2024-02-04 19:47:48,914 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8711792729877458
2024-02-04 19:47:48,915 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6260706902845896
2024-02-04 19:47:48,916 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.39990368996131687
2024-02-04 19:47:48,917 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8138639889520033
2024-02-04 19:47:48,919 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8271817770611316
2024-02-04 19:47:48,920 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.371667429089985
2024-02-04 19:47:48,921 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.84647401760425
2024-02-04 19:48:06,200 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0204-180511/checkpoints/checkpoint-epoch1.pth ...
2024-02-04 19:48:37,274 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2/0204-180511/checkpoints/model_best.pth
2024-02-04 19:48:39,095 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [0/10308 (0%)] Loss: 7.730390
2024-02-04 19:52:55,176 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [900/10308 (9%)] Loss: 7.059001
2024-02-04 19:57:08,809 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [1800/10308 (17%)] Loss: 6.564957
2024-02-04 20:01:34,989 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [2700/10308 (26%)] Loss: 6.714495
2024-02-04 20:05:53,405 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [3600/10308 (35%)] Loss: 7.690203
2024-02-04 20:10:13,691 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [4500/10308 (44%)] Loss: 6.350368
2024-02-04 20:14:50,493 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [5400/10308 (52%)] Loss: 6.619982
2024-02-04 20:19:13,379 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [6300/10308 (61%)] Loss: 6.769866
2024-02-04 20:23:39,712 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [7200/10308 (70%)] Loss: 6.477510
2024-02-04 20:28:04,745 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [8100/10308 (79%)] Loss: 7.303727
2024-02-04 20:32:17,327 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [9000/10308 (87%)] Loss: 8.242397
2024-02-04 20:36:32,892 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [9900/10308 (96%)] Loss: 6.639144
2024-02-04 20:39:43,039 - PROT.PROT.base.base_trainer - INFO - epoch          : 2
2024-02-04 20:39:43,040 - PROT.PROT.base.base_trainer - INFO - loss           : 7.039576030207819
2024-02-04 20:39:43,042 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7632386982440948
2024-02-04 20:39:43,043 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8814320613940557
2024-02-04 20:39:43,045 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.5677069587012132
2024-02-04 20:39:43,046 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.5090951472520828
2024-02-04 20:39:43,047 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8130000332991282
2024-02-04 20:39:43,049 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.826947882771492
2024-02-04 20:39:43,050 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 16.2159747282664
2024-02-04 20:39:43,051 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 22.34584951400757
2024-02-04 20:39:43,052 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.024678783663084
2024-02-04 20:39:43,053 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7656096703891825
2024-02-04 20:39:43,054 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8731279425955346
2024-02-04 20:39:43,056 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6209397658147691
2024-02-04 20:39:43,057 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.4138162885431709
2024-02-04 20:39:43,058 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8169729729420144
2024-02-04 20:39:43,059 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.830556166128039
2024-02-04 20:39:43,060 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.20134280616507
2024-02-04 20:39:43,061 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.635533297633774
2024-02-04 20:40:02,223 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0204-180511/checkpoints/checkpoint-epoch2.pth ...
2024-02-04 20:40:22,305 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2/0204-180511/checkpoints/model_best.pth
2024-02-04 20:40:23,597 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [0/10308 (0%)] Loss: 7.058994
2024-02-04 20:44:44,762 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [900/10308 (9%)] Loss: 6.312681
2024-02-04 20:49:08,831 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [1800/10308 (17%)] Loss: 5.831150
2024-02-04 20:53:41,466 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [2700/10308 (26%)] Loss: 6.227331
2024-02-04 20:58:11,608 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [3600/10308 (35%)] Loss: 6.711233
2024-02-04 21:02:25,868 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [4500/10308 (44%)] Loss: 5.947136
2024-02-04 21:06:44,423 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [5400/10308 (52%)] Loss: 7.463649
2024-02-04 21:11:02,041 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [6300/10308 (61%)] Loss: 7.355482
2024-02-04 21:15:21,383 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [7200/10308 (70%)] Loss: 6.488595
2024-02-04 21:19:40,915 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [8100/10308 (79%)] Loss: 5.879856
2024-02-04 21:23:50,524 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [9000/10308 (87%)] Loss: 6.390192
2024-02-04 21:28:12,235 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [9900/10308 (96%)] Loss: 6.192898
2024-02-04 21:31:18,700 - PROT.PROT.base.base_trainer - INFO - epoch          : 3
2024-02-04 21:31:18,704 - PROT.PROT.base.base_trainer - INFO - loss           : 7.012877376662998
2024-02-04 21:31:18,705 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.8004695177078247
2024-02-04 21:31:18,707 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8977264960606893
2024-02-04 21:31:18,708 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.5954121015965939
2024-02-04 21:31:18,710 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.45505619638909894
2024-02-04 21:31:18,711 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8452048997084299
2024-02-04 21:31:18,712 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8616990546385447
2024-02-04 21:31:18,713 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 15.023355642954508
2024-02-04 21:31:18,715 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 19.956252654393513
2024-02-04 21:31:18,716 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.029337791499176
2024-02-04 21:31:18,717 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7670460640284408
2024-02-04 21:31:18,718 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8730378800872507
2024-02-04 21:31:18,720 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6023876594417558
2024-02-04 21:31:18,721 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.47634459845030575
2024-02-04 21:31:18,722 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8185198239954635
2024-02-04 21:31:18,723 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8313177158691787
2024-02-04 21:31:18,724 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.312215124109134
2024-02-04 21:31:18,726 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.60420378547753
2024-02-04 21:31:37,822 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0204-180511/checkpoints/checkpoint-epoch3.pth ...
2024-02-04 21:31:40,428 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [0/10308 (0%)] Loss: 6.219823
2024-02-04 21:35:51,130 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [900/10308 (9%)] Loss: 7.511547
2024-02-04 21:40:13,601 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [1800/10308 (17%)] Loss: 7.016562
2024-02-04 21:44:42,080 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [2700/10308 (26%)] Loss: 6.102309
2024-02-04 21:49:04,135 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [3600/10308 (35%)] Loss: 6.420256
2024-02-04 21:53:25,548 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [4500/10308 (44%)] Loss: 7.317002
2024-02-04 21:57:35,863 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [5400/10308 (52%)] Loss: 6.273355
2024-02-04 22:02:01,767 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [6300/10308 (61%)] Loss: 7.959647
2024-02-04 22:06:10,524 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [7200/10308 (70%)] Loss: 7.100733
2024-02-04 22:10:42,330 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [8100/10308 (79%)] Loss: 7.934747
2024-02-04 22:15:00,910 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [9000/10308 (87%)] Loss: 7.848829
2024-02-04 22:19:21,045 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [9900/10308 (96%)] Loss: 6.101611
2024-02-04 22:22:20,461 - PROT.PROT.base.base_trainer - INFO - epoch          : 4
2024-02-04 22:22:20,466 - PROT.PROT.base.base_trainer - INFO - loss           : 6.995992865766943
2024-02-04 22:22:20,467 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7567909359931946
2024-02-04 22:22:20,468 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8646019250154495
2024-02-04 22:22:20,470 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.6116433516144753
2024-02-04 22:22:20,471 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.4310075656200449
2024-02-04 22:22:20,472 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8185839553674062
2024-02-04 22:22:20,473 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8277440369129181
2024-02-04 22:22:20,475 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 17.527357419331867
2024-02-04 22:22:20,476 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 24.190776586532593
2024-02-04 22:22:20,477 - PROT.PROT.base.base_trainer - INFO - val_loss       : 6.9891202608157785
2024-02-04 22:22:20,478 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7683893547726733
2024-02-04 22:22:20,480 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8760464125453766
2024-02-04 22:22:20,481 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6330406426252777
2024-02-04 22:22:20,482 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.38446801689511845
2024-02-04 22:22:20,483 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8187633114987194
2024-02-04 22:22:20,484 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8310796817510331
2024-02-04 22:22:20,486 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.175767405886493
2024-02-04 22:22:20,487 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.48461130712305
2024-02-04 22:22:38,050 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0204-180511/checkpoints/checkpoint-epoch4.pth ...
2024-02-04 22:22:58,888 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2/0204-180511/checkpoints/model_best.pth
2024-02-04 22:23:00,067 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [0/10308 (0%)] Loss: 6.350668
2024-02-04 22:27:21,215 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [900/10308 (9%)] Loss: 7.669361
2024-02-04 22:31:35,097 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [1800/10308 (17%)] Loss: 7.738903
2024-02-04 22:35:59,015 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [2700/10308 (26%)] Loss: 6.880479
2024-02-04 22:40:30,960 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [3600/10308 (35%)] Loss: 6.328785
2024-02-04 22:45:00,983 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [4500/10308 (44%)] Loss: 6.640831
2024-02-04 22:49:23,253 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [5400/10308 (52%)] Loss: 6.066392
2024-02-04 22:53:49,339 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [6300/10308 (61%)] Loss: 6.770849
2024-02-04 22:58:07,124 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [7200/10308 (70%)] Loss: 6.869992
2024-02-04 23:02:28,329 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [8100/10308 (79%)] Loss: 6.736392
2024-02-04 23:06:36,775 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [9000/10308 (87%)] Loss: 6.394732
2024-02-04 23:10:53,767 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [9900/10308 (96%)] Loss: 6.581717
2024-02-04 23:13:58,204 - PROT.PROT.base.base_trainer - INFO - epoch          : 5
2024-02-04 23:13:58,208 - PROT.PROT.base.base_trainer - INFO - loss           : 6.996833984721825
2024-02-04 23:13:58,210 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.8089476923147837
2024-02-04 23:13:58,211 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.892703170577685
2024-02-04 23:13:58,212 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.6574194331963857
2024-02-04 23:13:58,214 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.38563806128998596
2024-02-04 23:13:58,215 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8163481205701828
2024-02-04 23:13:58,216 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8250652352968851
2024-02-04 23:13:58,217 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 14.35734494527181
2024-02-04 23:13:58,219 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 20.893773317337036
2024-02-04 23:13:58,220 - PROT.PROT.base.base_trainer - INFO - val_loss       : 6.989741429191674
2024-02-04 23:13:58,221 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.768832513107145
2024-02-04 23:13:58,222 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8767017482391583
2024-02-04 23:13:58,223 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6333872312611154
2024-02-04 23:13:58,224 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.35042522477872695
2024-02-04 23:13:58,225 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8195946910504486
2024-02-04 23:13:58,226 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8319440995415198
2024-02-04 23:13:58,228 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.21947062763341
2024-02-04 23:13:58,229 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.52319226845604
2024-02-04 23:14:15,648 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0204-180511/checkpoints/checkpoint-epoch5.pth ...
2024-02-04 23:14:17,971 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [0/10308 (0%)] Loss: 6.436493
2024-02-04 23:18:48,566 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [900/10308 (9%)] Loss: 6.220939
2024-02-04 23:23:11,832 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [1800/10308 (17%)] Loss: 7.992335
2024-02-04 23:27:30,699 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [2700/10308 (26%)] Loss: 6.888294
2024-02-04 23:31:44,966 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [3600/10308 (35%)] Loss: 7.723632
2024-02-04 23:36:07,601 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [4500/10308 (44%)] Loss: 7.305945
2024-02-04 23:40:26,828 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [5400/10308 (52%)] Loss: 6.773920
2024-02-04 23:44:39,464 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [6300/10308 (61%)] Loss: 7.250639
2024-02-04 23:49:01,056 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [7200/10308 (70%)] Loss: 6.402875
2024-02-04 23:53:18,827 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [8100/10308 (79%)] Loss: 6.663154
2024-02-04 23:57:37,073 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [9000/10308 (87%)] Loss: 6.643760
2024-02-05 00:02:03,618 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [9900/10308 (96%)] Loss: 6.272581
2024-02-05 00:05:09,192 - PROT.PROT.base.base_trainer - INFO - epoch          : 6
2024-02-05 00:05:09,196 - PROT.PROT.base.base_trainer - INFO - loss           : 6.977283578230852
2024-02-05 00:05:09,197 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7794148822625478
2024-02-05 00:05:09,198 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8800216962893804
2024-02-05 00:05:09,200 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.5941298082470894
2024-02-05 00:05:09,201 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.46736927640934783
2024-02-05 00:05:09,202 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8290917277336121
2024-02-05 00:05:09,203 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8368760943412781
2024-02-05 00:05:09,204 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 16.045671304066975
2024-02-05 00:05:09,205 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 22.615302721659344
2024-02-05 00:05:09,206 - PROT.PROT.base.base_trainer - INFO - val_loss       : 6.994143353177173
2024-02-05 00:05:09,208 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7698739270002639
2024-02-05 00:05:09,208 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8758040864529205
2024-02-05 00:05:09,210 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6336570194186952
2024-02-05 00:05:09,211 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.36232653090118594
2024-02-05 00:05:09,212 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8188309006365463
2024-02-05 00:05:09,213 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8310595692523731
2024-02-05 00:05:09,214 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.22544852745929
2024-02-05 00:05:09,215 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.557834392983974
2024-02-05 00:05:27,339 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0204-180511/checkpoints/checkpoint-epoch6.pth ...
2024-02-05 00:05:29,660 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [0/10308 (0%)] Loss: 5.869961
2024-02-05 00:09:36,329 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [900/10308 (9%)] Loss: 7.204438
2024-02-05 00:13:52,313 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [1800/10308 (17%)] Loss: 8.119964
2024-02-05 00:18:12,150 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [2700/10308 (26%)] Loss: 7.365956
2024-02-05 00:22:35,320 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [3600/10308 (35%)] Loss: 7.209090
2024-02-05 00:26:58,385 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [4500/10308 (44%)] Loss: 7.256282
2024-02-05 00:31:28,400 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [5400/10308 (52%)] Loss: 6.668716
2024-02-05 00:35:53,726 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [6300/10308 (61%)] Loss: 6.759574
2024-02-05 00:40:11,913 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [7200/10308 (70%)] Loss: 6.172101
2024-02-05 00:44:29,999 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [8100/10308 (79%)] Loss: 6.535338
2024-02-05 00:48:56,903 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [9000/10308 (87%)] Loss: 6.335327
2024-02-05 00:53:10,588 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [9900/10308 (96%)] Loss: 6.409677
2024-02-05 00:56:20,429 - PROT.PROT.base.base_trainer - INFO - epoch          : 7
2024-02-05 00:56:20,433 - PROT.PROT.base.base_trainer - INFO - loss           : 6.982666464007389
2024-02-05 00:56:20,435 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7902447134256363
2024-02-05 00:56:20,436 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8911527742942175
2024-02-05 00:56:20,438 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.6559400459130605
2024-02-05 00:56:20,439 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.3533261561145385
2024-02-05 00:56:20,440 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8234426428874334
2024-02-05 00:56:20,441 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8347537318865458
2024-02-05 00:56:20,442 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 15.950875043869019
2024-02-05 00:56:20,444 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 21.87476460138957
2024-02-05 00:56:20,445 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.0206078981561415
2024-02-05 00:56:20,446 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.767305906848274
2024-02-05 00:56:20,447 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8743151966276204
2024-02-05 00:56:20,448 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.5974578872627142
2024-02-05 00:56:20,449 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.4775797078845563
2024-02-05 00:56:20,450 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8166658150313965
2024-02-05 00:56:20,451 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8292837498152827
2024-02-05 00:56:20,453 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.264120693136405
2024-02-05 00:56:20,454 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.63820806552563
2024-02-05 00:56:41,233 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0204-180511/checkpoints/checkpoint-epoch7.pth ...
2024-02-05 00:56:44,245 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [0/10308 (0%)] Loss: 6.710231
2024-02-05 01:01:03,904 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [900/10308 (9%)] Loss: 6.491596
2024-02-05 01:05:29,824 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [1800/10308 (17%)] Loss: 5.997526
2024-02-05 01:09:51,840 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [2700/10308 (26%)] Loss: 7.529309
2024-02-05 01:14:01,393 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [3600/10308 (35%)] Loss: 7.805490
2024-02-05 01:18:25,704 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [4500/10308 (44%)] Loss: 6.354577
2024-02-05 01:22:49,631 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [5400/10308 (52%)] Loss: 6.773522
2024-02-05 01:27:14,785 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [6300/10308 (61%)] Loss: 6.201400
2024-02-05 01:31:38,406 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [7200/10308 (70%)] Loss: 6.631123
2024-02-05 01:35:56,916 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [8100/10308 (79%)] Loss: 6.560956
2024-02-05 01:40:16,716 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [9000/10308 (87%)] Loss: 6.552564
2024-02-05 01:44:35,319 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [9900/10308 (96%)] Loss: 6.828979
2024-02-05 01:47:42,977 - PROT.PROT.base.base_trainer - INFO - epoch          : 8
2024-02-05 01:47:42,982 - PROT.PROT.base.base_trainer - INFO - loss           : 6.978357957169636
2024-02-05 01:47:42,984 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7871637791395187
2024-02-05 01:47:42,985 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8904696057240168
2024-02-05 01:47:42,987 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.6671699433354661
2024-02-05 01:47:42,988 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.37550109811127186
2024-02-05 01:47:42,989 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8336647798617681
2024-02-05 01:47:42,990 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8424584319194158
2024-02-05 01:47:42,991 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 16.322819073994953
2024-02-05 01:47:42,993 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 21.822296142578125
2024-02-05 01:47:42,994 - PROT.PROT.base.base_trainer - INFO - val_loss       : 6.975466572490565
2024-02-05 01:47:42,995 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7682495011614697
2024-02-05 01:47:42,996 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8758923488349493
2024-02-05 01:47:42,997 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6264337456984466
2024-02-05 01:47:42,998 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.40461413738252494
2024-02-05 01:47:43,000 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8188196383279188
2024-02-05 01:47:43,001 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8312394750954041
2024-02-05 01:47:43,002 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.28642909553218
2024-02-05 01:47:43,003 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.512224814988592
2024-02-05 01:48:00,759 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0204-180511/checkpoints/checkpoint-epoch8.pth ...
2024-02-05 01:48:20,240 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2/0204-180511/checkpoints/model_best.pth
2024-02-05 01:48:22,028 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [0/10308 (0%)] Loss: 7.029046
2024-02-05 01:52:53,494 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [900/10308 (9%)] Loss: 6.718162
2024-02-05 01:57:09,430 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [1800/10308 (17%)] Loss: 6.493070
2024-02-05 02:01:28,486 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [2700/10308 (26%)] Loss: 8.838443
2024-02-05 02:05:44,987 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [3600/10308 (35%)] Loss: 7.453278
2024-02-05 02:09:59,692 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [4500/10308 (44%)] Loss: 8.546463
2024-02-05 02:14:20,526 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [5400/10308 (52%)] Loss: 7.252399
2024-02-05 02:18:53,046 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [6300/10308 (61%)] Loss: 7.045078
2024-02-05 02:23:03,192 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [7200/10308 (70%)] Loss: 6.377841
2024-02-05 02:27:25,813 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [8100/10308 (79%)] Loss: 6.695965
2024-02-05 02:31:44,240 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [9000/10308 (87%)] Loss: 6.684786
2024-02-05 02:36:06,031 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [9900/10308 (96%)] Loss: 6.425041
2024-02-05 02:39:13,072 - PROT.PROT.base.base_trainer - INFO - epoch          : 9
2024-02-05 02:39:13,077 - PROT.PROT.base.base_trainer - INFO - loss           : 6.9693931324746154
2024-02-05 02:39:13,078 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7404827872912089
2024-02-05 02:39:13,079 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.860151951511701
2024-02-05 02:39:13,081 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.664900653064251
2024-02-05 02:39:13,082 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.4040493415668607
2024-02-05 02:39:13,083 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.7957595586776733
2024-02-05 02:39:13,084 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.811594620347023
2024-02-05 02:39:13,086 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 18.256453037261963
2024-02-05 02:39:13,087 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 25.65969642003377
2024-02-05 02:39:13,088 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.02581373791853
2024-02-05 02:39:13,089 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7670803050273459
2024-02-05 02:39:13,090 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.875517805783951
2024-02-05 02:39:13,091 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.5936856694522126
2024-02-05 02:39:13,092 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.4872247584219744
2024-02-05 02:39:13,093 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.817399109751536
2024-02-05 02:39:13,095 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8302352713262903
2024-02-05 02:39:13,096 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.327340192900373
2024-02-05 02:39:13,097 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.60802464467573
2024-02-05 02:39:32,786 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0204-180511/checkpoints/checkpoint-epoch9.pth ...
2024-02-05 02:39:35,693 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [0/10308 (0%)] Loss: 7.384314
2024-02-05 02:44:03,411 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [900/10308 (9%)] Loss: 7.342740
2024-02-05 02:48:33,910 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [1800/10308 (17%)] Loss: 6.154507
2024-02-05 02:52:45,485 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [2700/10308 (26%)] Loss: 8.119180
2024-02-05 02:57:11,235 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [3600/10308 (35%)] Loss: 6.798457
2024-02-05 03:01:22,972 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [4500/10308 (44%)] Loss: 7.779227
2024-02-05 03:05:48,234 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [5400/10308 (52%)] Loss: 7.899812
2024-02-05 03:10:18,138 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [6300/10308 (61%)] Loss: 6.656505
2024-02-05 03:14:37,656 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [7200/10308 (70%)] Loss: 7.168306
2024-02-05 03:19:03,419 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [8100/10308 (79%)] Loss: 6.834673
2024-02-05 03:23:22,178 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [9000/10308 (87%)] Loss: 6.629949
2024-02-05 03:27:28,960 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [9900/10308 (96%)] Loss: 7.062310
2024-02-05 03:30:37,012 - PROT.PROT.base.base_trainer - INFO - epoch          : 10
2024-02-05 03:30:37,016 - PROT.PROT.base.base_trainer - INFO - loss           : 6.9645235988669505
2024-02-05 03:30:37,017 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7588652223348618
2024-02-05 03:30:37,019 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8691882193088531
2024-02-05 03:30:37,020 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.4949018980065982
2024-02-05 03:30:37,021 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.5909201204776764
2024-02-05 03:30:37,022 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8080112189054489
2024-02-05 03:30:37,024 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8174419353405634
2024-02-05 03:30:37,025 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 17.81887650489807
2024-02-05 03:30:37,026 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 23.780359268188477
2024-02-05 03:30:37,027 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.04610086367139
2024-02-05 03:30:37,028 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7671595775553222
2024-02-05 03:30:37,029 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8751108598005288
2024-02-05 03:30:37,030 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.629129986000347
2024-02-05 03:30:37,032 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.38417371154532554
2024-02-05 03:30:37,033 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8166486484538145
2024-02-05 03:30:37,034 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8297746882007571
2024-02-05 03:30:37,035 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.31871746386989
2024-02-05 03:30:37,037 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.648192178719157
2024-02-05 03:30:55,036 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0204-180511/checkpoints/checkpoint-epoch10.pth ...
2024-02-05 03:30:57,238 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [0/10308 (0%)] Loss: 6.566248
2024-02-05 03:35:03,469 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [900/10308 (9%)] Loss: 6.692945
2024-02-05 03:39:17,827 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [1800/10308 (17%)] Loss: 7.819941
2024-02-05 03:43:33,848 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [2700/10308 (26%)] Loss: 7.151970
2024-02-05 03:47:58,347 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [3600/10308 (35%)] Loss: 7.265170
2024-02-05 03:52:18,604 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [4500/10308 (44%)] Loss: 6.684643
2024-02-05 03:56:36,992 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [5400/10308 (52%)] Loss: 6.757182
2024-02-05 04:01:01,724 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [6300/10308 (61%)] Loss: 6.375597
2024-02-05 04:05:27,174 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [7200/10308 (70%)] Loss: 6.381485
2024-02-05 04:10:00,153 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [8100/10308 (79%)] Loss: 8.970864
2024-02-05 04:14:25,491 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [9000/10308 (87%)] Loss: 7.093853
2024-02-05 04:18:45,106 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [9900/10308 (96%)] Loss: 7.346254
2024-02-05 04:21:49,882 - PROT.PROT.base.base_trainer - INFO - epoch          : 11
2024-02-05 04:21:49,883 - PROT.PROT.base.base_trainer - INFO - loss           : 6.9662431396374815
2024-02-05 04:21:49,885 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7716266413529714
2024-02-05 04:21:49,886 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8693023870388666
2024-02-05 04:21:49,887 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.7302219395836195
2024-02-05 04:21:49,889 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.3118695095181465
2024-02-05 04:21:49,890 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.7938382079203924
2024-02-05 04:21:49,891 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.7991462647914886
2024-02-05 04:21:49,892 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 17.539339145024616
2024-02-05 04:21:49,894 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 24.841171423594158
2024-02-05 04:21:49,895 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.0102803839088805
2024-02-05 04:21:49,896 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7692253506051658
2024-02-05 04:21:49,897 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8760071791625991
2024-02-05 04:21:49,899 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6219935564544422
2024-02-05 04:21:49,900 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.4019147252990628
2024-02-05 04:21:49,901 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8178480975742269
2024-02-05 04:21:49,902 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8303609447505641
2024-02-05 04:21:49,904 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.254246824341948
2024-02-05 04:21:49,905 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.90920514462179
2024-02-05 04:22:06,983 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0204-180511/checkpoints/checkpoint-epoch11.pth ...
2024-02-05 04:22:09,456 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [0/10308 (0%)] Loss: 6.787598
2024-02-05 04:26:20,735 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [900/10308 (9%)] Loss: 7.060258
2024-02-05 04:30:32,872 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [1800/10308 (17%)] Loss: 7.159841
2024-02-05 04:34:54,045 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [2700/10308 (26%)] Loss: 6.513825
2024-02-05 04:39:21,341 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [3600/10308 (35%)] Loss: 5.842963
2024-02-05 04:43:50,663 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [4500/10308 (44%)] Loss: 6.609455
2024-02-05 04:48:07,691 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [5400/10308 (52%)] Loss: 7.866834
2024-02-05 04:52:25,772 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [6300/10308 (61%)] Loss: 6.508679
2024-02-05 04:56:39,867 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [7200/10308 (70%)] Loss: 7.084248
2024-02-05 05:01:05,338 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [8100/10308 (79%)] Loss: 6.581262
2024-02-05 05:05:29,877 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [9000/10308 (87%)] Loss: 6.835346
2024-02-05 05:09:54,743 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [9900/10308 (96%)] Loss: 5.910015
2024-02-05 05:13:04,654 - PROT.PROT.base.base_trainer - INFO - epoch          : 12
2024-02-05 05:13:04,658 - PROT.PROT.base.base_trainer - INFO - loss           : 6.957948321165355
2024-02-05 05:13:04,662 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7751849889755249
2024-02-05 05:13:04,665 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8839736680189768
2024-02-05 05:13:04,668 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.6326380024353663
2024-02-05 05:13:04,671 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.44151311212529737
2024-02-05 05:13:04,675 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8352367430925369
2024-02-05 05:13:04,678 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8423884858687719
2024-02-05 05:13:04,680 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 16.101691722869873
2024-02-05 05:13:04,683 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 21.542296409606934
2024-02-05 05:13:04,688 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.01336229859243
2024-02-05 05:13:04,715 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7677254534955394
2024-02-05 05:13:04,720 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8752725987636735
2024-02-05 05:13:04,723 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6024289980154063
2024-02-05 05:13:04,726 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.44427647257434527
2024-02-05 05:13:04,728 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8183787592662657
2024-02-05 05:13:04,737 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8304274858583823
2024-02-05 05:13:04,747 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.20743404895177
2024-02-05 05:13:04,750 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.34582022811214
2024-02-05 05:13:21,534 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0204-180511/checkpoints/checkpoint-epoch12.pth ...
2024-02-05 05:13:23,993 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [0/10308 (0%)] Loss: 6.249256
2024-02-05 05:17:41,651 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [900/10308 (9%)] Loss: 6.974486
2024-02-05 05:22:05,880 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [1800/10308 (17%)] Loss: 6.280928
2024-02-05 05:26:25,473 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [2700/10308 (26%)] Loss: 6.842689
2024-02-05 05:30:38,508 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [3600/10308 (35%)] Loss: 7.363757
2024-02-05 05:34:56,941 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [4500/10308 (44%)] Loss: 7.813681
2024-02-05 05:39:19,675 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [5400/10308 (52%)] Loss: 7.722696
2024-02-05 05:43:35,696 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [6300/10308 (61%)] Loss: 7.345606
2024-02-05 05:48:04,189 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [7200/10308 (70%)] Loss: 6.147656
2024-02-05 05:52:23,246 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [8100/10308 (79%)] Loss: 6.333420
2024-02-05 05:56:51,961 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [9000/10308 (87%)] Loss: 6.333327
2024-02-05 06:01:17,101 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [9900/10308 (96%)] Loss: 7.369303
2024-02-05 06:04:20,057 - PROT.PROT.base.base_trainer - INFO - epoch          : 13
2024-02-05 06:04:20,061 - PROT.PROT.base.base_trainer - INFO - loss           : 6.956952434777056
2024-02-05 06:04:20,063 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7618798911571503
2024-02-05 06:04:20,064 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8784205416838328
2024-02-05 06:04:20,065 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.7029642636577288
2024-02-05 06:04:20,066 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.3489383092770974
2024-02-05 06:04:20,068 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8084380477666855
2024-02-05 06:04:20,069 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.821731706460317
2024-02-05 06:04:20,070 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 17.741758664449055
2024-02-05 06:04:20,071 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 24.33624203999837
2024-02-05 06:04:20,072 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.047993705721359
2024-02-05 06:04:20,073 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7655470542802142
2024-02-05 06:04:20,074 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8740232038102027
2024-02-05 06:04:20,075 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.5987043339798632
2024-02-05 06:04:20,076 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.4727154536846324
2024-02-05 06:04:20,077 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8165978820121597
2024-02-05 06:04:20,078 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8293439367600473
2024-02-05 06:04:20,079 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.390794752268775
2024-02-05 06:04:20,080 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.69032082962374
2024-02-05 06:04:38,274 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0204-180511/checkpoints/checkpoint-epoch13.pth ...
2024-02-05 06:04:40,574 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [0/10308 (0%)] Loss: 6.520181
2024-02-05 06:08:56,570 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [900/10308 (9%)] Loss: 6.297214
2024-02-05 06:13:16,298 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [1800/10308 (17%)] Loss: 7.545016
2024-02-05 06:17:55,199 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [2700/10308 (26%)] Loss: 6.603014
2024-02-05 06:22:16,625 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [3600/10308 (35%)] Loss: 7.691643
2024-02-05 06:26:31,248 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [4500/10308 (44%)] Loss: 7.048069
2024-02-05 06:30:55,880 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [5400/10308 (52%)] Loss: 6.076587
2024-02-05 06:35:15,340 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [6300/10308 (61%)] Loss: 7.468038
2024-02-05 06:39:27,840 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [7200/10308 (70%)] Loss: 6.715000
2024-02-05 06:43:55,851 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [8100/10308 (79%)] Loss: 8.148710
2024-02-05 06:48:11,058 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [9000/10308 (87%)] Loss: 6.533379
2024-02-05 06:52:36,507 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [9900/10308 (96%)] Loss: 7.410554
2024-02-05 06:55:40,444 - PROT.PROT.base.base_trainer - INFO - epoch          : 14
2024-02-05 06:55:40,447 - PROT.PROT.base.base_trainer - INFO - loss           : 6.94296785315194
2024-02-05 06:55:40,449 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7542659838994344
2024-02-05 06:55:40,450 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8729756822188696
2024-02-05 06:55:40,452 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.6301930266975736
2024-02-05 06:55:40,453 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.43035333044826984
2024-02-05 06:55:40,454 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8011761059363683
2024-02-05 06:55:40,455 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8116936683654785
2024-02-05 06:55:40,457 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 18.09102471669515
2024-02-05 06:55:40,458 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 24.105692704518635
2024-02-05 06:55:40,459 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.01187566637553
2024-02-05 06:55:40,460 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7666428162822864
2024-02-05 06:55:40,461 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8752437156944697
2024-02-05 06:55:40,462 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6103678259044775
2024-02-05 06:55:40,463 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.41956312502414705
2024-02-05 06:55:40,464 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8175286783723373
2024-02-05 06:55:40,465 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8301743714351936
2024-02-05 06:55:40,467 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.2366152812634
2024-02-05 06:55:40,468 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.46260169335397
2024-02-05 06:55:58,666 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0204-180511/checkpoints/checkpoint-epoch14.pth ...
2024-02-05 06:55:59,158 - PROT.PROT.main - INFO - Initialising evaluation
2024-02-05 06:56:12,523 - PROT.PROT.base.base_eval - INFO - Starting evaluating...
2024-02-05 06:56:17,871 - PROT.PROT.base.base_eval - INFO - metric_ss8: 0.6839870129312787
2024-02-05 06:56:17,872 - PROT.PROT.base.base_eval - INFO - metric_ss3: 0.7935937217303685
2024-02-05 06:56:17,874 - PROT.PROT.base.base_eval - INFO - metric_dis_mcc: 0.5779818296432495
2024-02-05 06:56:17,875 - PROT.PROT.base.base_eval - INFO - metric_dis_fnr: 0.45541223298226086
2024-02-05 06:56:17,876 - PROT.PROT.base.base_eval - INFO - metric_rsa: 0.721199095249176
2024-02-05 06:56:17,877 - PROT.PROT.base.base_eval - INFO - metric_asa: 0.7366729548999241
2024-02-05 06:56:17,878 - PROT.PROT.base.base_eval - INFO - metric_phi: 20.413818631853378
2024-02-05 06:56:17,880 - PROT.PROT.base.base_eval - INFO - metric_psi: 31.801055090767996
2024-02-05 06:56:19,074 - PROT.PROT.base.base_eval - INFO - Starting evaluating...
2024-02-05 06:57:23,241 - PROT.PROT.base.base_eval - INFO - metric_ss8: 0.7372017499299077
2024-02-05 06:57:23,245 - PROT.PROT.base.base_eval - INFO - metric_ss3: 0.8675571516243338
2024-02-05 06:57:23,246 - PROT.PROT.base.base_eval - INFO - metric_dis_mcc: 0.10682370797406103
2024-02-05 06:57:23,247 - PROT.PROT.base.base_eval - INFO - metric_dis_fnr: 0.8782079249036079
2024-02-05 06:57:23,248 - PROT.PROT.base.base_eval - INFO - metric_rsa: 0.815448891001138
2024-02-05 06:57:23,250 - PROT.PROT.base.base_eval - INFO - metric_asa: 0.8281313393548219
2024-02-05 06:57:23,251 - PROT.PROT.base.base_eval - INFO - metric_phi: 18.93720271573429
2024-02-05 06:57:23,252 - PROT.PROT.base.base_eval - INFO - metric_psi: 25.536577280501874
2024-02-05 06:57:24,349 - PROT.PROT.base.base_eval - INFO - Starting evaluating...
2024-02-05 06:57:40,941 - PROT.PROT.base.base_eval - INFO - metric_ss8: 0.7641811635183251
2024-02-05 06:57:40,941 - PROT.PROT.base.base_eval - INFO - metric_ss3: 0.8676562713540119
2024-02-05 06:57:40,943 - PROT.PROT.base.base_eval - INFO - metric_dis_mcc: 0.6576610451645177
2024-02-05 06:57:40,944 - PROT.PROT.base.base_eval - INFO - metric_dis_fnr: 0.3859398316430009
2024-02-05 06:57:40,945 - PROT.PROT.base.base_eval - INFO - metric_rsa: 0.794197502343551
2024-02-05 06:57:40,947 - PROT.PROT.base.base_eval - INFO - metric_asa: 0.8129661217979763
2024-02-05 06:57:40,948 - PROT.PROT.base.base_eval - INFO - metric_phi: 16.35846071657927
2024-02-05 06:57:40,949 - PROT.PROT.base.base_eval - INFO - metric_psi: 23.58308634550675
2024-02-05 06:57:40,951 - PROT.PROT.main - INFO - Finished!
done
