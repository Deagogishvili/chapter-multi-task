Requirement already satisfied: click in /scistor/informatica/dgi460/anaconda3/envs/dl2022/lib/python3.10/site-packages (8.1.3)
Requirement already satisfied: fair-esm in /scistor/informatica/dgi460/anaconda3/envs/dl2022/lib/python3.10/site-packages (2.0.0)
Processing /scistor/informatica/dgi460/PROT/PROT
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Building wheels for collected packages: PROT
  Building wheel for PROT (setup.py): started
  Building wheel for PROT (setup.py): finished with status 'done'
  Created wheel for PROT: filename=PROT-0.0.1-py3-none-any.whl size=95669 sha256=e896651feadb893c529206b633aca3e7a95dd3820a9c4916dfe4171432abfbe5
  Stored in directory: /scratch/671591/pip-ephem-wheel-cache-_5lzldda/wheels/f3/f3/d5/e95d019bbe728e863c8658a0c362e103b5264b28afecea62b9
Successfully built PROT
Installing collected packages: PROT
  Attempting uninstall: PROT
    Found existing installation: PROT 0.0.1
    Uninstalling PROT-0.0.1:
      Successfully uninstalled PROT-0.0.1
Successfully installed PROT-0.0.1
2024-02-10 19:41:04,086 - PROT.PROT.main - INFO - Building: PROT.models.ESM2_multitask
FINETUNING CHOSEN: LoRA
Gradient Checkpointing 2
2024-02-10 19:41:09,458 - PROT.PROT.models.ESM2_multitask.model - INFO - Params to learn:
2024-02-10 19:41:09,458 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.0.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,463 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.0.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,467 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.0.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,471 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.0.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,476 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.1.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,479 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.1.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,485 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.1.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,491 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.1.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,494 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.2.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,499 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.2.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,504 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.2.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,508 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.2.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,512 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.3.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,516 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.3.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,520 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.3.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,524 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.3.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,529 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.4.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,535 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.4.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,540 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.4.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,545 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.4.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,550 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.5.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,553 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.5.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,558 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.5.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,563 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.5.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,568 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.6.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,572 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.6.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,576 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.6.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,579 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.6.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,584 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.7.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,587 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.7.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,591 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.7.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,595 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.7.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,598 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.8.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,602 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.8.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,607 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.8.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,611 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.8.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,615 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.9.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,620 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.9.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,624 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.9.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,628 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.9.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,632 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.10.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,638 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.10.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,657 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.10.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,662 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.10.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,667 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.11.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,682 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.11.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,693 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.11.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,698 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.11.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,703 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.12.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,706 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.12.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,710 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.12.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,714 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.12.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,719 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.13.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,722 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.13.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,726 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.13.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,731 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.13.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,735 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.14.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,740 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.14.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,746 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.14.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,750 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.14.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,755 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.15.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,759 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.15.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,762 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.15.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,768 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.15.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,772 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.16.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,775 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.16.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,779 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.16.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,784 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.16.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,788 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.17.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,793 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.17.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,797 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.17.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,802 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.17.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,806 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.18.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,810 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.18.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,814 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.18.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,818 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.18.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,823 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.19.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,827 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.19.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,832 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.19.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,836 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.19.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,840 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.20.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,844 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.20.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,849 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.20.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,853 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.20.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,857 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.21.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,861 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.21.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,864 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.21.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,868 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.21.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,872 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.22.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,877 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.22.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,881 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.22.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,885 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.22.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,891 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.23.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,895 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.23.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,899 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.23.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,904 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.23.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,908 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.24.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,911 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.24.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,916 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.24.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,921 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.24.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,925 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.25.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,930 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.25.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,934 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.25.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,937 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.25.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,941 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.26.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,945 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.26.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,949 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.26.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,953 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.26.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,957 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.27.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,962 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.27.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,966 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.27.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,970 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.27.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,973 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.28.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,977 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.28.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,981 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.28.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:09,985 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.28.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:09,989 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.29.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:09,993 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.29.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:09,997 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.29.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:10,001 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.29.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:10,005 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.30.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:10,010 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.30.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:10,013 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.30.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:10,017 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.30.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:10,020 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.31.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:10,024 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.31.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:10,028 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.31.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:10,031 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.31.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:10,035 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.32.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:10,038 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.32.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:10,042 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.32.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:10,047 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.32.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:10,051 - PROT.PROT.models.ESM2_multitask.model - INFO - 	ss8.0.weight
2024-02-10 19:41:10,055 - PROT.PROT.models.ESM2_multitask.model - INFO - 	ss8.0.bias
2024-02-10 19:41:10,058 - PROT.PROT.models.ESM2_multitask.model - INFO - 	ss3.0.weight
2024-02-10 19:41:10,062 - PROT.PROT.models.ESM2_multitask.model - INFO - 	ss3.0.bias
2024-02-10 19:41:10,065 - PROT.PROT.models.ESM2_multitask.model - INFO - 	disorder.0.weight
2024-02-10 19:41:10,070 - PROT.PROT.models.ESM2_multitask.model - INFO - 	disorder.0.bias
2024-02-10 19:41:10,073 - PROT.PROT.models.ESM2_multitask.model - INFO - 	rsa.0.weight
2024-02-10 19:41:10,080 - PROT.PROT.models.ESM2_multitask.model - INFO - 	rsa.0.bias
2024-02-10 19:41:10,083 - PROT.PROT.models.ESM2_multitask.model - INFO - 	phi.0.weight
2024-02-10 19:41:10,087 - PROT.PROT.models.ESM2_multitask.model - INFO - 	phi.0.bias
2024-02-10 19:41:10,090 - PROT.PROT.models.ESM2_multitask.model - INFO - 	psi.0.weight
2024-02-10 19:41:10,094 - PROT.PROT.models.ESM2_multitask.model - INFO - 	psi.0.bias
2024-02-10 19:41:10,098 - PROT.PROT.models.ESM2_multitask.model - INFO - 	tasa.0.weight
2024-02-10 19:41:10,103 - PROT.PROT.models.ESM2_multitask.model - INFO - 	tasa.0.bias
2024-02-10 19:41:10,106 - PROT.PROT.models.ESM2_multitask.model - INFO - 	thsa.0.weight
2024-02-10 19:41:10,111 - PROT.PROT.models.ESM2_multitask.model - INFO - 	thsa.0.bias
2024-02-10 19:41:10,114 - PROT.PROT.models.ESM2_multitask.model - INFO - 	lhp.0.weight
2024-02-10 19:41:10,119 - PROT.PROT.models.ESM2_multitask.model - INFO - 	lhp.0.bias
2024-02-10 19:41:10,123 - PROT.PROT.models.ESM2_multitask.model - INFO - 	hp_loc.0.weight
2024-02-10 19:41:10,126 - PROT.PROT.models.ESM2_multitask.model - INFO - 	hp_loc.0.bias
2024-02-10 19:41:10,131 - PROT.PROT.models.ESM2_multitask.model - INFO - 	lhp_loc.0.weight
2024-02-10 19:41:10,134 - PROT.PROT.models.ESM2_multitask.model - INFO - 	lhp_loc.0.bias
2024-02-10 19:41:10,139 - PROT.PROT.models.ESM2_multitask.model - INFO - 	species.0.weight
2024-02-10 19:41:10,143 - PROT.PROT.models.ESM2_multitask.model - INFO - 	species.0.bias
2024-02-10 19:41:10,148 - PROT.PROT.models.ESM2_multitask.model - INFO - 	expression.0.weight
2024-02-10 19:41:10,151 - PROT.PROT.models.ESM2_multitask.model - INFO - 	expression.0.bias
2024-02-10 19:41:10,155 - PROT.PROT.models.ESM2_multitask.model - INFO - 	aggregation.0.weight
2024-02-10 19:41:10,158 - PROT.PROT.models.ESM2_multitask.model - INFO - 	aggregation.0.bias
2024-02-10 19:41:10,165 - PROT.PROT.models.ESM2_multitask.model - INFO - <init>: 
ESM2_multitask(
  (embedding): ESM2Embedding(
    (model): ESM2(
      (embed_tokens): Embedding(33, 1280, padding_idx=1)
      (layers): ModuleList(
        (0): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (12): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (13): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (14): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (15): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (16): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (17): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (18): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (19): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (20): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (21): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (22): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (23): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (24): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (25): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (26): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (27): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (28): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (29): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (30): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (31): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (32): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (contact_head): ContactPredictionHead(
        (regression): Linear(in_features=660, out_features=1, bias=True)
        (activation): Sigmoid()
      )
      (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (lm_head): RobertaLMHead(
        (dense): Linear(in_features=1280, out_features=1280, bias=True)
        (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (ss8): Sequential(
    (0): Linear(in_features=1280, out_features=8, bias=True)
  )
  (ss3): Sequential(
    (0): Linear(in_features=1280, out_features=3, bias=True)
  )
  (disorder): Sequential(
    (0): Linear(in_features=1280, out_features=2, bias=True)
  )
  (rsa): Sequential(
    (0): Linear(in_features=1280, out_features=1, bias=True)
    (1): Sigmoid()
  )
  (phi): Sequential(
    (0): Linear(in_features=1280, out_features=2, bias=True)
    (1): Tanh()
  )
  (psi): Sequential(
    (0): Linear(in_features=1280, out_features=2, bias=True)
    (1): Tanh()
  )
  (tasa): Sequential(
    (0): Linear(in_features=1280, out_features=1, bias=True)
  )
  (thsa): Sequential(
    (0): Linear(in_features=1280, out_features=1, bias=True)
  )
  (lhp): Sequential(
    (0): Linear(in_features=1280, out_features=1, bias=True)
  )
  (hp_loc): Sequential(
    (0): Linear(in_features=1280, out_features=2, bias=True)
  )
  (lhp_loc): Sequential(
    (0): Linear(in_features=1280, out_features=2, bias=True)
  )
  (species): Sequential(
    (0): Linear(in_features=1280, out_features=10, bias=True)
  )
  (expression): Sequential(
    (0): Linear(in_features=1280, out_features=2, bias=True)
  )
  (aggregation): Sequential(
    (0): Linear(in_features=1280, out_features=2, bias=True)
  )
)
Trainable parameters: 1063719
2024-02-10 19:41:10,177 - PROT.PROT.main - INFO - Using devices [0] of available devices [0]
Using devices [0] of available devices [0]
2024-02-10 19:41:11,026 - PROT.PROT.main - INFO - Building: torch.optim.Adam
2024-02-10 19:41:11,031 - PROT.PROT.models.ESM2_multitask.model - INFO - Params to learn:
2024-02-10 19:41:11,041 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.0.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,046 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.0.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,050 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.0.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,055 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.0.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,058 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.1.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,063 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.1.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,066 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.1.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,071 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.1.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,075 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.2.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,080 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.2.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,083 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.2.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,087 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.2.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,091 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.3.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,095 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.3.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,099 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.3.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,103 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.3.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,109 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.4.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,113 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.4.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,117 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.4.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,121 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.4.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,126 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.5.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,129 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.5.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,133 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.5.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,136 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.5.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,141 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.6.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,146 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.6.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,150 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.6.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,154 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.6.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,161 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.7.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,166 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.7.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,170 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.7.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,174 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.7.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,179 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.8.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,182 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.8.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,187 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.8.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,191 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.8.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,195 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.9.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,199 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.9.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,205 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.9.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,208 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.9.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,214 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.10.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,217 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.10.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,222 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.10.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,226 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.10.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,232 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.11.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,236 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.11.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,241 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.11.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,246 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.11.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,250 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.12.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,254 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.12.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,259 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.12.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,264 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.12.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,268 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.13.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,272 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.13.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,276 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.13.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,280 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.13.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,285 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.14.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,288 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.14.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,294 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.14.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,298 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.14.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,303 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.15.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,307 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.15.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,311 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.15.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,314 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.15.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,318 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.16.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,321 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.16.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,326 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.16.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,330 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.16.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,336 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.17.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,340 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.17.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,344 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.17.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,348 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.17.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,353 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.18.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,358 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.18.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,362 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.18.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,366 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.18.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,369 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.19.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,373 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.19.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,377 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.19.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,381 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.19.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,386 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.20.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,390 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.20.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,395 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.20.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,400 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.20.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,404 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.21.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,409 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.21.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,412 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.21.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,417 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.21.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,420 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.22.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,426 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.22.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,430 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.22.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,434 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.22.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,438 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.23.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,442 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.23.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,446 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.23.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,450 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.23.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,454 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.24.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,458 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.24.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,462 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.24.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,468 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.24.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,472 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.25.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,475 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.25.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,481 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.25.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,484 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.25.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,489 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.26.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,493 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.26.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,497 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.26.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,501 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.26.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,505 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.27.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,508 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.27.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,512 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.27.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,515 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.27.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,520 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.28.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,525 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.28.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,528 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.28.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,534 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.28.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,537 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.29.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,541 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.29.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,544 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.29.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,548 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.29.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,552 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.30.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,555 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.30.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,559 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.30.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,563 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.30.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,567 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.31.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,570 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.31.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,576 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.31.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,579 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.31.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,583 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.32.self_attn.k_LoRA.LoRA_A
2024-02-10 19:41:11,586 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.32.self_attn.k_LoRA.LoRA_B
2024-02-10 19:41:11,590 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.32.self_attn.v_LoRA.LoRA_A
2024-02-10 19:41:11,594 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.32.self_attn.v_LoRA.LoRA_B
2024-02-10 19:41:11,597 - PROT.PROT.models.ESM2_multitask.model - INFO - 	ss8.0.weight
2024-02-10 19:41:11,601 - PROT.PROT.models.ESM2_multitask.model - INFO - 	ss8.0.bias
2024-02-10 19:41:11,604 - PROT.PROT.models.ESM2_multitask.model - INFO - 	ss3.0.weight
2024-02-10 19:41:11,610 - PROT.PROT.models.ESM2_multitask.model - INFO - 	ss3.0.bias
2024-02-10 19:41:11,613 - PROT.PROT.models.ESM2_multitask.model - INFO - 	disorder.0.weight
2024-02-10 19:41:11,617 - PROT.PROT.models.ESM2_multitask.model - INFO - 	disorder.0.bias
2024-02-10 19:41:11,620 - PROT.PROT.models.ESM2_multitask.model - INFO - 	rsa.0.weight
2024-02-10 19:41:11,625 - PROT.PROT.models.ESM2_multitask.model - INFO - 	rsa.0.bias
2024-02-10 19:41:11,628 - PROT.PROT.models.ESM2_multitask.model - INFO - 	phi.0.weight
2024-02-10 19:41:11,633 - PROT.PROT.models.ESM2_multitask.model - INFO - 	phi.0.bias
2024-02-10 19:41:11,636 - PROT.PROT.models.ESM2_multitask.model - INFO - 	psi.0.weight
2024-02-10 19:41:11,641 - PROT.PROT.models.ESM2_multitask.model - INFO - 	psi.0.bias
2024-02-10 19:41:11,646 - PROT.PROT.models.ESM2_multitask.model - INFO - 	tasa.0.weight
2024-02-10 19:41:11,650 - PROT.PROT.models.ESM2_multitask.model - INFO - 	tasa.0.bias
2024-02-10 19:41:11,653 - PROT.PROT.models.ESM2_multitask.model - INFO - 	thsa.0.weight
2024-02-10 19:41:11,656 - PROT.PROT.models.ESM2_multitask.model - INFO - 	thsa.0.bias
2024-02-10 19:41:11,660 - PROT.PROT.models.ESM2_multitask.model - INFO - 	lhp.0.weight
2024-02-10 19:41:11,664 - PROT.PROT.models.ESM2_multitask.model - INFO - 	lhp.0.bias
2024-02-10 19:41:11,668 - PROT.PROT.models.ESM2_multitask.model - INFO - 	hp_loc.0.weight
2024-02-10 19:41:11,672 - PROT.PROT.models.ESM2_multitask.model - INFO - 	hp_loc.0.bias
2024-02-10 19:41:11,676 - PROT.PROT.models.ESM2_multitask.model - INFO - 	lhp_loc.0.weight
2024-02-10 19:41:11,679 - PROT.PROT.models.ESM2_multitask.model - INFO - 	lhp_loc.0.bias
2024-02-10 19:41:11,683 - PROT.PROT.models.ESM2_multitask.model - INFO - 	species.0.weight
2024-02-10 19:41:11,687 - PROT.PROT.models.ESM2_multitask.model - INFO - 	species.0.bias
2024-02-10 19:41:11,691 - PROT.PROT.models.ESM2_multitask.model - INFO - 	expression.0.weight
2024-02-10 19:41:11,695 - PROT.PROT.models.ESM2_multitask.model - INFO - 	expression.0.bias
2024-02-10 19:41:11,699 - PROT.PROT.models.ESM2_multitask.model - INFO - 	aggregation.0.weight
2024-02-10 19:41:11,702 - PROT.PROT.models.ESM2_multitask.model - INFO - 	aggregation.0.bias
2024-02-10 19:41:11,705 - PROT.PROT.main - INFO - Building: PROT.data_loader.augmentation.sparse_token
FINETUNING CHOSEN: no finetuning
Gradient Checkpointing None
2024-02-10 19:41:13,029 - PROT.PROT.main - INFO - Building: PROT.data_loader.data_loaders.NSPDataLoader
2024-02-10 19:45:15,292 - PROT.PROT.main - INFO - Getting loss and metric function handles
2024-02-10 19:45:15,293 - PROT.PROT.main - INFO - Initialising trainer
GRADIENT ACCUMULATION 6
2024-02-10 19:45:15,335 - PROT.PROT.base.base_trainer - INFO - Starting training...
Multi Task Loss
SS8 : 1 // SS3: 5 // DIS: 5 // RSA: 100 // PHI: 5 // PSI: 5
2024-02-10 19:45:17,085 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [0/10308 (0%)] Loss: 18.115793
2024-02-10 19:49:06,096 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [900/10308 (9%)] Loss: 11.165866
2024-02-10 19:53:01,922 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [1800/10308 (17%)] Loss: 9.779931
2024-02-10 19:56:49,698 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [2700/10308 (26%)] Loss: 8.323317
2024-02-10 20:00:39,972 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [3600/10308 (35%)] Loss: 8.624589
2024-02-10 20:04:30,125 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [4500/10308 (44%)] Loss: 8.327094
2024-02-10 20:08:22,154 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [5400/10308 (52%)] Loss: 7.713501
2024-02-10 20:12:09,586 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [6300/10308 (61%)] Loss: 8.199673
2024-02-10 20:16:04,623 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [7200/10308 (70%)] Loss: 7.711205
2024-02-10 20:19:42,851 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [8100/10308 (79%)] Loss: 7.125294
2024-02-10 20:23:35,129 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [9000/10308 (87%)] Loss: 6.496620
2024-02-10 20:27:18,399 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [9900/10308 (96%)] Loss: 7.594860
2024-02-10 20:30:01,240 - PROT.PROT.base.base_trainer - INFO - epoch          : 0
2024-02-10 20:30:01,243 - PROT.PROT.base.base_trainer - INFO - loss           : 8.589665628650131
2024-02-10 20:30:01,249 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.6236426383256912
2024-02-10 20:30:01,255 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.7428041473031044
2024-02-10 20:30:01,260 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.5477143964833684
2024-02-10 20:30:01,264 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.5497076654185852
2024-02-10 20:30:01,269 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.6759779155254364
2024-02-10 20:30:01,275 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.702446182568868
2024-02-10 20:30:01,281 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 25.533004919687908
2024-02-10 20:30:01,287 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 34.929515520731606
2024-02-10 20:30:01,294 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.40634188968757
2024-02-10 20:30:01,301 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7407668453960841
2024-02-10 20:30:01,308 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8549524179463897
2024-02-10 20:30:01,314 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.5893162963018371
2024-02-10 20:30:01,321 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.45853243390874887
2024-02-10 20:30:01,330 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.7823302080490493
2024-02-10 20:30:01,334 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.7988962207773075
2024-02-10 20:30:01,339 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 17.4018186009678
2024-02-10 20:30:01,347 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 24.87452549248164
2024-02-10 20:30:29,817 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-194515/checkpoints/checkpoint-epoch0.pth ...
2024-02-10 20:30:56,054 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2/0210-194515/checkpoints/model_best.pth
2024-02-10 20:30:57,451 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [0/10308 (0%)] Loss: 7.102773
2024-02-10 20:34:41,598 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [900/10308 (9%)] Loss: 6.552651
2024-02-10 20:38:27,837 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [1800/10308 (17%)] Loss: 7.594704
2024-02-10 20:42:20,930 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [2700/10308 (26%)] Loss: 8.017306
2024-02-10 20:46:10,832 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [3600/10308 (35%)] Loss: 7.907477
2024-02-10 20:50:09,179 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [4500/10308 (44%)] Loss: 7.697969
2024-02-10 20:54:01,328 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [5400/10308 (52%)] Loss: 7.999687
2024-02-10 20:57:49,965 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [6300/10308 (61%)] Loss: 7.233879
2024-02-10 21:01:50,834 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [7200/10308 (70%)] Loss: 7.056090
2024-02-10 21:05:38,447 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [8100/10308 (79%)] Loss: 8.790389
2024-02-10 21:09:30,723 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [9000/10308 (87%)] Loss: 7.216467
2024-02-10 21:13:20,839 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [9900/10308 (96%)] Loss: 7.336596
2024-02-10 21:16:06,509 - PROT.PROT.base.base_trainer - INFO - epoch          : 1
2024-02-10 21:16:06,511 - PROT.PROT.base.base_trainer - INFO - loss           : 7.324551914262466
2024-02-10 21:16:06,512 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7266990393400192
2024-02-10 21:16:06,515 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8371924261252085
2024-02-10 21:16:06,516 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.5088566554089388
2024-02-10 21:16:06,519 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.5239592306315899
2024-02-10 21:16:06,520 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.7796091139316559
2024-02-10 21:16:06,522 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.7964350432157516
2024-02-10 21:16:06,523 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 17.487934192021687
2024-02-10 21:16:06,526 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 24.646286884943645
2024-02-10 21:16:06,527 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.270042388641526
2024-02-10 21:16:06,530 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7506924774154086
2024-02-10 21:16:06,532 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8610248240157687
2024-02-10 21:16:06,535 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.5754005296470283
2024-02-10 21:16:06,536 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.4903023412420046
2024-02-10 21:16:06,540 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.7945247409088585
2024-02-10 21:16:06,542 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8088627489510497
2024-02-10 21:16:06,543 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 17.00775102143798
2024-02-10 21:16:06,545 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 24.103260193363766
2024-02-10 21:16:29,042 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-194515/checkpoints/checkpoint-epoch1.pth ...
2024-02-10 21:16:54,133 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2/0210-194515/checkpoints/model_best.pth
2024-02-10 21:16:55,322 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [0/10308 (0%)] Loss: 7.144250
2024-02-10 21:20:41,688 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [900/10308 (9%)] Loss: 7.458909
2024-02-10 21:24:37,868 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [1800/10308 (17%)] Loss: 7.462125
2024-02-10 21:28:24,868 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [2700/10308 (26%)] Loss: 6.761779
2024-02-10 21:32:23,080 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [3600/10308 (35%)] Loss: 7.078034
2024-02-10 21:36:30,641 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [4500/10308 (44%)] Loss: 7.384952
2024-02-10 21:40:16,476 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [5400/10308 (52%)] Loss: 8.514411
2024-02-10 21:44:10,003 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [6300/10308 (61%)] Loss: 7.077474
2024-02-10 21:47:59,768 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [7200/10308 (70%)] Loss: 7.690649
2024-02-10 21:51:54,511 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [8100/10308 (79%)] Loss: 7.465357
2024-02-10 21:55:33,891 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [9000/10308 (87%)] Loss: 6.774988
2024-02-10 21:59:19,852 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [9900/10308 (96%)] Loss: 7.412054
2024-02-10 22:02:01,924 - PROT.PROT.base.base_trainer - INFO - epoch          : 2
2024-02-10 22:02:01,926 - PROT.PROT.base.base_trainer - INFO - loss           : 7.244757600647719
2024-02-10 22:02:01,928 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7478476613759995
2024-02-10 22:02:01,931 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.860484262307485
2024-02-10 22:02:01,933 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.5856843578318754
2024-02-10 22:02:01,935 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.5039443808297316
2024-02-10 22:02:01,938 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.7671482612689337
2024-02-10 22:02:01,940 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.7921172678470612
2024-02-10 22:02:01,942 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 16.936747074127197
2024-02-10 22:02:01,944 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 24.46628014246623
2024-02-10 22:02:01,946 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.200635289793965
2024-02-10 22:02:01,949 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7548352973707487
2024-02-10 22:02:01,951 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8653614339573357
2024-02-10 22:02:01,953 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6061106648320174
2024-02-10 22:02:01,955 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.45052245367882
2024-02-10 22:02:01,958 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.7995725189407813
2024-02-10 22:02:01,960 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8138659071878314
2024-02-10 22:02:01,964 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.91845928171024
2024-02-10 22:02:01,966 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.815132283636565
2024-02-10 22:02:24,148 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-194515/checkpoints/checkpoint-epoch2.pth ...
2024-02-10 22:02:49,296 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2/0210-194515/checkpoints/model_best.pth
2024-02-10 22:02:51,312 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [0/10308 (0%)] Loss: 7.025341
2024-02-10 22:06:45,100 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [900/10308 (9%)] Loss: 6.637883
2024-02-10 22:10:32,065 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [1800/10308 (17%)] Loss: 7.033560
2024-02-10 22:14:31,955 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [2700/10308 (26%)] Loss: 7.022506
2024-02-10 22:18:14,463 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [3600/10308 (35%)] Loss: 7.366221
2024-02-10 22:22:06,440 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [4500/10308 (44%)] Loss: 6.463048
2024-02-10 22:25:53,335 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [5400/10308 (52%)] Loss: 7.070384
2024-02-10 22:29:37,603 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [6300/10308 (61%)] Loss: 6.610994
2024-02-10 22:33:32,587 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [7200/10308 (70%)] Loss: 6.830008
2024-02-10 22:37:26,749 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [8100/10308 (79%)] Loss: 6.965880
2024-02-10 22:41:08,488 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [9000/10308 (87%)] Loss: 6.455054
2024-02-10 22:45:01,230 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [9900/10308 (96%)] Loss: 6.641123
2024-02-10 22:47:46,614 - PROT.PROT.base.base_trainer - INFO - epoch          : 3
2024-02-10 22:47:46,618 - PROT.PROT.base.base_trainer - INFO - loss           : 7.207758497733781
2024-02-10 22:47:46,621 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7731903791427612
2024-02-10 22:47:46,622 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8852841456731161
2024-02-10 22:47:46,626 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.6116568769017855
2024-02-10 22:47:46,627 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.4254645562420289
2024-02-10 22:47:46,632 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.806306799252828
2024-02-10 22:47:46,634 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8219922532637914
2024-02-10 22:47:46,636 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 15.505714416503906
2024-02-10 22:47:46,639 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 21.2827787399292
2024-02-10 22:47:46,641 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.198224094960962
2024-02-10 22:47:46,643 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7547388409996385
2024-02-10 22:47:46,645 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8652536225714806
2024-02-10 22:47:46,648 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.5960232053687486
2024-02-10 22:47:46,649 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.4341673211596659
2024-02-10 22:47:46,654 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8024158129173011
2024-02-10 22:47:46,656 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8164909194976201
2024-02-10 22:47:46,660 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.808258867791658
2024-02-10 22:47:46,661 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.710006787768148
2024-02-10 22:48:15,949 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-194515/checkpoints/checkpoint-epoch3.pth ...
2024-02-10 22:48:48,178 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2/0210-194515/checkpoints/model_best.pth
2024-02-10 22:48:49,266 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [0/10308 (0%)] Loss: 7.121641
2024-02-10 22:52:46,680 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [900/10308 (9%)] Loss: 7.027973
2024-02-10 22:56:38,403 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [1800/10308 (17%)] Loss: 7.830867
2024-02-10 23:00:43,299 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [2700/10308 (26%)] Loss: 6.679676
2024-02-10 23:04:37,308 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [3600/10308 (35%)] Loss: 6.359759
2024-02-10 23:08:24,384 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [4500/10308 (44%)] Loss: 7.020953
2024-02-10 23:12:09,364 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [5400/10308 (52%)] Loss: 7.544752
2024-02-10 23:15:59,995 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [6300/10308 (61%)] Loss: 6.857681
2024-02-10 23:19:52,835 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [7200/10308 (70%)] Loss: 7.484266
2024-02-10 23:23:46,100 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [8100/10308 (79%)] Loss: 8.513001
2024-02-10 23:27:29,196 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [9000/10308 (87%)] Loss: 6.773790
2024-02-10 23:31:19,521 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [9900/10308 (96%)] Loss: 6.484339
2024-02-10 23:34:01,787 - PROT.PROT.base.base_trainer - INFO - epoch          : 4
2024-02-10 23:34:01,788 - PROT.PROT.base.base_trainer - INFO - loss           : 7.188483010998472
2024-02-10 23:34:01,792 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7575046320756277
2024-02-10 23:34:01,797 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8690759340922037
2024-02-10 23:34:01,802 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.6655804614226023
2024-02-10 23:34:01,808 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.38938669736186665
2024-02-10 23:34:01,813 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.7943035066127777
2024-02-10 23:34:01,817 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8090716203053793
2024-02-10 23:34:01,820 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 17.182021935780842
2024-02-10 23:34:01,823 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 24.213478724161785
2024-02-10 23:34:01,829 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.1926325097735075
2024-02-10 23:34:01,833 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7542142843848225
2024-02-10 23:34:01,836 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8654321315543678
2024-02-10 23:34:01,842 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6056819941717067
2024-02-10 23:34:01,847 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.435345410321224
2024-02-10 23:34:01,851 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8032334060906484
2024-02-10 23:34:01,855 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8175104615653133
2024-02-10 23:34:01,859 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.800646776642747
2024-02-10 23:34:01,862 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.786061716255666
2024-02-10 23:34:33,487 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-194515/checkpoints/checkpoint-epoch4.pth ...
2024-02-10 23:35:14,632 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2/0210-194515/checkpoints/model_best.pth
2024-02-10 23:35:15,890 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [0/10308 (0%)] Loss: 7.046811
2024-02-10 23:39:09,220 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [900/10308 (9%)] Loss: 7.723793
2024-02-10 23:43:05,648 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [1800/10308 (17%)] Loss: 7.017856
2024-02-10 23:46:56,330 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [2700/10308 (26%)] Loss: 7.290160
2024-02-10 23:50:42,532 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [3600/10308 (35%)] Loss: 6.232493
2024-02-10 23:54:26,148 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [4500/10308 (44%)] Loss: 7.672451
2024-02-10 23:58:20,535 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [5400/10308 (52%)] Loss: 6.793922
2024-02-11 00:01:59,845 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [6300/10308 (61%)] Loss: 7.063569
2024-02-11 00:05:56,684 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [7200/10308 (70%)] Loss: 7.053759
2024-02-11 00:09:48,294 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [8100/10308 (79%)] Loss: 6.636302
2024-02-11 00:13:40,416 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [9000/10308 (87%)] Loss: 7.495145
2024-02-11 00:17:36,227 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [9900/10308 (96%)] Loss: 6.991561
2024-02-11 00:20:24,707 - PROT.PROT.base.base_trainer - INFO - epoch          : 5
2024-02-11 00:20:24,710 - PROT.PROT.base.base_trainer - INFO - loss           : 7.170280085104126
2024-02-11 00:20:24,724 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7707534730434418
2024-02-11 00:20:24,744 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8755135436852773
2024-02-11 00:20:24,758 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.5683841925735275
2024-02-11 00:20:24,779 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.522100418806076
2024-02-11 00:20:24,804 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8097355216741562
2024-02-11 00:20:24,813 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.822766641775767
2024-02-11 00:20:24,831 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 16.35551079114278
2024-02-11 00:20:24,848 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 22.28730058670044
2024-02-11 00:20:24,863 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.166887614119976
2024-02-11 00:20:24,879 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7565868780621743
2024-02-11 00:20:24,884 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8666020967203752
2024-02-11 00:20:24,889 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6224967052488213
2024-02-11 00:20:24,895 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.37877311749638115
2024-02-11 00:20:24,900 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8052038827505499
2024-02-11 00:20:24,904 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.818885105892301
2024-02-11 00:20:24,908 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.73500549309368
2024-02-11 00:20:24,912 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.60705026401365
2024-02-11 00:20:52,762 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-194515/checkpoints/checkpoint-epoch5.pth ...
2024-02-11 00:21:26,209 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2/0210-194515/checkpoints/model_best.pth
2024-02-11 00:21:27,907 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [0/10308 (0%)] Loss: 6.779857
2024-02-11 00:25:27,038 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [900/10308 (9%)] Loss: 8.511395
2024-02-11 00:29:16,863 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [1800/10308 (17%)] Loss: 7.865260
2024-02-11 00:33:04,675 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [2700/10308 (26%)] Loss: 6.842913
2024-02-11 00:37:02,080 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [3600/10308 (35%)] Loss: 6.816368
2024-02-11 00:40:53,861 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [4500/10308 (44%)] Loss: 7.327765
2024-02-11 00:44:44,187 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [5400/10308 (52%)] Loss: 7.248468
2024-02-11 00:48:34,915 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [6300/10308 (61%)] Loss: 6.660727
2024-02-11 00:52:22,284 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [7200/10308 (70%)] Loss: 6.693526
2024-02-11 00:56:18,346 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [8100/10308 (79%)] Loss: 7.627052
2024-02-11 01:00:04,977 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [9000/10308 (87%)] Loss: 6.765369
2024-02-11 01:03:50,810 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [9900/10308 (96%)] Loss: 7.415771
2024-02-11 01:06:38,608 - PROT.PROT.base.base_trainer - INFO - epoch          : 6
2024-02-11 01:06:38,612 - PROT.PROT.base.base_trainer - INFO - loss           : 7.166595537501031
2024-02-11 01:06:38,617 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7727368275324503
2024-02-11 01:06:38,620 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8681228955586752
2024-02-11 01:06:38,625 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.4986113924533129
2024-02-11 01:06:38,629 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.47288078255951405
2024-02-11 01:06:38,632 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8092453529437383
2024-02-11 01:06:38,635 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.821285143494606
2024-02-11 01:06:38,640 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 16.62682302792867
2024-02-11 01:06:38,642 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 22.88830312093099
2024-02-11 01:06:38,646 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.168245682417247
2024-02-11 01:06:38,650 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7573369077210936
2024-02-11 01:06:38,654 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.86736468239464
2024-02-11 01:06:38,658 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6020870471649847
2024-02-11 01:06:38,663 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.4517746522500286
2024-02-11 01:06:38,668 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8025154858497676
2024-02-11 01:06:38,672 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8173423370971891
2024-02-11 01:06:38,675 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.652820518535883
2024-02-11 01:06:38,679 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.470732442567268
2024-02-11 01:07:07,130 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-194515/checkpoints/checkpoint-epoch6.pth ...
2024-02-11 01:07:10,091 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [0/10308 (0%)] Loss: 6.714916
2024-02-11 01:11:07,472 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [900/10308 (9%)] Loss: 7.457338
2024-02-11 01:14:59,006 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [1800/10308 (17%)] Loss: 6.258616
2024-02-11 01:18:41,140 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [2700/10308 (26%)] Loss: 6.444286
2024-02-11 01:22:32,680 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [3600/10308 (35%)] Loss: 6.253822
2024-02-11 01:26:28,929 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [4500/10308 (44%)] Loss: 6.819139
2024-02-11 01:30:14,403 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [5400/10308 (52%)] Loss: 6.316023
2024-02-11 01:33:58,244 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [6300/10308 (61%)] Loss: 7.380191
2024-02-11 01:37:53,681 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [7200/10308 (70%)] Loss: 7.195103
2024-02-11 01:41:55,695 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [8100/10308 (79%)] Loss: 6.950284
2024-02-11 01:45:54,644 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [9000/10308 (87%)] Loss: 7.763778
2024-02-11 01:49:50,944 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [9900/10308 (96%)] Loss: 7.184973
2024-02-11 01:52:37,561 - PROT.PROT.base.base_trainer - INFO - epoch          : 7
2024-02-11 01:52:37,567 - PROT.PROT.base.base_trainer - INFO - loss           : 7.155390981884806
2024-02-11 01:52:37,571 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7701477209726969
2024-02-11 01:52:37,574 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8811593453089396
2024-02-11 01:52:37,579 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.5961206052452326
2024-02-11 01:52:37,582 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.43056027342875797
2024-02-11 01:52:37,584 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8229891111453375
2024-02-11 01:52:37,586 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8314000219106674
2024-02-11 01:52:37,589 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 16.332430919011433
2024-02-11 01:52:37,593 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 22.880348682403564
2024-02-11 01:52:37,595 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.1663735783847935
2024-02-11 01:52:37,599 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7576082021107973
2024-02-11 01:52:37,602 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8678038748208007
2024-02-11 01:52:37,606 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.5993550358584144
2024-02-11 01:52:37,609 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.4538858292754946
2024-02-11 01:52:37,613 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8050325925279808
2024-02-11 01:52:37,616 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8187433778139938
2024-02-11 01:52:37,618 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.802294428498104
2024-02-11 01:52:37,621 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.61697809458659
2024-02-11 01:53:00,073 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-194515/checkpoints/checkpoint-epoch7.pth ...
2024-02-11 01:53:25,601 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2/0210-194515/checkpoints/model_best.pth
2024-02-11 01:53:27,046 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [0/10308 (0%)] Loss: 6.475063
2024-02-11 01:57:26,639 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [900/10308 (9%)] Loss: 6.451890
2024-02-11 02:01:25,330 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [1800/10308 (17%)] Loss: 7.270342
2024-02-11 02:05:23,892 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [2700/10308 (26%)] Loss: 7.644993
2024-02-11 02:09:10,469 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [3600/10308 (35%)] Loss: 7.085643
2024-02-11 02:12:59,331 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [4500/10308 (44%)] Loss: 6.287829
2024-02-11 02:16:45,649 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [5400/10308 (52%)] Loss: 6.986277
2024-02-11 02:20:33,202 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [6300/10308 (61%)] Loss: 7.421759
2024-02-11 02:24:15,156 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [7200/10308 (70%)] Loss: 6.749977
2024-02-11 02:28:07,181 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [8100/10308 (79%)] Loss: 7.323147
2024-02-11 02:32:01,335 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [9000/10308 (87%)] Loss: 7.741516
2024-02-11 02:35:53,290 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [9900/10308 (96%)] Loss: 6.570879
2024-02-11 02:38:48,636 - PROT.PROT.base.base_trainer - INFO - epoch          : 8
2024-02-11 02:38:48,639 - PROT.PROT.base.base_trainer - INFO - loss           : 7.151250331766879
2024-02-11 02:38:48,642 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7637484868367513
2024-02-11 02:38:48,646 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.866581529378891
2024-02-11 02:38:48,648 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.6605889946222305
2024-02-11 02:38:48,649 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.34515622196098167
2024-02-11 02:38:48,654 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8093244334061941
2024-02-11 02:38:48,656 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8229485402504603
2024-02-11 02:38:48,658 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 17.132532755533855
2024-02-11 02:38:48,661 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 23.139724254608154
2024-02-11 02:38:48,664 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.16272192687566
2024-02-11 02:38:48,666 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.757842151551229
2024-02-11 02:38:48,668 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8671168818025131
2024-02-11 02:38:48,672 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6016123233280554
2024-02-11 02:38:48,673 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.4485857824200414
2024-02-11 02:38:48,676 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.803535521360341
2024-02-11 02:38:48,677 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8173374951545602
2024-02-11 02:38:48,680 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.657769819027383
2024-02-11 02:38:48,682 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.449405360485795
2024-02-11 02:39:12,963 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-194515/checkpoints/checkpoint-epoch8.pth ...
2024-02-11 02:39:38,556 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2/0210-194515/checkpoints/model_best.pth
2024-02-11 02:39:39,645 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [0/10308 (0%)] Loss: 5.986254
2024-02-11 02:43:27,163 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [900/10308 (9%)] Loss: 7.612273
2024-02-11 02:47:20,047 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [1800/10308 (17%)] Loss: 6.972088
2024-02-11 02:51:07,438 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [2700/10308 (26%)] Loss: 7.654994
2024-02-11 02:54:51,666 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [3600/10308 (35%)] Loss: 7.782744
2024-02-11 02:58:45,840 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [4500/10308 (44%)] Loss: 6.575539
2024-02-11 03:02:46,164 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [5400/10308 (52%)] Loss: 8.351490
2024-02-11 03:06:46,616 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [6300/10308 (61%)] Loss: 7.271519
2024-02-11 03:10:31,425 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [7200/10308 (70%)] Loss: 7.384973
2024-02-11 03:14:20,941 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [8100/10308 (79%)] Loss: 6.440675
2024-02-11 03:18:13,954 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [9000/10308 (87%)] Loss: 6.692022
2024-02-11 03:22:11,054 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [9900/10308 (96%)] Loss: 6.698033
2024-02-11 03:24:56,891 - PROT.PROT.base.base_trainer - INFO - epoch          : 9
2024-02-11 03:24:56,893 - PROT.PROT.base.base_trainer - INFO - loss           : 7.143187146313602
2024-02-11 03:24:56,896 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.752467542886734
2024-02-11 03:24:56,900 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8596705347299576
2024-02-11 03:24:56,903 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.6106641764442126
2024-02-11 03:24:56,904 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.46507096476852894
2024-02-11 03:24:56,907 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.7998935580253601
2024-02-11 03:24:56,909 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8125814745823542
2024-02-11 03:24:56,911 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 16.72653341293335
2024-02-11 03:24:56,912 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 23.75853657722473
2024-02-11 03:24:56,915 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.132909847801462
2024-02-11 03:24:56,916 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7592881980637343
2024-02-11 03:24:56,919 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8686055715893467
2024-02-11 03:24:56,920 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6113151887541712
2024-02-11 03:24:56,923 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.4251328236956108
2024-02-11 03:24:56,925 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8034342138639675
2024-02-11 03:24:56,935 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8184070935768395
2024-02-11 03:24:56,937 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.603196620941162
2024-02-11 03:24:56,940 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.365145180058214
2024-02-11 03:25:22,099 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-194515/checkpoints/checkpoint-epoch9.pth ...
2024-02-11 03:25:43,082 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2/0210-194515/checkpoints/model_best.pth
2024-02-11 03:25:44,625 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [0/10308 (0%)] Loss: 7.339490
2024-02-11 03:29:37,855 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [900/10308 (9%)] Loss: 6.110760
2024-02-11 03:33:22,291 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [1800/10308 (17%)] Loss: 6.527623
2024-02-11 03:37:12,930 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [2700/10308 (26%)] Loss: 7.102221
2024-02-11 03:41:04,913 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [3600/10308 (35%)] Loss: 7.425904
2024-02-11 03:45:02,856 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [4500/10308 (44%)] Loss: 7.797857
2024-02-11 03:48:45,569 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [5400/10308 (52%)] Loss: 6.804088
2024-02-11 03:52:46,731 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [6300/10308 (61%)] Loss: 7.065597
2024-02-11 03:56:39,891 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [7200/10308 (70%)] Loss: 5.939116
2024-02-11 04:00:29,865 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [8100/10308 (79%)] Loss: 6.898686
2024-02-11 04:04:22,518 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [9000/10308 (87%)] Loss: 7.023707
2024-02-11 04:08:13,231 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [9900/10308 (96%)] Loss: 6.662947
2024-02-11 04:11:04,399 - PROT.PROT.base.base_trainer - INFO - epoch          : 10
2024-02-11 04:11:04,403 - PROT.PROT.base.base_trainer - INFO - loss           : 7.148285734336538
2024-02-11 04:11:04,415 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7693252811829249
2024-02-11 04:11:04,421 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8809538086255392
2024-02-11 04:11:04,428 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.5933395822842916
2024-02-11 04:11:04,435 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.45307022146880627
2024-02-11 04:11:04,448 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8221724728743235
2024-02-11 04:11:04,461 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8304256498813629
2024-02-11 04:11:04,470 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 15.997829834620157
2024-02-11 04:11:04,479 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 22.09773127237956
2024-02-11 04:11:04,483 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.163923816927245
2024-02-11 04:11:04,491 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7579625711889725
2024-02-11 04:11:04,495 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8679815900281787
2024-02-11 04:11:04,506 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.5933481923240567
2024-02-11 04:11:04,508 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.4698817775038336
2024-02-11 04:11:04,518 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8038974484614341
2024-02-11 04:11:04,535 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8184594858396537
2024-02-11 04:11:04,541 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.71443820615536
2024-02-11 04:11:04,548 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.406858326324237
2024-02-11 04:11:35,433 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-194515/checkpoints/checkpoint-epoch10.pth ...
2024-02-11 04:11:37,879 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [0/10308 (0%)] Loss: 6.382936
2024-02-11 04:15:29,646 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [900/10308 (9%)] Loss: 7.281025
2024-02-11 04:19:20,071 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [1800/10308 (17%)] Loss: 6.713120
2024-02-11 04:23:15,011 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [2700/10308 (26%)] Loss: 7.639332
2024-02-11 04:27:07,813 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [3600/10308 (35%)] Loss: 7.331603
2024-02-11 04:30:58,186 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [4500/10308 (44%)] Loss: 9.699711
2024-02-11 04:34:52,341 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [5400/10308 (52%)] Loss: 7.144632
2024-02-11 04:38:49,495 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [6300/10308 (61%)] Loss: 7.226829
2024-02-11 04:42:46,558 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [7200/10308 (70%)] Loss: 7.562508
2024-02-11 04:46:40,621 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [8100/10308 (79%)] Loss: 7.659032
2024-02-11 04:50:25,148 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [9000/10308 (87%)] Loss: 7.637859
2024-02-11 04:54:07,982 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [9900/10308 (96%)] Loss: 7.015586
2024-02-11 04:56:59,501 - PROT.PROT.base.base_trainer - INFO - epoch          : 11
2024-02-11 04:56:59,504 - PROT.PROT.base.base_trainer - INFO - loss           : 7.142890603292944
2024-02-11 04:56:59,513 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7371686299641927
2024-02-11 04:56:59,519 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8575151364008585
2024-02-11 04:56:59,526 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.586901975174745
2024-02-11 04:56:59,530 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.43747459538280964
2024-02-11 04:56:59,537 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.7998228271802267
2024-02-11 04:56:59,546 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8148300796747208
2024-02-11 04:56:59,552 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 18.007967233657837
2024-02-11 04:56:59,559 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 25.489217201868694
2024-02-11 04:56:59,565 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.146126583493504
2024-02-11 04:56:59,571 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7565511770793872
2024-02-11 04:56:59,581 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8665703758322445
2024-02-11 04:56:59,586 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6101522549731114
2024-02-11 04:56:59,593 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.4581014939078656
2024-02-11 04:56:59,601 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8058205919951971
2024-02-11 04:56:59,607 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8198402911094722
2024-02-11 04:56:59,614 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.66631536202237
2024-02-11 04:56:59,621 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.582556884667092
2024-02-11 04:57:32,010 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-194515/checkpoints/checkpoint-epoch11.pth ...
2024-02-11 04:57:33,320 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [0/10308 (0%)] Loss: 6.744005
2024-02-11 05:01:13,216 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [900/10308 (9%)] Loss: 6.263574
2024-02-11 05:05:14,838 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [1800/10308 (17%)] Loss: 6.800450
2024-02-11 05:09:09,443 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [2700/10308 (26%)] Loss: 6.893493
2024-02-11 05:13:03,412 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [3600/10308 (35%)] Loss: 6.529437
2024-02-11 05:16:56,984 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [4500/10308 (44%)] Loss: 6.482057
2024-02-11 05:20:42,876 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [5400/10308 (52%)] Loss: 7.052196
2024-02-11 05:24:37,430 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [6300/10308 (61%)] Loss: 7.622387
2024-02-11 05:28:28,514 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [7200/10308 (70%)] Loss: 6.563409
2024-02-11 05:32:20,739 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [8100/10308 (79%)] Loss: 6.626728
2024-02-11 05:36:12,957 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [9000/10308 (87%)] Loss: 7.649254
2024-02-11 05:40:01,136 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [9900/10308 (96%)] Loss: 8.755409
2024-02-11 05:42:48,271 - PROT.PROT.base.base_trainer - INFO - epoch          : 12
2024-02-11 05:42:48,274 - PROT.PROT.base.base_trainer - INFO - loss           : 7.138028837143276
2024-02-11 05:42:48,276 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7561140209436417
2024-02-11 05:42:48,278 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8700909266869227
2024-02-11 05:42:48,280 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.598797446427246
2024-02-11 05:42:48,282 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.475899759059151
2024-02-11 05:42:48,283 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8009106814861298
2024-02-11 05:42:48,285 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8164268235365549
2024-02-11 05:42:48,286 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 16.27207310994466
2024-02-11 05:42:48,288 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 22.861183643341064
2024-02-11 05:42:48,289 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.154700841411014
2024-02-11 05:42:48,291 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7580896358648349
2024-02-11 05:42:48,293 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8677797657317341
2024-02-11 05:42:48,294 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6014139240812097
2024-02-11 05:42:48,295 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.44805721453360087
2024-02-11 05:42:48,297 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8064289162299729
2024-02-11 05:42:48,298 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.819934331183064
2024-02-11 05:42:48,300 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.687592291743993
2024-02-11 05:42:48,301 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.587042588589377
2024-02-11 05:43:06,538 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-194515/checkpoints/checkpoint-epoch12.pth ...
2024-02-11 05:43:08,799 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [0/10308 (0%)] Loss: 7.292678
2024-02-11 05:46:57,471 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [900/10308 (9%)] Loss: 6.754226
2024-02-11 05:50:44,856 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [1800/10308 (17%)] Loss: 7.088209
2024-02-11 05:54:31,653 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [2700/10308 (26%)] Loss: 7.307934
2024-02-11 05:58:19,016 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [3600/10308 (35%)] Loss: 7.127949
2024-02-11 06:02:15,004 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [4500/10308 (44%)] Loss: 7.538525
2024-02-11 06:06:03,217 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [5400/10308 (52%)] Loss: 6.840621
2024-02-11 06:10:03,305 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [6300/10308 (61%)] Loss: 6.360968
2024-02-11 06:13:56,723 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [7200/10308 (70%)] Loss: 8.102211
2024-02-11 06:17:50,259 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [8100/10308 (79%)] Loss: 6.437016
2024-02-11 06:21:42,419 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [9000/10308 (87%)] Loss: 7.388878
2024-02-11 06:25:38,005 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [9900/10308 (96%)] Loss: 6.299106
2024-02-11 06:28:26,488 - PROT.PROT.base.base_trainer - INFO - epoch          : 13
2024-02-11 06:28:26,492 - PROT.PROT.base.base_trainer - INFO - loss           : 7.138790554336823
2024-02-11 06:28:26,494 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7692925383647283
2024-02-11 06:28:26,496 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8739237934350967
2024-02-11 06:28:26,498 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.5258155291279157
2024-02-11 06:28:26,500 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.5351480878889561
2024-02-11 06:28:26,503 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.7897026439507803
2024-02-11 06:28:26,504 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8090554773807526
2024-02-11 06:28:26,507 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 16.806174357732136
2024-02-11 06:28:26,509 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 23.106478373209637
2024-02-11 06:28:26,511 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.150990204617546
2024-02-11 06:28:26,512 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7569827725526591
2024-02-11 06:28:26,515 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.867684928253568
2024-02-11 06:28:26,516 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.609043421205617
2024-02-11 06:28:26,518 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.42119062872662283
2024-02-11 06:28:26,519 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8064065516214969
2024-02-11 06:28:26,521 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8197826037107798
2024-02-11 06:28:26,526 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.835228115870063
2024-02-11 06:28:26,528 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.609820154760158
2024-02-11 06:28:46,403 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-194515/checkpoints/checkpoint-epoch13.pth ...
2024-02-11 06:28:48,457 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [0/10308 (0%)] Loss: 8.272753
2024-02-11 06:32:37,240 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [900/10308 (9%)] Loss: 7.463419
2024-02-11 06:36:24,667 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [1800/10308 (17%)] Loss: 6.445384
2024-02-11 06:40:30,464 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [2700/10308 (26%)] Loss: 6.828660
2024-02-11 06:44:31,438 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [3600/10308 (35%)] Loss: 6.756047
2024-02-11 06:48:12,763 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [4500/10308 (44%)] Loss: 7.344326
2024-02-11 06:52:03,239 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [5400/10308 (52%)] Loss: 7.116775
2024-02-11 06:55:50,217 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [6300/10308 (61%)] Loss: 6.786030
2024-02-11 06:59:45,501 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [7200/10308 (70%)] Loss: 7.029696
2024-02-11 07:03:29,671 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [8100/10308 (79%)] Loss: 6.267576
2024-02-11 07:07:20,768 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [9000/10308 (87%)] Loss: 7.105925
2024-02-11 07:11:04,694 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [9900/10308 (96%)] Loss: 6.577394
2024-02-11 07:13:49,971 - PROT.PROT.base.base_trainer - INFO - epoch          : 14
2024-02-11 07:13:49,973 - PROT.PROT.base.base_trainer - INFO - loss           : 7.130496340725183
2024-02-11 07:13:49,976 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7715437809626261
2024-02-11 07:13:49,978 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8814374208450317
2024-02-11 07:13:49,981 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.6372526362538338
2024-02-11 07:13:49,983 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.41405095625668764
2024-02-11 07:13:49,986 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8246941516796747
2024-02-11 07:13:49,988 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8339541306098303
2024-02-11 07:13:49,991 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 16.2511146068573
2024-02-11 07:13:49,992 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 21.829354921976726
2024-02-11 07:13:49,995 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.177780472484462
2024-02-11 07:13:49,996 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7559843254705196
2024-02-11 07:13:49,999 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8666058273992855
2024-02-11 07:13:50,001 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6117194192345792
2024-02-11 07:13:50,005 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.4185877493919601
2024-02-11 07:13:50,006 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8045626413558242
2024-02-11 07:13:50,009 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8180055401641945
2024-02-11 07:13:50,010 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.757822346423385
2024-02-11 07:13:50,013 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.759063414542
2024-02-11 07:14:12,975 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-194515/checkpoints/checkpoint-epoch14.pth ...
2024-02-11 07:14:13,814 - PROT.PROT.main - INFO - Initialising evaluation
2024-02-11 07:14:15,015 - PROT.PROT.base.base_eval - INFO - Starting evaluating...
2024-02-11 07:14:19,105 - PROT.PROT.base.base_eval - INFO - metric_ss8: 0.6664552944047111
2024-02-11 07:14:19,106 - PROT.PROT.base.base_eval - INFO - metric_ss3: 0.7774924550737653
2024-02-11 07:14:19,108 - PROT.PROT.base.base_eval - INFO - metric_dis_mcc: 0.5593646381582532
2024-02-11 07:14:19,109 - PROT.PROT.base.base_eval - INFO - metric_dis_fnr: 0.49896162482244627
2024-02-11 07:14:19,112 - PROT.PROT.base.base_eval - INFO - metric_rsa: 0.7072873967034476
2024-02-11 07:14:19,113 - PROT.PROT.base.base_eval - INFO - metric_asa: 0.7236969470977783
2024-02-11 07:14:19,115 - PROT.PROT.base.base_eval - INFO - metric_phi: 20.60733331952776
2024-02-11 07:14:19,117 - PROT.PROT.base.base_eval - INFO - metric_psi: 32.501014709472656
2024-02-11 07:14:20,541 - PROT.PROT.base.base_eval - INFO - Starting evaluating...
2024-02-11 07:15:16,971 - PROT.PROT.base.base_eval - INFO - metric_ss8: 0.7240939499341954
2024-02-11 07:15:16,974 - PROT.PROT.base.base_eval - INFO - metric_ss3: 0.8586717837038096
2024-02-11 07:15:16,976 - PROT.PROT.base.base_eval - INFO - metric_dis_mcc: 0.060304974070425295
2024-02-11 07:15:16,980 - PROT.PROT.base.base_eval - INFO - metric_dis_fnr: 0.9192991177825367
2024-02-11 07:15:16,981 - PROT.PROT.base.base_eval - INFO - metric_rsa: 0.8032443044129868
2024-02-11 07:15:16,983 - PROT.PROT.base.base_eval - INFO - metric_asa: 0.817485297283931
2024-02-11 07:15:16,985 - PROT.PROT.base.base_eval - INFO - metric_phi: 19.341488944159615
2024-02-11 07:15:16,987 - PROT.PROT.base.base_eval - INFO - metric_psi: 26.604242118478517
2024-02-11 07:15:18,244 - PROT.PROT.base.base_eval - INFO - Starting evaluating...
2024-02-11 07:15:32,695 - PROT.PROT.base.base_eval - INFO - metric_ss8: 0.752636846770411
2024-02-11 07:15:32,696 - PROT.PROT.base.base_eval - INFO - metric_ss3: 0.8579935172329778
2024-02-11 07:15:32,698 - PROT.PROT.base.base_eval - INFO - metric_dis_mcc: 0.6459913005485483
2024-02-11 07:15:32,701 - PROT.PROT.base.base_eval - INFO - metric_dis_fnr: 0.39222489294150603
2024-02-11 07:15:32,702 - PROT.PROT.base.base_eval - INFO - metric_rsa: 0.7853043094925258
2024-02-11 07:15:32,706 - PROT.PROT.base.base_eval - INFO - metric_asa: 0.8052197803621707
2024-02-11 07:15:32,707 - PROT.PROT.base.base_eval - INFO - metric_phi: 16.75584180251412
2024-02-11 07:15:32,709 - PROT.PROT.base.base_eval - INFO - metric_psi: 24.418053328472634
2024-02-11 07:15:32,737 - PROT.PROT.main - INFO - Finished!
done
