Requirement already satisfied: click in /scistor/informatica/dgi460/anaconda3/envs/dl2022/lib/python3.10/site-packages (8.1.3)
Requirement already satisfied: fair-esm in /scistor/informatica/dgi460/anaconda3/envs/dl2022/lib/python3.10/site-packages (2.0.0)
Processing /scistor/informatica/dgi460/PROT/PROT
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Building wheels for collected packages: PROT
  Building wheel for PROT (setup.py): started
  Building wheel for PROT (setup.py): finished with status 'done'
  Created wheel for PROT: filename=PROT-0.0.1-py3-none-any.whl size=95787 sha256=9b72a5c15c0ffe334366eedb93142b64b8bdfb261fc930d51962eb4e2a7caa15
  Stored in directory: /scratch/671579/pip-ephem-wheel-cache-xm5c_bj_/wheels/f3/f3/d5/e95d019bbe728e863c8658a0c362e103b5264b28afecea62b9
Successfully built PROT
Installing collected packages: PROT
  Attempting uninstall: PROT
    Found existing installation: PROT 0.0.1
    Uninstalling PROT-0.0.1:
      Successfully uninstalled PROT-0.0.1
Successfully installed PROT-0.0.1
2024-02-10 11:33:30,810 - PROT.PROT.main - INFO - Building: PROT.models.ESM2_original_extended
FINETUNING CHOSEN: LoRA
Gradient Checkpointing 2
2024-02-10 11:33:43,453 - PROT.PROT.models.ESM2_original_extended.model - INFO - <init>: 
ESM2_original_extended(
  (embedding): ESM2Embedding(
    (model): ESM2(
      (embed_tokens): Embedding(33, 1280, padding_idx=1)
      (layers): ModuleList(
        (0): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (12): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (13): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (14): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (15): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (16): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (17): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (18): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (19): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (20): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (21): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (22): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (23): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (24): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (25): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (26): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (27): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (28): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (29): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (30): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (31): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (32): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (contact_head): ContactPredictionHead(
        (regression): Linear(in_features=660, out_features=1, bias=True)
        (activation): Sigmoid()
      )
      (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (lm_head): RobertaLMHead(
        (dense): Linear(in_features=1280, out_features=1280, bias=True)
        (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (conv): ModuleList(
    (0): Sequential(
      (0): Dropout(p=0.5, inplace=False)
      (1): Conv1d(1280, 32, kernel_size=(129,), stride=(1,), padding=(64,))
      (2): ReLU()
    )
    (1): Sequential(
      (0): Dropout(p=0.5, inplace=False)
      (1): Conv1d(1280, 32, kernel_size=(257,), stride=(1,), padding=(128,))
      (2): ReLU()
    )
  )
  (batch_norm): BatchNorm1d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (lstm): LSTM(1344, 1024, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)
  (lstm_dropout_layer): Dropout(p=0.5, inplace=False)
  (ss8): Sequential(
    (0): Linear(in_features=2048, out_features=8, bias=True)
  )
  (ss3): Sequential(
    (0): Linear(in_features=2048, out_features=3, bias=True)
  )
  (disorder): Sequential(
    (0): Linear(in_features=2048, out_features=2, bias=True)
  )
  (rsa): Sequential(
    (0): Linear(in_features=2048, out_features=1, bias=True)
    (1): Sigmoid()
  )
  (phi): Sequential(
    (0): Linear(in_features=2048, out_features=2, bias=True)
    (1): Tanh()
  )
  (psi): Sequential(
    (0): Linear(in_features=2048, out_features=2, bias=True)
    (1): Tanh()
  )
  (tasa): Sequential(
    (0): Linear(in_features=2048, out_features=1, bias=True)
  )
  (thsa): Sequential(
    (0): Linear(in_features=2048, out_features=1, bias=True)
  )
  (lhp): Sequential(
    (0): Linear(in_features=2048, out_features=1, bias=True)
  )
  (hp_loc): Sequential(
    (0): Linear(in_features=2048, out_features=2, bias=True)
  )
  (lhp_loc): Sequential(
    (0): Linear(in_features=2048, out_features=2, bias=True)
  )
  (species): Sequential(
    (0): Linear(in_features=2048, out_features=10, bias=True)
  )
  (expression): Sequential(
    (0): Linear(in_features=2048, out_features=2, bias=True)
  )
  (aggregation): Sequential(
    (0): Linear(in_features=2048, out_features=2, bias=True)
  )
)
Trainable parameters: 61504231
2024-02-10 11:33:43,463 - PROT.PROT.main - INFO - Using devices [0] of available devices [0]
Using devices [0] of available devices [0]
2024-02-10 11:33:58,212 - PROT.PROT.main - INFO - Building: torch.optim.Adam
2024-02-10 11:33:58,215 - PROT.PROT.main - INFO - Building: PROT.data_loader.augmentation.sparse_token
FINETUNING CHOSEN: no finetuning
Gradient Checkpointing None
2024-02-10 11:34:00,765 - PROT.PROT.main - INFO - Building: PROT.data_loader.data_loaders.NSPDataLoader
2024-02-10 11:36:36,516 - PROT.PROT.main - INFO - Getting loss and metric function handles
2024-02-10 11:36:36,518 - PROT.PROT.main - INFO - Initialising trainer
GRADIENT ACCUMULATION 6
2024-02-10 11:36:36,534 - PROT.PROT.base.base_trainer - INFO - Starting training...
2024-02-10 11:36:45,879 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [0/6114 (0%)] Loss: 36.176277
2024-02-10 11:40:51,029 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [900/6114 (15%)] Loss: 24.417889
2024-02-10 11:44:55,479 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [1800/6114 (29%)] Loss: 4.716904
2024-02-10 11:48:58,062 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [2700/6114 (44%)] Loss: 17.765640
2024-02-10 11:52:58,695 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [3600/6114 (59%)] Loss: 10.239934
2024-02-10 11:57:01,856 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [4500/6114 (74%)] Loss: 6.382977
2024-02-10 12:01:11,577 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [5400/6114 (88%)] Loss: 4.668524
2024-02-10 12:05:02,979 - PROT.PROT.base.base_trainer - INFO - epoch          : 0
2024-02-10 12:05:02,980 - PROT.PROT.base.base_trainer - INFO - loss           : 13.1616106216274
2024-02-10 12:05:02,984 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 4403.237043108259
2024-02-10 12:05:02,989 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0.4344030243477651
2024-02-10 12:05:02,993 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0.36452414308275494
2024-02-10 12:05:02,998 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0.31510881653853823
2024-02-10 12:05:03,002 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0.31094828248023987
2024-02-10 12:05:03,005 - PROT.PROT.base.base_trainer - INFO - val_loss       : 8.927192768203877
2024-02-10 12:05:03,010 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 3890.2057164272414
2024-02-10 12:05:03,015 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0.6829736206297562
2024-02-10 12:05:03,018 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0.20762787329328952
2024-02-10 12:05:03,021 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0.38078859002790716
2024-02-10 12:05:03,026 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0.18347633099966795
2024-02-10 12:05:45,754 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-113636/checkpoints/checkpoint-epoch0.pth ...
2024-02-10 12:06:32,713 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2/0210-113636/checkpoints/model_best.pth
2024-02-10 12:06:34,004 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [0/6114 (0%)] Loss: 6.136455
2024-02-10 12:10:38,433 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [900/6114 (15%)] Loss: 4.335687
2024-02-10 12:14:46,857 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [1800/6114 (29%)] Loss: 7.070113
2024-02-10 12:18:47,831 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [2700/6114 (44%)] Loss: 6.299071
2024-02-10 12:22:50,839 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [3600/6114 (59%)] Loss: 5.847317
2024-02-10 12:26:59,942 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [4500/6114 (74%)] Loss: 6.390138
2024-02-10 12:31:04,541 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [5400/6114 (88%)] Loss: 6.172984
2024-02-10 12:35:02,925 - PROT.PROT.base.base_trainer - INFO - epoch          : 1
2024-02-10 12:35:02,926 - PROT.PROT.base.base_trainer - INFO - loss           : 7.117314890378121
2024-02-10 12:35:02,933 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 2665.0765555245534
2024-02-10 12:35:02,937 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0.5478096302332622
2024-02-10 12:35:02,940 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0.3322896201695715
2024-02-10 12:35:02,944 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0.3353512132806437
2024-02-10 12:35:02,947 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0.29184295343501226
2024-02-10 12:35:02,950 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.789745110217656
2024-02-10 12:35:02,954 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 3946.746352543341
2024-02-10 12:35:02,959 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0.7003173149913271
2024-02-10 12:35:02,962 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0.1765942719164435
2024-02-10 12:35:02,966 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0.3853976123522375
2024-02-10 12:35:02,972 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0.1590073579643792
2024-02-10 12:35:33,223 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-113636/checkpoints/checkpoint-epoch1.pth ...
2024-02-10 12:36:10,766 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2/0210-113636/checkpoints/model_best.pth
2024-02-10 12:36:12,774 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [0/6114 (0%)] Loss: 4.117169
2024-02-10 12:40:09,442 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [900/6114 (15%)] Loss: 5.772397
2024-02-10 12:44:12,255 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [1800/6114 (29%)] Loss: 5.632299
2024-02-10 12:48:14,589 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [2700/6114 (44%)] Loss: 4.123991
2024-02-10 12:52:34,058 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [3600/6114 (59%)] Loss: 4.775946
2024-02-10 12:56:48,741 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [4500/6114 (74%)] Loss: 5.819811
2024-02-10 13:00:56,220 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [5400/6114 (88%)] Loss: 5.778006
2024-02-10 13:04:46,734 - PROT.PROT.base.base_trainer - INFO - epoch          : 2
2024-02-10 13:04:46,735 - PROT.PROT.base.base_trainer - INFO - loss           : 6.267235159717929
2024-02-10 13:04:46,738 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 2119.925859723772
2024-02-10 13:04:46,741 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0.6409483019794736
2024-02-10 13:04:46,743 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0.26180441145386013
2024-02-10 13:04:46,747 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0.321963617844241
2024-02-10 13:04:46,750 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0.24098176722015655
2024-02-10 13:04:46,751 - PROT.PROT.base.base_trainer - INFO - val_loss       : 6.585294335802025
2024-02-10 13:04:46,754 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 3557.6815721743574
2024-02-10 13:04:46,755 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0.6998671215271282
2024-02-10 13:04:46,757 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0.22163021508778366
2024-02-10 13:04:46,759 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0.3816984762202635
2024-02-10 13:04:46,761 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0.20865983846727934
2024-02-10 13:05:19,125 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-113636/checkpoints/checkpoint-epoch2.pth ...
2024-02-10 13:05:53,979 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2/0210-113636/checkpoints/model_best.pth
2024-02-10 13:05:55,157 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [0/6114 (0%)] Loss: 4.446095
2024-02-10 13:09:51,209 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [900/6114 (15%)] Loss: 5.231030
2024-02-10 13:13:57,538 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [1800/6114 (29%)] Loss: 4.153639
2024-02-10 13:18:07,020 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [2700/6114 (44%)] Loss: 4.612031
2024-02-10 13:22:13,856 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [3600/6114 (59%)] Loss: 5.449412
2024-02-10 13:26:17,406 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [4500/6114 (74%)] Loss: 4.726877
2024-02-10 13:30:31,806 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [5400/6114 (88%)] Loss: 5.673594
2024-02-10 13:34:26,583 - PROT.PROT.base.base_trainer - INFO - epoch          : 3
2024-02-10 13:34:26,584 - PROT.PROT.base.base_trainer - INFO - loss           : 5.913406238544986
2024-02-10 13:34:26,586 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 2101.099530901228
2024-02-10 13:34:26,587 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0.7774333996432168
2024-02-10 13:34:26,589 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0.15034168040645973
2024-02-10 13:34:26,590 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0.42841285467147827
2024-02-10 13:34:26,592 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0.1317001533295427
2024-02-10 13:34:26,594 - PROT.PROT.base.base_trainer - INFO - val_loss       : 6.36330253387166
2024-02-10 13:34:26,596 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 3495.0766535963967
2024-02-10 13:34:26,598 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0.6840917644935234
2024-02-10 13:34:26,600 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0.2361411143129117
2024-02-10 13:34:26,601 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0.3766132733114412
2024-02-10 13:34:26,603 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0.21404743587998587
2024-02-10 13:34:56,718 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-113636/checkpoints/checkpoint-epoch3.pth ...
2024-02-10 13:35:34,229 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2/0210-113636/checkpoints/model_best.pth
2024-02-10 13:35:35,699 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [0/6114 (0%)] Loss: 4.415864
2024-02-10 13:39:42,411 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [900/6114 (15%)] Loss: 6.715820
2024-02-10 13:43:46,372 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [1800/6114 (29%)] Loss: 5.975111
2024-02-10 13:47:58,999 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [2700/6114 (44%)] Loss: 7.221569
2024-02-10 13:52:05,290 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [3600/6114 (59%)] Loss: 5.013501
2024-02-10 13:56:06,682 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [4500/6114 (74%)] Loss: 5.302999
2024-02-10 14:00:10,685 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [5400/6114 (88%)] Loss: 5.446514
2024-02-10 14:03:57,423 - PROT.PROT.base.base_trainer - INFO - epoch          : 4
2024-02-10 14:03:57,424 - PROT.PROT.base.base_trainer - INFO - loss           : 5.75246277139449
2024-02-10 14:03:57,426 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 2894.4753069196427
2024-02-10 14:03:57,429 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0.6980035858494895
2024-02-10 14:03:57,431 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0.2304399460554123
2024-02-10 14:03:57,434 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0.389924909387316
2024-02-10 14:03:57,435 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0.18432870294366563
2024-02-10 14:03:57,439 - PROT.PROT.base.base_trainer - INFO - val_loss       : 6.8000827192146085
2024-02-10 14:03:57,440 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 3969.059217791691
2024-02-10 14:03:57,442 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0.6988645053912546
2024-02-10 14:03:57,445 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0.2082452717472181
2024-02-10 14:03:57,447 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0.38853116970831336
2024-02-10 14:03:57,448 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0.18527563229621014
2024-02-10 14:04:28,492 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-113636/checkpoints/checkpoint-epoch4.pth ...
2024-02-10 14:04:30,433 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [0/6114 (0%)] Loss: 6.069488
2024-02-10 14:08:37,941 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [900/6114 (15%)] Loss: 4.572747
2024-02-10 14:12:45,503 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [1800/6114 (29%)] Loss: 6.792067
2024-02-10 14:16:36,896 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [2700/6114 (44%)] Loss: 6.183908
2024-02-10 14:20:41,358 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [3600/6114 (59%)] Loss: 4.563291
2024-02-10 14:24:50,834 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [4500/6114 (74%)] Loss: 4.798985
2024-02-10 14:28:57,449 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [5400/6114 (88%)] Loss: 4.300385
2024-02-10 14:32:50,665 - PROT.PROT.base.base_trainer - INFO - epoch          : 5
2024-02-10 14:32:50,667 - PROT.PROT.base.base_trainer - INFO - loss           : 5.691370615628378
2024-02-10 14:32:50,668 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 2824.500313895089
2024-02-10 14:32:50,671 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0.7913746067455837
2024-02-10 14:32:50,672 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0.12541167251765728
2024-02-10 14:32:50,674 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0.4594519723738943
2024-02-10 14:32:50,675 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0.1489844625549657
2024-02-10 14:32:50,677 - PROT.PROT.base.base_trainer - INFO - val_loss       : 6.126270601682574
2024-02-10 14:32:50,678 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 3394.0780571197797
2024-02-10 14:32:50,680 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0.7114426455347338
2024-02-10 14:32:50,681 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0.19206427962075326
2024-02-10 14:32:50,682 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0.3922041778879188
2024-02-10 14:32:50,684 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0.17776367432986187
2024-02-10 14:33:21,968 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-113636/checkpoints/checkpoint-epoch5.pth ...
2024-02-10 14:33:57,490 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2/0210-113636/checkpoints/model_best.pth
2024-02-10 14:33:59,263 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [0/6114 (0%)] Loss: 5.697411
2024-02-10 14:38:01,458 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [900/6114 (15%)] Loss: 5.258286
2024-02-10 14:42:03,348 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [1800/6114 (29%)] Loss: 4.693705
2024-02-10 14:46:10,692 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [2700/6114 (44%)] Loss: 6.906426
2024-02-10 14:50:15,217 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [3600/6114 (59%)] Loss: 6.038810
2024-02-10 14:54:22,492 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [4500/6114 (74%)] Loss: 6.288880
2024-02-10 14:58:35,079 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [5400/6114 (88%)] Loss: 5.464089
2024-02-10 15:02:27,469 - PROT.PROT.base.base_trainer - INFO - epoch          : 6
2024-02-10 15:02:27,470 - PROT.PROT.base.base_trainer - INFO - loss           : 5.63177743335669
2024-02-10 15:02:27,474 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 2988.885567801339
2024-02-10 15:02:27,479 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0.584663912653923
2024-02-10 15:02:27,483 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0.3053141715271132
2024-02-10 15:02:27,486 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0.33997227464403423
2024-02-10 15:02:27,490 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0.2892815587776048
2024-02-10 15:02:27,493 - PROT.PROT.base.base_trainer - INFO - val_loss       : 6.5127441058649085
2024-02-10 15:02:27,497 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 3805.903022552205
2024-02-10 15:02:27,500 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0.7017510223994466
2024-02-10 15:02:27,504 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0.22029303602665384
2024-02-10 15:02:27,507 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0.3948640843035183
2024-02-10 15:02:27,511 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0.19020573508481833
2024-02-10 15:03:08,500 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-113636/checkpoints/checkpoint-epoch6.pth ...
2024-02-10 15:03:09,662 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [0/6114 (0%)] Loss: 4.582739
2024-02-10 15:07:06,519 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [900/6114 (15%)] Loss: 4.789371
2024-02-10 15:11:14,143 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [1800/6114 (29%)] Loss: 10.349408
2024-02-10 15:15:22,396 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [2700/6114 (44%)] Loss: 5.321780
2024-02-10 15:19:32,733 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [3600/6114 (59%)] Loss: 5.116335
2024-02-10 15:23:35,402 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [4500/6114 (74%)] Loss: 5.961763
2024-02-10 15:27:35,070 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [5400/6114 (88%)] Loss: 5.413673
2024-02-10 15:31:28,301 - PROT.PROT.base.base_trainer - INFO - epoch          : 7
2024-02-10 15:31:28,303 - PROT.PROT.base.base_trainer - INFO - loss           : 5.543346731372528
2024-02-10 15:31:28,309 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 3199.8094482421875
2024-02-10 15:31:28,314 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0.7962171605655125
2024-02-10 15:31:28,318 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0.13075103610754013
2024-02-10 15:31:28,320 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0.4687377767903464
2024-02-10 15:31:28,325 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0.12356821713703019
2024-02-10 15:31:28,328 - PROT.PROT.base.base_trainer - INFO - val_loss       : 6.411286960138339
2024-02-10 15:31:28,331 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 3621.2450515889677
2024-02-10 15:31:28,335 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0.698017694578271
2024-02-10 15:31:28,339 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0.21748631170768046
2024-02-10 15:31:28,346 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0.3907703319651501
2024-02-10 15:31:28,351 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0.19531088536518199
2024-02-10 15:32:10,608 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-113636/checkpoints/checkpoint-epoch7.pth ...
2024-02-10 15:32:11,924 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [0/6114 (0%)] Loss: 4.936227
2024-02-10 15:36:08,524 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [900/6114 (15%)] Loss: 4.709072
2024-02-10 15:40:16,017 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [1800/6114 (29%)] Loss: 5.289955
2024-02-10 15:44:22,204 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [2700/6114 (44%)] Loss: 5.733875
2024-02-10 15:48:42,311 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [3600/6114 (59%)] Loss: 4.254807
2024-02-10 15:52:43,936 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [4500/6114 (74%)] Loss: 6.671623
2024-02-10 15:56:38,517 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [5400/6114 (88%)] Loss: 5.202738
2024-02-10 16:00:33,207 - PROT.PROT.base.base_trainer - INFO - epoch          : 8
2024-02-10 16:00:33,208 - PROT.PROT.base.base_trainer - INFO - loss           : 5.413188421640409
2024-02-10 16:00:33,210 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 1555.5274788992745
2024-02-10 16:00:33,213 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0.5984745153359005
2024-02-10 16:00:33,215 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0.29960888837065014
2024-02-10 16:00:33,216 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0.37684172817638945
2024-02-10 16:00:33,218 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0.2028409583227975
2024-02-10 16:00:33,220 - PROT.PROT.base.base_trainer - INFO - val_loss       : 6.354256915154858
2024-02-10 16:00:33,222 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 3411.0242954147197
2024-02-10 16:00:33,224 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0.6884702885540847
2024-02-10 16:00:33,225 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0.2399331624402064
2024-02-10 16:00:33,227 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0.3818508838520986
2024-02-10 16:00:33,229 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0.21892351268956037
2024-02-10 16:01:13,913 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-113636/checkpoints/checkpoint-epoch8.pth ...
2024-02-10 16:01:16,464 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [0/6114 (0%)] Loss: 8.108763
2024-02-10 16:05:31,689 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [900/6114 (15%)] Loss: 5.737333
2024-02-10 16:09:30,400 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [1800/6114 (29%)] Loss: 5.434736
2024-02-10 16:13:32,770 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [2700/6114 (44%)] Loss: 6.517704
2024-02-10 16:17:31,948 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [3600/6114 (59%)] Loss: 6.512256
2024-02-10 16:21:35,790 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [4500/6114 (74%)] Loss: 5.474400
2024-02-10 16:25:40,395 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [5400/6114 (88%)] Loss: 6.959381
2024-02-10 16:29:38,485 - PROT.PROT.base.base_trainer - INFO - epoch          : 9
2024-02-10 16:29:38,486 - PROT.PROT.base.base_trainer - INFO - loss           : 5.294249752897243
2024-02-10 16:29:38,489 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 3215.894374302455
2024-02-10 16:29:38,490 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0.40697091285671505
2024-02-10 16:29:38,494 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0.4218533230679376
2024-02-10 16:29:38,496 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0.2790303943412645
2024-02-10 16:29:38,497 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0.319366493927581
2024-02-10 16:29:38,500 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.39877020978482
2024-02-10 16:29:38,504 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 4253.661157340647
2024-02-10 16:29:38,507 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0.7249961458668809
2024-02-10 16:29:38,511 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0.16185409867283063
2024-02-10 16:29:38,513 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0.4065707493399348
2024-02-10 16:29:38,518 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0.14245789014151164
2024-02-10 16:30:04,554 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-113636/checkpoints/checkpoint-epoch9.pth ...
2024-02-10 16:30:07,091 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [0/6114 (0%)] Loss: 5.874276
2024-02-10 16:34:07,479 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [900/6114 (15%)] Loss: 4.217268
2024-02-10 16:38:03,887 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [1800/6114 (29%)] Loss: 4.447708
2024-02-10 16:42:13,960 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [2700/6114 (44%)] Loss: 5.049517
2024-02-10 16:46:24,966 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [3600/6114 (59%)] Loss: 4.268517
2024-02-10 16:50:24,209 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [4500/6114 (74%)] Loss: 4.563524
2024-02-10 16:54:32,152 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [5400/6114 (88%)] Loss: 5.730569
2024-02-10 16:58:27,919 - PROT.PROT.base.base_trainer - INFO - epoch          : 10
2024-02-10 16:58:27,920 - PROT.PROT.base.base_trainer - INFO - loss           : 5.2760213612729965
2024-02-10 16:58:27,924 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 1327.0997358049665
2024-02-10 16:58:27,925 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0.6267704495361873
2024-02-10 16:58:27,928 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0.27388538100889753
2024-02-10 16:58:27,930 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0.27360843973500387
2024-02-10 16:58:27,932 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0.27538368105888367
2024-02-10 16:58:27,935 - PROT.PROT.base.base_trainer - INFO - val_loss       : 8.438217457209792
2024-02-10 16:58:27,937 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 4800.054705468294
2024-02-10 16:58:27,939 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0.7082220116699828
2024-02-10 16:58:27,940 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0.20384482823521177
2024-02-10 16:58:27,942 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0.38744215021414735
2024-02-10 16:58:27,944 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0.18969825836264084
2024-02-10 16:59:00,795 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-113636/checkpoints/checkpoint-epoch10.pth ...
2024-02-10 16:59:01,845 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [0/6114 (0%)] Loss: 4.172410
2024-02-10 17:03:03,570 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [900/6114 (15%)] Loss: 5.523999
2024-02-10 17:07:12,307 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [1800/6114 (29%)] Loss: 4.829012
2024-02-10 17:11:09,071 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [2700/6114 (44%)] Loss: 6.994853
2024-02-10 17:15:13,381 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [3600/6114 (59%)] Loss: 5.476493
2024-02-10 17:19:22,410 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [4500/6114 (74%)] Loss: 3.932813
2024-02-10 17:23:29,060 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [5400/6114 (88%)] Loss: 5.819298
2024-02-10 17:27:24,241 - PROT.PROT.base.base_trainer - INFO - epoch          : 11
2024-02-10 17:27:24,244 - PROT.PROT.base.base_trainer - INFO - loss           : 5.126799735099233
2024-02-10 17:27:24,246 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 2298.9435076032364
2024-02-10 17:27:24,249 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0.6414242322955813
2024-02-10 17:27:24,251 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0.2568476934518133
2024-02-10 17:27:24,253 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0.3673127804483686
2024-02-10 17:27:24,255 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0.23214940833193914
2024-02-10 17:27:24,257 - PROT.PROT.base.base_trainer - INFO - val_loss       : 6.959859317708238
2024-02-10 17:27:24,259 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 3635.0868447562243
2024-02-10 17:27:24,261 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0.7049883715460233
2024-02-10 17:27:24,263 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0.18698551126311871
2024-02-10 17:27:24,265 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0.39251327041153594
2024-02-10 17:27:24,267 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0.1617424782087034
2024-02-10 17:27:54,136 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-113636/checkpoints/checkpoint-epoch11.pth ...
2024-02-10 17:27:56,821 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [0/6114 (0%)] Loss: 6.304606
2024-02-10 17:31:57,149 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [900/6114 (15%)] Loss: 4.013986
2024-02-10 17:36:07,016 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [1800/6114 (29%)] Loss: 3.878680
2024-02-10 17:40:13,593 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [2700/6114 (44%)] Loss: 6.236388
2024-02-10 17:44:17,838 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [3600/6114 (59%)] Loss: 4.634017
2024-02-10 17:48:23,391 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [4500/6114 (74%)] Loss: 4.130397
2024-02-10 17:52:36,407 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [5400/6114 (88%)] Loss: 5.115536
2024-02-10 17:56:24,133 - PROT.PROT.base.base_trainer - INFO - epoch          : 12
2024-02-10 17:56:24,134 - PROT.PROT.base.base_trainer - INFO - loss           : 5.056557929679673
2024-02-10 17:56:24,142 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 1680.507289341518
2024-02-10 17:56:24,151 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0.6401393498693194
2024-02-10 17:56:24,172 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0.26064422566975864
2024-02-10 17:56:24,175 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0.3806180698531015
2024-02-10 17:56:24,179 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0.18219231707709177
2024-02-10 17:56:24,184 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.047098445001049
2024-02-10 17:56:24,186 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 3481.8516751583493
2024-02-10 17:56:24,188 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0.7112369590969844
2024-02-10 17:56:24,193 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0.1933842737436573
2024-02-10 17:56:24,195 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0.39123462560090505
2024-02-10 17:56:24,202 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0.17998073504210632
2024-02-10 17:56:56,618 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-113636/checkpoints/checkpoint-epoch12.pth ...
2024-02-10 17:56:58,461 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [0/6114 (0%)] Loss: 4.791390
2024-02-10 18:01:02,557 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [900/6114 (15%)] Loss: 4.205240
2024-02-10 18:05:06,204 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [1800/6114 (29%)] Loss: 4.959255
2024-02-10 18:09:06,608 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [2700/6114 (44%)] Loss: 4.693457
2024-02-10 18:13:19,666 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [3600/6114 (59%)] Loss: 5.285069
2024-02-10 18:17:24,107 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [4500/6114 (74%)] Loss: 4.374163
2024-02-10 18:21:24,527 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [5400/6114 (88%)] Loss: 4.556272
2024-02-10 18:25:25,636 - PROT.PROT.base.base_trainer - INFO - epoch          : 13
2024-02-10 18:25:25,638 - PROT.PROT.base.base_trainer - INFO - loss           : 4.988646775991192
2024-02-10 18:25:25,646 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 1450.8925083705358
2024-02-10 18:25:25,652 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0.70038389308112
2024-02-10 18:25:25,661 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0.2080028142247881
2024-02-10 18:25:25,675 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0.33360745651381357
2024-02-10 18:25:25,680 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0.19296700400965555
2024-02-10 18:25:25,681 - PROT.PROT.base.base_trainer - INFO - val_loss       : 6.8394620173445375
2024-02-10 18:25:25,687 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 3386.2752742589078
2024-02-10 18:25:25,689 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0.7118213316338642
2024-02-10 18:25:25,691 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0.18961264464621233
2024-02-10 18:25:25,717 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0.39735848790852824
2024-02-10 18:25:25,724 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0.16466802845178086
2024-02-10 18:25:59,350 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-113636/checkpoints/checkpoint-epoch13.pth ...
2024-02-10 18:26:00,741 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [0/6114 (0%)] Loss: 3.802492
2024-02-10 18:30:07,010 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [900/6114 (15%)] Loss: 4.239251
2024-02-10 18:34:15,669 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [1800/6114 (29%)] Loss: 5.696281
2024-02-10 18:38:14,010 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [2700/6114 (44%)] Loss: 6.238932
2024-02-10 18:42:17,261 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [3600/6114 (59%)] Loss: 5.706547
2024-02-10 18:46:28,344 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [4500/6114 (74%)] Loss: 4.956613
2024-02-10 18:50:36,144 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [5400/6114 (88%)] Loss: 4.027026
2024-02-10 18:54:28,765 - PROT.PROT.base.base_trainer - INFO - epoch          : 14
2024-02-10 18:54:28,767 - PROT.PROT.base.base_trainer - INFO - loss           : 4.875223294446606
2024-02-10 18:54:28,777 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 1602.899126325335
2024-02-10 18:54:28,782 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0.580164543219975
2024-02-10 18:54:28,785 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0.29803948955876486
2024-02-10 18:54:28,788 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0.3640837094613484
2024-02-10 18:54:28,793 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0.25723695462303503
2024-02-10 18:54:28,801 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.807909127707793
2024-02-10 18:54:28,808 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 3855.833737952687
2024-02-10 18:54:28,813 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0.7116083948759833
2024-02-10 18:54:28,818 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0.17308578467884353
2024-02-10 18:54:28,825 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0.3900941851609778
2024-02-10 18:54:28,838 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0.15592904297513105
2024-02-10 18:55:19,393 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2/0210-113636/checkpoints/checkpoint-epoch14.pth ...
2024-02-10 18:55:19,395 - PROT.PROT.main - INFO - Initialising evaluation
2024-02-10 18:55:21,440 - PROT.PROT.base.base_eval - INFO - Starting evaluating...
2024-02-10 18:55:23,860 - PROT.PROT.base.base_eval - INFO - metric_lhp: 2630.750284830729
2024-02-10 18:55:23,861 - PROT.PROT.base.base_eval - INFO - metric_hp_loc_mcc: 0.8410954276720682
2024-02-10 18:55:23,870 - PROT.PROT.base.base_eval - INFO - metric_hp_loc_fnr: 0.09938931465148926
2024-02-10 18:55:23,878 - PROT.PROT.base.base_eval - INFO - metric_lhp_loc_mcc: 0.4617816706498464
2024-02-10 18:55:23,885 - PROT.PROT.base.base_eval - INFO - metric_lhp_loc_fnr: 0.10463213175535202
2024-02-10 18:55:25,142 - PROT.PROT.base.base_eval - INFO - Starting evaluating...
2024-02-10 18:56:06,172 - PROT.PROT.base.base_eval - INFO - metric_lhp: 3636.483262208562
2024-02-10 18:56:06,176 - PROT.PROT.base.base_eval - INFO - metric_hp_loc_mcc: 0.8065191254159874
2024-02-10 18:56:06,182 - PROT.PROT.base.base_eval - INFO - metric_hp_loc_fnr: 0.13450316490961653
2024-02-10 18:56:06,191 - PROT.PROT.base.base_eval - INFO - metric_lhp_loc_mcc: 0.41539823629339284
2024-02-10 18:56:06,200 - PROT.PROT.base.base_eval - INFO - metric_lhp_loc_fnr: 0.12258453755580549
2024-02-10 18:56:07,348 - PROT.PROT.base.base_eval - INFO - Starting evaluating...
2024-02-10 18:56:16,058 - PROT.PROT.base.base_eval - INFO - metric_lhp: 3589.5146493484726
2024-02-10 18:56:16,059 - PROT.PROT.base.base_eval - INFO - metric_hp_loc_mcc: 0.7723439771737626
2024-02-10 18:56:16,062 - PROT.PROT.base.base_eval - INFO - metric_hp_loc_fnr: 0.14885547607025104
2024-02-10 18:56:16,072 - PROT.PROT.base.base_eval - INFO - metric_lhp_loc_mcc: 0.462990502145753
2024-02-10 18:56:16,081 - PROT.PROT.base.base_eval - INFO - metric_lhp_loc_fnr: 0.12937828678804547
2024-02-10 18:56:16,097 - PROT.PROT.main - INFO - Finished!
done
