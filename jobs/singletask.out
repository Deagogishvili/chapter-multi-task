Requirement already satisfied: click in /scistor/informatica/dgi460/anaconda3/envs/dl2022/lib/python3.10/site-packages (8.1.3)
Requirement already satisfied: fair-esm in /scistor/informatica/dgi460/anaconda3/envs/dl2022/lib/python3.10/site-packages (2.0.0)
Processing /scistor/informatica/dgi460/multitask/patchprot
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Building wheels for collected packages: PROT
  Building wheel for PROT (setup.py): started
  Building wheel for PROT (setup.py): finished with status 'done'
  Created wheel for PROT: filename=PROT-0.0.1-py3-none-any.whl size=86162 sha256=d0b436bfaee67692297297ec32489ddbd0e30fa57657c464c477491189b7747f
  Stored in directory: /scratch/452553/pip-ephem-wheel-cache-edbb8qm8/wheels/60/99/6a/7184f8915db8f8057756599c6a5fa5106340a8d9e80ffa3b17
Successfully built PROT
Installing collected packages: PROT
  Attempting uninstall: PROT
    Found existing installation: PROT 0.0.1
    Uninstalling PROT-0.0.1:
      Successfully uninstalled PROT-0.0.1
Successfully installed PROT-0.0.1
available gpus True 1
2024-01-23 19:36:01,188 - PROT.PROT.main - INFO - Building: PROT.models.ESM2_multitask
FINETUNING CHOSEN: LoRA
Gradient Checkpointing 2
2024-01-23 19:36:23,185 - PROT.PROT.models.ESM2_multitask.model - INFO - Params to learn:
2024-01-23 19:36:23,188 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.0.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,190 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.0.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,192 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.0.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,193 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.0.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,194 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.1.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,195 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.1.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,196 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.1.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,197 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.1.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,198 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.2.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,200 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.2.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,201 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.2.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,202 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.2.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,203 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.3.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,204 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.3.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,206 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.3.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,207 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.3.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,208 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.4.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,209 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.4.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,210 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.4.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,211 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.4.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,213 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.5.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,214 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.5.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,215 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.5.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,216 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.5.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,217 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.6.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,218 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.6.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,219 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.6.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,221 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.6.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,222 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.7.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,223 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.7.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,224 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.7.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,225 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.7.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,226 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.8.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,228 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.8.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,229 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.8.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,230 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.8.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,231 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.9.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,232 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.9.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,233 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.9.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,234 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.9.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,236 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.10.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,237 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.10.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,238 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.10.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,239 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.10.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,240 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.11.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,241 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.11.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,242 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.11.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,243 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.11.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,244 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.12.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,246 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.12.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,247 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.12.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,248 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.12.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,249 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.13.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,250 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.13.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,251 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.13.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,253 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.13.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,254 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.14.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,255 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.14.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,256 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.14.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,257 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.14.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,258 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.15.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,259 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.15.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,261 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.15.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,262 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.15.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,264 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.16.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,265 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.16.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,266 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.16.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,267 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.16.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,268 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.17.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,269 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.17.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,271 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.17.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,272 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.17.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,273 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.18.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,274 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.18.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,275 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.18.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,276 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.18.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,277 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.19.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,278 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.19.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,280 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.19.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,281 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.19.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,282 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.20.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,283 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.20.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,284 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.20.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,285 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.20.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,287 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.21.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,288 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.21.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,289 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.21.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,290 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.21.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,291 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.22.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,292 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.22.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,293 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.22.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,294 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.22.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,296 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.23.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,297 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.23.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,298 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.23.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,299 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.23.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,300 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.24.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,301 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.24.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,302 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.24.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,303 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.24.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,305 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.25.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,306 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.25.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,307 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.25.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,308 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.25.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,309 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.26.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,310 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.26.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,311 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.26.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,312 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.26.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,313 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.27.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,314 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.27.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,316 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.27.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,317 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.27.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,318 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.28.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,319 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.28.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,321 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.28.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,322 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.28.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,323 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.29.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,324 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.29.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,325 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.29.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,326 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.29.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,327 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.30.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,328 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.30.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,330 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.30.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,331 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.30.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,332 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.31.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,333 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.31.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,334 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.31.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,335 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.31.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,337 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.32.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:23,338 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.32.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:23,339 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.32.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:23,340 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.32.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:23,341 - PROT.PROT.models.ESM2_multitask.model - INFO - 	ss8.0.weight
2024-01-23 19:36:23,342 - PROT.PROT.models.ESM2_multitask.model - INFO - 	ss8.0.bias
2024-01-23 19:36:23,344 - PROT.PROT.models.ESM2_multitask.model - INFO - 	ss3.0.weight
2024-01-23 19:36:23,346 - PROT.PROT.models.ESM2_multitask.model - INFO - 	ss3.0.bias
2024-01-23 19:36:23,347 - PROT.PROT.models.ESM2_multitask.model - INFO - 	disorder.0.weight
2024-01-23 19:36:23,348 - PROT.PROT.models.ESM2_multitask.model - INFO - 	disorder.0.bias
2024-01-23 19:36:23,349 - PROT.PROT.models.ESM2_multitask.model - INFO - 	rsa.0.weight
2024-01-23 19:36:23,350 - PROT.PROT.models.ESM2_multitask.model - INFO - 	rsa.0.bias
2024-01-23 19:36:23,352 - PROT.PROT.models.ESM2_multitask.model - INFO - 	phi.0.weight
2024-01-23 19:36:23,353 - PROT.PROT.models.ESM2_multitask.model - INFO - 	phi.0.bias
2024-01-23 19:36:23,354 - PROT.PROT.models.ESM2_multitask.model - INFO - 	psi.0.weight
2024-01-23 19:36:23,355 - PROT.PROT.models.ESM2_multitask.model - INFO - 	psi.0.bias
2024-01-23 19:36:23,356 - PROT.PROT.models.ESM2_multitask.model - INFO - 	tasa.0.weight
2024-01-23 19:36:23,358 - PROT.PROT.models.ESM2_multitask.model - INFO - 	tasa.0.bias
2024-01-23 19:36:23,359 - PROT.PROT.models.ESM2_multitask.model - INFO - 	thsa.0.weight
2024-01-23 19:36:23,360 - PROT.PROT.models.ESM2_multitask.model - INFO - 	thsa.0.bias
2024-01-23 19:36:23,361 - PROT.PROT.models.ESM2_multitask.model - INFO - 	lhp.0.weight
2024-01-23 19:36:23,362 - PROT.PROT.models.ESM2_multitask.model - INFO - 	lhp.0.bias
2024-01-23 19:36:23,363 - PROT.PROT.models.ESM2_multitask.model - INFO - 	hp_loc.0.weight
2024-01-23 19:36:23,364 - PROT.PROT.models.ESM2_multitask.model - INFO - 	hp_loc.0.bias
2024-01-23 19:36:23,366 - PROT.PROT.models.ESM2_multitask.model - INFO - 	lhp_loc.0.weight
2024-01-23 19:36:23,367 - PROT.PROT.models.ESM2_multitask.model - INFO - 	lhp_loc.0.bias
2024-01-23 19:36:23,368 - PROT.PROT.models.ESM2_multitask.model - INFO - 	species.0.weight
2024-01-23 19:36:23,369 - PROT.PROT.models.ESM2_multitask.model - INFO - 	species.0.bias
2024-01-23 19:36:23,371 - PROT.PROT.models.ESM2_multitask.model - INFO - 	expression.0.weight
2024-01-23 19:36:23,372 - PROT.PROT.models.ESM2_multitask.model - INFO - 	expression.0.bias
2024-01-23 19:36:23,373 - PROT.PROT.models.ESM2_multitask.model - INFO - 	aggregation.0.weight
2024-01-23 19:36:23,374 - PROT.PROT.models.ESM2_multitask.model - INFO - 	aggregation.0.bias
2024-01-23 19:36:23,379 - PROT.PROT.models.ESM2_multitask.model - INFO - <init>: 
ESM2_multitask(
  (embedding): ESM2Embedding(
    (model): ESM2(
      (embed_tokens): Embedding(33, 1280, padding_idx=1)
      (layers): ModuleList(
        (0): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (12): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (13): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (14): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (15): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (16): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (17): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (18): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (19): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (20): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (21): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (22): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (23): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (24): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (25): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (26): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (27): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (28): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (29): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (30): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (31): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (32): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (contact_head): ContactPredictionHead(
        (regression): Linear(in_features=660, out_features=1, bias=True)
        (activation): Sigmoid()
      )
      (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (lm_head): RobertaLMHead(
        (dense): Linear(in_features=1280, out_features=1280, bias=True)
        (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (ss8): Sequential(
    (0): Linear(in_features=1280, out_features=8, bias=True)
  )
  (ss3): Sequential(
    (0): Linear(in_features=1280, out_features=3, bias=True)
  )
  (disorder): Sequential(
    (0): Linear(in_features=1280, out_features=2, bias=True)
  )
  (rsa): Sequential(
    (0): Linear(in_features=1280, out_features=1, bias=True)
    (1): Sigmoid()
  )
  (phi): Sequential(
    (0): Linear(in_features=1280, out_features=2, bias=True)
    (1): Tanh()
  )
  (psi): Sequential(
    (0): Linear(in_features=1280, out_features=2, bias=True)
    (1): Tanh()
  )
  (tasa): Sequential(
    (0): Linear(in_features=1280, out_features=1, bias=True)
  )
  (thsa): Sequential(
    (0): Linear(in_features=1280, out_features=1, bias=True)
  )
  (lhp): Sequential(
    (0): Linear(in_features=1280, out_features=1, bias=True)
  )
  (hp_loc): Sequential(
    (0): Linear(in_features=1280, out_features=2, bias=True)
  )
  (lhp_loc): Sequential(
    (0): Linear(in_features=1280, out_features=2, bias=True)
  )
  (species): Sequential(
    (0): Linear(in_features=1280, out_features=10, bias=True)
  )
  (expression): Sequential(
    (0): Linear(in_features=1280, out_features=10, bias=True)
  )
  (aggregation): Sequential(
    (0): Linear(in_features=1280, out_features=2, bias=True)
  )
)
Trainable parameters: 1073967
2024-01-23 19:36:23,381 - PROT.PROT.main - INFO - Using devices [0] of available devices [0]
Using devices [0] of available devices [0]
2024-01-23 19:36:31,696 - PROT.PROT.main - INFO - Building: torch.optim.Adam
2024-01-23 19:36:31,697 - PROT.PROT.models.ESM2_multitask.model - INFO - Params to learn:
2024-01-23 19:36:31,699 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.0.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,700 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.0.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,701 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.0.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,702 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.0.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,703 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.1.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,704 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.1.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,706 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.1.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,707 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.1.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,708 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.2.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,709 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.2.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,710 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.2.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,711 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.2.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,713 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.3.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,714 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.3.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,715 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.3.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,716 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.3.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,718 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.4.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,719 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.4.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,720 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.4.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,721 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.4.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,722 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.5.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,723 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.5.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,724 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.5.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,726 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.5.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,727 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.6.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,728 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.6.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,729 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.6.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,730 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.6.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,732 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.7.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,733 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.7.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,734 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.7.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,735 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.7.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,736 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.8.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,737 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.8.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,738 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.8.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,739 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.8.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,740 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.9.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,742 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.9.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,743 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.9.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,744 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.9.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,745 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.10.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,746 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.10.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,747 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.10.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,748 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.10.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,750 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.11.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,751 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.11.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,752 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.11.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,753 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.11.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,754 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.12.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,755 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.12.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,756 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.12.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,757 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.12.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,759 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.13.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,760 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.13.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,761 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.13.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,762 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.13.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,763 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.14.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,764 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.14.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,765 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.14.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,766 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.14.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,767 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.15.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,774 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.15.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,777 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.15.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,779 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.15.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,780 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.16.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,781 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.16.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,782 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.16.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,784 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.16.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,785 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.17.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,786 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.17.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,787 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.17.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,788 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.17.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,789 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.18.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,790 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.18.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,792 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.18.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,793 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.18.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,794 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.19.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,795 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.19.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,796 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.19.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,798 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.19.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,799 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.20.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,800 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.20.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,801 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.20.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,802 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.20.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,804 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.21.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,805 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.21.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,806 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.21.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,807 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.21.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,808 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.22.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,809 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.22.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,811 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.22.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,812 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.22.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,813 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.23.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,814 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.23.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,815 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.23.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,816 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.23.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,818 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.24.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,819 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.24.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,820 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.24.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,821 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.24.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,822 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.25.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,823 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.25.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,824 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.25.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,825 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.25.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,827 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.26.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,828 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.26.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,829 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.26.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,830 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.26.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,831 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.27.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,832 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.27.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,834 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.27.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,835 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.27.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,836 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.28.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,837 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.28.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,838 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.28.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,839 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.28.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,840 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.29.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,841 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.29.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,843 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.29.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,844 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.29.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,845 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.30.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,846 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.30.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,847 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.30.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,848 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.30.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,849 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.31.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,851 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.31.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,852 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.31.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,853 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.31.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,854 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.32.self_attn.k_LoRA.LoRA_A
2024-01-23 19:36:31,855 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.32.self_attn.k_LoRA.LoRA_B
2024-01-23 19:36:31,856 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.32.self_attn.v_LoRA.LoRA_A
2024-01-23 19:36:31,857 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.32.self_attn.v_LoRA.LoRA_B
2024-01-23 19:36:31,859 - PROT.PROT.models.ESM2_multitask.model - INFO - 	ss8.0.weight
2024-01-23 19:36:31,860 - PROT.PROT.models.ESM2_multitask.model - INFO - 	ss8.0.bias
2024-01-23 19:36:31,861 - PROT.PROT.models.ESM2_multitask.model - INFO - 	ss3.0.weight
2024-01-23 19:36:31,862 - PROT.PROT.models.ESM2_multitask.model - INFO - 	ss3.0.bias
2024-01-23 19:36:31,863 - PROT.PROT.models.ESM2_multitask.model - INFO - 	disorder.0.weight
2024-01-23 19:36:31,864 - PROT.PROT.models.ESM2_multitask.model - INFO - 	disorder.0.bias
2024-01-23 19:36:31,865 - PROT.PROT.models.ESM2_multitask.model - INFO - 	rsa.0.weight
2024-01-23 19:36:31,866 - PROT.PROT.models.ESM2_multitask.model - INFO - 	rsa.0.bias
2024-01-23 19:36:31,868 - PROT.PROT.models.ESM2_multitask.model - INFO - 	phi.0.weight
2024-01-23 19:36:31,869 - PROT.PROT.models.ESM2_multitask.model - INFO - 	phi.0.bias
2024-01-23 19:36:31,870 - PROT.PROT.models.ESM2_multitask.model - INFO - 	psi.0.weight
2024-01-23 19:36:31,871 - PROT.PROT.models.ESM2_multitask.model - INFO - 	psi.0.bias
2024-01-23 19:36:31,872 - PROT.PROT.models.ESM2_multitask.model - INFO - 	tasa.0.weight
2024-01-23 19:36:31,873 - PROT.PROT.models.ESM2_multitask.model - INFO - 	tasa.0.bias
2024-01-23 19:36:31,874 - PROT.PROT.models.ESM2_multitask.model - INFO - 	thsa.0.weight
2024-01-23 19:36:31,875 - PROT.PROT.models.ESM2_multitask.model - INFO - 	thsa.0.bias
2024-01-23 19:36:31,877 - PROT.PROT.models.ESM2_multitask.model - INFO - 	lhp.0.weight
2024-01-23 19:36:31,878 - PROT.PROT.models.ESM2_multitask.model - INFO - 	lhp.0.bias
2024-01-23 19:36:31,879 - PROT.PROT.models.ESM2_multitask.model - INFO - 	hp_loc.0.weight
2024-01-23 19:36:31,880 - PROT.PROT.models.ESM2_multitask.model - INFO - 	hp_loc.0.bias
2024-01-23 19:36:31,881 - PROT.PROT.models.ESM2_multitask.model - INFO - 	lhp_loc.0.weight
2024-01-23 19:36:31,882 - PROT.PROT.models.ESM2_multitask.model - INFO - 	lhp_loc.0.bias
2024-01-23 19:36:31,884 - PROT.PROT.models.ESM2_multitask.model - INFO - 	species.0.weight
2024-01-23 19:36:31,885 - PROT.PROT.models.ESM2_multitask.model - INFO - 	species.0.bias
2024-01-23 19:36:31,886 - PROT.PROT.models.ESM2_multitask.model - INFO - 	expression.0.weight
2024-01-23 19:36:31,887 - PROT.PROT.models.ESM2_multitask.model - INFO - 	expression.0.bias
2024-01-23 19:36:31,888 - PROT.PROT.models.ESM2_multitask.model - INFO - 	aggregation.0.weight
2024-01-23 19:36:31,889 - PROT.PROT.models.ESM2_multitask.model - INFO - 	aggregation.0.bias
2024-01-23 19:36:31,892 - PROT.PROT.main - INFO - Building: PROT.data_loader.augmentation.sparse_token
2024-01-23 19:36:38,698 - PROT.PROT.main - INFO - Building: PROT.data_loader.data_loaders.NSPDataLoader
2024-01-23 19:37:07,978 - PROT.PROT.main - INFO - Getting loss and metric function handles
2024-01-23 19:37:07,979 - PROT.PROT.main - INFO - Initialising trainer
GRADIENT ACCUMULATION 6
2024-01-23 19:37:07,992 - PROT.PROT.base.base_trainer - INFO - Starting training...
Multi Task Loss
SS8 : 1 // SS3: 5 // DIS: 5 // RSA: 100 // PHI: 5 // PSI: 5 // TASA: 1e-07 // THSA: 2e-06 // LHP: 5e-06 // HP LOC: 5 // LHP LOC: 5 // SPECIES: 0.04 // EXPRESSION: 0.06
2024-01-23 19:37:15,854 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [0/1068 (0%)] Loss: 0.755530
2024-01-23 19:37:43,749 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [24/1068 (2%)] Loss: 0.748541
2024-01-23 19:38:11,734 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [48/1068 (4%)] Loss: 0.741581
2024-01-23 19:38:39,838 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [72/1068 (7%)] Loss: 0.734654
2024-01-23 19:39:08,065 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [96/1068 (9%)] Loss: 0.727765
2024-01-23 19:39:36,422 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [120/1068 (11%)] Loss: 0.720916
2024-01-23 19:40:04,911 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [144/1068 (13%)] Loss: 0.714112
2024-01-23 19:40:33,488 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [168/1068 (16%)] Loss: 0.707354
2024-01-23 19:41:02,114 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [192/1068 (18%)] Loss: 0.700646
2024-01-23 19:41:30,776 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [216/1068 (20%)] Loss: 0.693990
2024-01-23 19:41:59,494 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [240/1068 (22%)] Loss: 0.687388
2024-01-23 19:42:28,275 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [264/1068 (25%)] Loss: 0.680843
2024-01-23 19:42:57,169 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [288/1068 (27%)] Loss: 0.674358
2024-01-23 19:43:26,018 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [312/1068 (29%)] Loss: 0.667935
2024-01-23 19:43:54,913 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [336/1068 (31%)] Loss: 0.661577
2024-01-23 19:44:23,844 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [360/1068 (34%)] Loss: 0.655287
2024-01-23 19:44:52,790 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [384/1068 (36%)] Loss: 0.649066
2024-01-23 19:45:21,762 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [408/1068 (38%)] Loss: 0.642918
2024-01-23 19:45:50,739 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [432/1068 (40%)] Loss: 0.636846
2024-01-23 19:46:19,739 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [456/1068 (43%)] Loss: 0.630852
2024-01-23 19:46:48,746 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [480/1068 (45%)] Loss: 0.624939
2024-01-23 19:47:17,767 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [504/1068 (47%)] Loss: 0.619110
2024-01-23 19:47:46,883 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [528/1068 (49%)] Loss: 0.613368
2024-01-23 19:48:15,923 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [552/1068 (52%)] Loss: 0.607715
2024-01-23 19:48:44,962 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [576/1068 (54%)] Loss: 0.602155
2024-01-23 19:49:14,003 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [600/1068 (56%)] Loss: 0.596690
2024-01-23 19:49:43,057 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [624/1068 (58%)] Loss: 0.591323
2024-01-23 19:50:12,107 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [648/1068 (61%)] Loss: 0.586057
2024-01-23 19:50:41,157 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [672/1068 (63%)] Loss: 0.580894
2024-01-23 19:51:10,215 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [696/1068 (65%)] Loss: 0.575838
2024-01-23 19:51:39,279 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [720/1068 (67%)] Loss: 0.570891
2024-01-23 19:52:08,347 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [744/1068 (70%)] Loss: 0.566055
2024-01-23 19:52:37,426 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [768/1068 (72%)] Loss: 0.561334
2024-01-23 19:53:06,584 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [792/1068 (74%)] Loss: 0.556729
2024-01-23 19:53:35,658 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [816/1068 (76%)] Loss: 0.552243
2024-01-23 19:54:04,736 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [840/1068 (79%)] Loss: 0.547879
2024-01-23 19:54:33,820 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [864/1068 (81%)] Loss: 0.543637
2024-01-23 19:55:02,883 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [888/1068 (83%)] Loss: 0.539521
2024-01-23 19:55:31,961 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [912/1068 (85%)] Loss: 0.535532
2024-01-23 19:56:01,022 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [936/1068 (88%)] Loss: 0.531672
2024-01-23 19:56:30,092 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [960/1068 (90%)] Loss: 0.527942
2024-01-23 19:56:59,151 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [984/1068 (92%)] Loss: 0.524343
2024-01-23 19:57:28,200 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [1008/1068 (94%)] Loss: 0.520876
2024-01-23 19:57:57,253 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [1032/1068 (97%)] Loss: 0.517543
2024-01-23 19:58:26,363 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [1056/1068 (99%)] Loss: 0.514344
2024-01-23 19:59:00,506 - PROT.PROT.base.base_trainer - INFO - epoch          : 0
2024-01-23 19:59:00,507 - PROT.PROT.base.base_trainer - INFO - loss           : 0.6198335501590768
2024-01-23 19:59:00,508 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0
2024-01-23 19:59:00,509 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0
2024-01-23 19:59:00,511 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0
2024-01-23 19:59:00,512 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0
2024-01-23 19:59:00,513 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0
2024-01-23 19:59:00,515 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0
2024-01-23 19:59:00,516 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 0
2024-01-23 19:59:00,517 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 0
2024-01-23 19:59:00,518 - PROT.PROT.base.base_trainer - INFO - metric_tasa    : 0.0
2024-01-23 19:59:00,519 - PROT.PROT.base.base_trainer - INFO - metric_thsa    : 0.0
2024-01-23 19:59:00,520 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 0.0
2024-01-23 19:59:00,522 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0
2024-01-23 19:59:00,523 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0
2024-01-23 19:59:00,524 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0
2024-01-23 19:59:00,525 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0
2024-01-23 19:59:00,527 - PROT.PROT.base.base_trainer - INFO - metric_species : 0
2024-01-23 19:59:00,528 - PROT.PROT.base.base_trainer - INFO - metric_expression: 0
2024-01-23 19:59:00,529 - PROT.PROT.base.base_trainer - INFO - metric_aggregation: 0.34814815786149766
2024-01-23 19:59:00,530 - PROT.PROT.base.base_trainer - INFO - val_loss       : 0.5111494596515384
2024-01-23 19:59:00,531 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0
2024-01-23 19:59:00,533 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0
2024-01-23 19:59:00,534 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0
2024-01-23 19:59:00,535 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0
2024-01-23 19:59:00,536 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0
2024-01-23 19:59:00,537 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0
2024-01-23 19:59:00,539 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 0
2024-01-23 19:59:00,540 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 0
2024-01-23 19:59:00,541 - PROT.PROT.base.base_trainer - INFO - val_metric_tasa: 0.0
2024-01-23 19:59:00,542 - PROT.PROT.base.base_trainer - INFO - val_metric_thsa: 0.0
2024-01-23 19:59:00,543 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 0.0
2024-01-23 19:59:00,544 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0
2024-01-23 19:59:00,546 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0
2024-01-23 19:59:00,547 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0
2024-01-23 19:59:00,548 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0
2024-01-23 19:59:00,549 - PROT.PROT.base.base_trainer - INFO - val_metric_species: 0
2024-01-23 19:59:00,551 - PROT.PROT.base.base_trainer - INFO - val_metric_expression: 0
2024-01-23 19:59:00,552 - PROT.PROT.base.base_trainer - INFO - val_metric_aggregation: 0.42857143921511515
2024-01-23 19:59:13,401 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2_singletask/0123-193707/checkpoints/checkpoint-epoch0.pth ...
2024-01-23 19:59:26,734 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2_singletask/0123-193707/checkpoints/model_best.pth
2024-01-23 19:59:31,375 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [0/1068 (0%)] Loss: 0.512776
2024-01-23 20:00:00,085 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [24/1068 (2%)] Loss: 0.509686
2024-01-23 20:00:28,843 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [48/1068 (4%)] Loss: 0.506789
2024-01-23 20:00:57,656 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [72/1068 (7%)] Loss: 0.504050
2024-01-23 20:01:26,512 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [96/1068 (9%)] Loss: 0.501455
2024-01-23 20:01:55,413 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [120/1068 (11%)] Loss: 0.498995
2024-01-23 20:02:24,359 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [144/1068 (13%)] Loss: 0.496667
2024-01-23 20:02:53,458 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [168/1068 (16%)] Loss: 0.494468
2024-01-23 20:03:22,441 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [192/1068 (18%)] Loss: 0.492395
2024-01-23 20:03:51,426 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [216/1068 (20%)] Loss: 0.490446
2024-01-23 20:04:20,436 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [240/1068 (22%)] Loss: 0.488617
2024-01-23 20:04:49,446 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [264/1068 (25%)] Loss: 0.486907
2024-01-23 20:05:18,469 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [288/1068 (27%)] Loss: 0.485310
2024-01-23 20:05:47,501 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [312/1068 (29%)] Loss: 0.483825
2024-01-23 20:06:16,536 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [336/1068 (31%)] Loss: 0.482447
2024-01-23 20:06:45,588 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [360/1068 (34%)] Loss: 0.481172
2024-01-23 20:07:14,632 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [384/1068 (36%)] Loss: 0.479996
2024-01-23 20:07:43,743 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [408/1068 (38%)] Loss: 0.478915
2024-01-23 20:08:12,969 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [432/1068 (40%)] Loss: 0.477923
2024-01-23 20:08:42,013 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [456/1068 (43%)] Loss: 0.477017
2024-01-23 20:09:11,069 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [480/1068 (45%)] Loss: 0.476191
2024-01-23 20:09:40,127 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [504/1068 (47%)] Loss: 0.475442
2024-01-23 20:10:09,195 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [528/1068 (49%)] Loss: 0.474763
2024-01-23 20:10:38,253 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [552/1068 (52%)] Loss: 0.474151
2024-01-23 20:11:07,321 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [576/1068 (54%)] Loss: 0.473600
2024-01-23 20:11:36,392 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [600/1068 (56%)] Loss: 0.473107
2024-01-23 20:12:05,465 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [624/1068 (58%)] Loss: 0.472666
2024-01-23 20:12:34,533 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [648/1068 (61%)] Loss: 0.472274
2024-01-23 20:13:03,708 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [672/1068 (63%)] Loss: 0.471926
2024-01-23 20:13:32,823 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [696/1068 (65%)] Loss: 0.471618
2024-01-23 20:14:01,896 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [720/1068 (67%)] Loss: 0.471347
2024-01-23 20:14:30,966 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [744/1068 (70%)] Loss: 0.471109
2024-01-23 20:15:00,027 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [768/1068 (72%)] Loss: 0.470900
2024-01-23 20:15:29,097 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [792/1068 (74%)] Loss: 0.470718
2024-01-23 20:15:58,185 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [816/1068 (76%)] Loss: 0.470560
2024-01-23 20:16:27,263 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [840/1068 (79%)] Loss: 0.470423
2024-01-23 20:16:56,348 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [864/1068 (81%)] Loss: 0.470304
2024-01-23 20:17:25,434 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [888/1068 (83%)] Loss: 0.470202
2024-01-23 20:17:54,506 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [912/1068 (85%)] Loss: 0.470114
2024-01-23 20:18:23,649 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [936/1068 (88%)] Loss: 0.470039
2024-01-23 20:18:52,702 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [960/1068 (90%)] Loss: 0.469974
2024-01-23 20:19:21,769 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [984/1068 (92%)] Loss: 0.469920
2024-01-23 20:19:50,826 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [1008/1068 (94%)] Loss: 0.469873
2024-01-23 20:20:19,885 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [1032/1068 (97%)] Loss: 0.469834
2024-01-23 20:20:48,946 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [1056/1068 (99%)] Loss: 0.469801
2024-01-23 20:21:23,209 - PROT.PROT.base.base_trainer - INFO - epoch          : 1
2024-01-23 20:21:23,209 - PROT.PROT.base.base_trainer - INFO - loss           : 0.480037003671698
2024-01-23 20:21:23,211 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0
2024-01-23 20:21:23,212 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0
2024-01-23 20:21:23,213 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0
2024-01-23 20:21:23,215 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0
2024-01-23 20:21:23,216 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0
2024-01-23 20:21:23,217 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0
2024-01-23 20:21:23,219 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 0
2024-01-23 20:21:23,220 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 0
2024-01-23 20:21:23,221 - PROT.PROT.base.base_trainer - INFO - metric_tasa    : 0.0
2024-01-23 20:21:23,222 - PROT.PROT.base.base_trainer - INFO - metric_thsa    : 0.0
2024-01-23 20:21:23,223 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 0.0
2024-01-23 20:21:23,224 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0
2024-01-23 20:21:23,225 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0
2024-01-23 20:21:23,227 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0
2024-01-23 20:21:23,228 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0
2024-01-23 20:21:23,229 - PROT.PROT.base.base_trainer - INFO - metric_species : 0
2024-01-23 20:21:23,230 - PROT.PROT.base.base_trainer - INFO - metric_expression: 0
2024-01-23 20:21:23,231 - PROT.PROT.base.base_trainer - INFO - metric_aggregation: 0.3481481571992238
2024-01-23 20:21:23,232 - PROT.PROT.base.base_trainer - INFO - val_loss       : 0.46722771653107237
2024-01-23 20:21:23,233 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0
2024-01-23 20:21:23,234 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0
2024-01-23 20:21:23,236 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0
2024-01-23 20:21:23,237 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0
2024-01-23 20:21:23,238 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0
2024-01-23 20:21:23,239 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0
2024-01-23 20:21:23,240 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 0
2024-01-23 20:21:23,241 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 0
2024-01-23 20:21:23,242 - PROT.PROT.base.base_trainer - INFO - val_metric_tasa: 0.0
2024-01-23 20:21:23,243 - PROT.PROT.base.base_trainer - INFO - val_metric_thsa: 0.0
2024-01-23 20:21:23,244 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 0.0
2024-01-23 20:21:23,246 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0
2024-01-23 20:21:23,247 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0
2024-01-23 20:21:23,248 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0
2024-01-23 20:21:23,249 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0
2024-01-23 20:21:23,250 - PROT.PROT.base.base_trainer - INFO - val_metric_species: 0
2024-01-23 20:21:23,251 - PROT.PROT.base.base_trainer - INFO - val_metric_expression: 0
2024-01-23 20:21:23,252 - PROT.PROT.base.base_trainer - INFO - val_metric_aggregation: 0.42857143974729944
2024-01-23 20:21:37,150 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2_singletask/0123-193707/checkpoints/checkpoint-epoch1.pth ...
2024-01-23 20:21:52,080 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2_singletask/0123-193707/checkpoints/model_best.pth
2024-01-23 20:21:56,737 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [0/1068 (0%)] Loss: 0.469782
2024-01-23 20:22:25,412 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [24/1068 (2%)] Loss: 0.469739
2024-01-23 20:22:54,181 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [48/1068 (4%)] Loss: 0.469714
2024-01-23 20:23:23,058 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [72/1068 (7%)] Loss: 0.469697
2024-01-23 20:23:51,912 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [96/1068 (9%)] Loss: 0.469685
2024-01-23 20:24:20,874 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [120/1068 (11%)] Loss: 0.469676
2024-01-23 20:24:49,830 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [144/1068 (13%)] Loss: 0.469669
2024-01-23 20:25:18,801 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [168/1068 (16%)] Loss: 0.469663
2024-01-23 20:25:47,784 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [192/1068 (18%)] Loss: 0.469658
2024-01-23 20:26:16,785 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [216/1068 (20%)] Loss: 0.469654
2024-01-23 20:26:45,801 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [240/1068 (22%)] Loss: 0.469651
2024-01-23 20:27:14,826 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [264/1068 (25%)] Loss: 0.469648
2024-01-23 20:27:43,862 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [288/1068 (27%)] Loss: 0.469646
2024-01-23 20:28:12,990 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [312/1068 (29%)] Loss: 0.469644
2024-01-23 20:28:42,040 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [336/1068 (31%)] Loss: 0.469643
2024-01-23 20:29:11,100 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [360/1068 (34%)] Loss: 0.469641
2024-01-23 20:29:40,162 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [384/1068 (36%)] Loss: 0.469640
2024-01-23 20:30:09,226 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [408/1068 (38%)] Loss: 0.469640
2024-01-23 20:30:38,298 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [432/1068 (40%)] Loss: 0.469639
2024-01-23 20:31:07,352 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [456/1068 (43%)] Loss: 0.469638
2024-01-23 20:31:36,436 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [480/1068 (45%)] Loss: 0.469638
2024-01-23 20:32:05,517 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [504/1068 (47%)] Loss: 0.469638
2024-01-23 20:32:34,568 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [528/1068 (49%)] Loss: 0.469638
2024-01-23 20:33:03,718 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [552/1068 (52%)] Loss: 0.469637
2024-01-23 20:33:32,776 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [576/1068 (54%)] Loss: 0.469637
2024-01-23 20:34:01,834 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [600/1068 (56%)] Loss: 0.469637
2024-01-23 20:34:30,899 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [624/1068 (58%)] Loss: 0.469637
2024-01-23 20:34:59,953 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [648/1068 (61%)] Loss: 0.469637
2024-01-23 20:35:29,000 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [672/1068 (63%)] Loss: 0.469637
2024-01-23 20:35:58,057 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [696/1068 (65%)] Loss: 0.469637
2024-01-23 20:36:27,112 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [720/1068 (67%)] Loss: 0.469637
2024-01-23 20:36:56,169 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [744/1068 (70%)] Loss: 0.469637
2024-01-23 20:37:25,215 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [768/1068 (72%)] Loss: 0.469637
2024-01-23 20:37:54,252 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [792/1068 (74%)] Loss: 0.469637
2024-01-23 20:38:23,379 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [816/1068 (76%)] Loss: 0.469637
2024-01-23 20:38:52,417 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [840/1068 (79%)] Loss: 0.469637
2024-01-23 20:39:21,457 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [864/1068 (81%)] Loss: 0.469637
2024-01-23 20:39:50,497 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [888/1068 (83%)] Loss: 0.469637
2024-01-23 20:40:19,537 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [912/1068 (85%)] Loss: 0.469637
2024-01-23 20:40:48,578 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [936/1068 (88%)] Loss: 0.469637
2024-01-23 20:41:17,618 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [960/1068 (90%)] Loss: 0.469637
2024-01-23 20:41:46,666 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [984/1068 (92%)] Loss: 0.469637
2024-01-23 20:42:15,730 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [1008/1068 (94%)] Loss: 0.469637
2024-01-23 20:42:44,777 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [1032/1068 (97%)] Loss: 0.469637
2024-01-23 20:43:13,825 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [1056/1068 (99%)] Loss: 0.469637
2024-01-23 20:43:48,307 - PROT.PROT.base.base_trainer - INFO - epoch          : 2
2024-01-23 20:43:48,308 - PROT.PROT.base.base_trainer - INFO - loss           : 0.46951157612268296
2024-01-23 20:43:48,309 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0
2024-01-23 20:43:48,310 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0
2024-01-23 20:43:48,311 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0
2024-01-23 20:43:48,313 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0
2024-01-23 20:43:48,314 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0
2024-01-23 20:43:48,315 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0
2024-01-23 20:43:48,316 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 0
2024-01-23 20:43:48,318 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 0
2024-01-23 20:43:48,319 - PROT.PROT.base.base_trainer - INFO - metric_tasa    : 0.0
2024-01-23 20:43:48,320 - PROT.PROT.base.base_trainer - INFO - metric_thsa    : 0.0
2024-01-23 20:43:48,322 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 0.0
2024-01-23 20:43:48,323 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0
2024-01-23 20:43:48,324 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0
2024-01-23 20:43:48,326 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0
2024-01-23 20:43:48,327 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0
2024-01-23 20:43:48,328 - PROT.PROT.base.base_trainer - INFO - metric_species : 0
2024-01-23 20:43:48,329 - PROT.PROT.base.base_trainer - INFO - metric_expression: 0
2024-01-23 20:43:48,331 - PROT.PROT.base.base_trainer - INFO - metric_aggregation: 0.3703703780968984
2024-01-23 20:43:48,332 - PROT.PROT.base.base_trainer - INFO - val_loss       : 0.4670103458421571
2024-01-23 20:43:48,333 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0
2024-01-23 20:43:48,334 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0
2024-01-23 20:43:48,336 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0
2024-01-23 20:43:48,337 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0
2024-01-23 20:43:48,338 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0
2024-01-23 20:43:48,339 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0
2024-01-23 20:43:48,340 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 0
2024-01-23 20:43:48,342 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 0
2024-01-23 20:43:48,343 - PROT.PROT.base.base_trainer - INFO - val_metric_tasa: 0.0
2024-01-23 20:43:48,344 - PROT.PROT.base.base_trainer - INFO - val_metric_thsa: 0.0
2024-01-23 20:43:48,345 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 0.0
2024-01-23 20:43:48,346 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0
2024-01-23 20:43:48,348 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0
2024-01-23 20:43:48,349 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0
2024-01-23 20:43:48,350 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0
2024-01-23 20:43:48,352 - PROT.PROT.base.base_trainer - INFO - val_metric_species: 0
2024-01-23 20:43:48,353 - PROT.PROT.base.base_trainer - INFO - val_metric_expression: 0
2024-01-23 20:43:48,354 - PROT.PROT.base.base_trainer - INFO - val_metric_aggregation: 0.42857144081166815
2024-01-23 20:44:00,984 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2_singletask/0123-193707/checkpoints/checkpoint-epoch2.pth ...
2024-01-23 20:44:14,520 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2_singletask/0123-193707/checkpoints/model_best.pth
2024-01-23 20:44:18,605 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [0/1068 (0%)] Loss: 0.469637
2024-01-23 20:44:47,269 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [24/1068 (2%)] Loss: 0.469639
2024-01-23 20:45:16,025 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [48/1068 (4%)] Loss: 0.469640
2024-01-23 20:45:44,851 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [72/1068 (7%)] Loss: 0.469640
2024-01-23 20:46:13,713 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [96/1068 (9%)] Loss: 0.469640
2024-01-23 20:46:42,609 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [120/1068 (11%)] Loss: 0.469639
2024-01-23 20:47:11,525 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [144/1068 (13%)] Loss: 0.469639
2024-01-23 20:47:40,606 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [168/1068 (16%)] Loss: 0.469638
2024-01-23 20:48:09,555 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [192/1068 (18%)] Loss: 0.469638
2024-01-23 20:48:38,538 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [216/1068 (20%)] Loss: 0.469637
2024-01-23 20:49:07,527 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [240/1068 (22%)] Loss: 0.469637
2024-01-23 20:49:36,514 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [264/1068 (25%)] Loss: 0.469637
2024-01-23 20:50:05,504 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [288/1068 (27%)] Loss: 0.469637
2024-01-23 20:50:34,510 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [312/1068 (29%)] Loss: 0.469637
2024-01-23 20:51:03,511 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [336/1068 (31%)] Loss: 0.469637
2024-01-23 20:51:32,536 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [360/1068 (34%)] Loss: 0.469637
2024-01-23 20:52:01,543 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [384/1068 (36%)] Loss: 0.469637
2024-01-23 20:52:30,642 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [408/1068 (38%)] Loss: 0.469637
2024-01-23 20:52:59,663 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [432/1068 (40%)] Loss: 0.469637
2024-01-23 20:53:28,673 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [456/1068 (43%)] Loss: 0.469637
2024-01-23 20:53:57,688 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [480/1068 (45%)] Loss: 0.469637
2024-01-23 20:54:26,703 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [504/1068 (47%)] Loss: 0.469637
2024-01-23 20:54:55,717 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [528/1068 (49%)] Loss: 0.469637
2024-01-23 20:55:24,750 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [552/1068 (52%)] Loss: 0.469637
2024-01-23 20:55:53,820 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [576/1068 (54%)] Loss: 0.469637
2024-01-23 20:56:22,881 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [600/1068 (56%)] Loss: 0.469637
2024-01-23 20:56:51,890 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [624/1068 (58%)] Loss: 0.469637
2024-01-23 20:57:20,824 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [648/1068 (61%)] Loss: 0.469637
2024-01-23 20:57:49,948 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [672/1068 (63%)] Loss: 0.469636
2024-01-23 20:58:18,969 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [696/1068 (65%)] Loss: 0.469636
2024-01-23 20:58:47,978 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [720/1068 (67%)] Loss: 0.469637
2024-01-23 20:59:16,991 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [744/1068 (70%)] Loss: 0.469637
2024-01-23 20:59:46,004 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [768/1068 (72%)] Loss: 0.469637
2024-01-23 21:00:15,018 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [792/1068 (74%)] Loss: 0.469637
2024-01-23 21:00:44,033 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [816/1068 (76%)] Loss: 0.469637
2024-01-23 21:01:13,045 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [840/1068 (79%)] Loss: 0.469637
2024-01-23 21:01:42,057 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [864/1068 (81%)] Loss: 0.469637
2024-01-23 21:02:11,060 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [888/1068 (83%)] Loss: 0.469636
2024-01-23 21:02:40,069 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [912/1068 (85%)] Loss: 0.469637
2024-01-23 21:03:09,170 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [936/1068 (88%)] Loss: 0.469637
2024-01-23 21:03:38,186 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [960/1068 (90%)] Loss: 0.469637
2024-01-23 21:04:07,193 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [984/1068 (92%)] Loss: 0.469636
2024-01-23 21:04:36,192 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [1008/1068 (94%)] Loss: 0.469637
2024-01-23 21:05:05,224 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [1032/1068 (97%)] Loss: 0.469637
2024-01-23 21:05:34,260 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [1056/1068 (99%)] Loss: 0.469637
2024-01-23 21:06:07,983 - PROT.PROT.base.base_trainer - INFO - epoch          : 3
2024-01-23 21:06:07,984 - PROT.PROT.base.base_trainer - INFO - loss           : 0.46949917424686854
2024-01-23 21:06:07,985 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0
2024-01-23 21:06:07,986 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0
2024-01-23 21:06:07,988 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0
2024-01-23 21:06:07,989 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0
2024-01-23 21:06:07,990 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0
2024-01-23 21:06:07,992 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0
2024-01-23 21:06:07,993 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 0
2024-01-23 21:06:07,994 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 0
2024-01-23 21:06:07,995 - PROT.PROT.base.base_trainer - INFO - metric_tasa    : 0.0
2024-01-23 21:06:07,996 - PROT.PROT.base.base_trainer - INFO - metric_thsa    : 0.0
2024-01-23 21:06:07,997 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 0.0
2024-01-23 21:06:07,998 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0
2024-01-23 21:06:08,000 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0
2024-01-23 21:06:08,001 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0
2024-01-23 21:06:08,002 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0
2024-01-23 21:06:08,003 - PROT.PROT.base.base_trainer - INFO - metric_species : 0
2024-01-23 21:06:08,004 - PROT.PROT.base.base_trainer - INFO - metric_expression: 0
2024-01-23 21:06:08,006 - PROT.PROT.base.base_trainer - INFO - metric_aggregation: 0.37037038008371986
2024-01-23 21:06:08,007 - PROT.PROT.base.base_trainer - INFO - val_loss       : 0.46700959652662277
2024-01-23 21:06:08,008 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0
2024-01-23 21:06:08,009 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0
2024-01-23 21:06:08,010 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0
2024-01-23 21:06:08,011 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0
2024-01-23 21:06:08,012 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0
2024-01-23 21:06:08,014 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0
2024-01-23 21:06:08,015 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 0
2024-01-23 21:06:08,016 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 0
2024-01-23 21:06:08,017 - PROT.PROT.base.base_trainer - INFO - val_metric_tasa: 0.0
2024-01-23 21:06:08,018 - PROT.PROT.base.base_trainer - INFO - val_metric_thsa: 0.0
2024-01-23 21:06:08,019 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 0.0
2024-01-23 21:06:08,020 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0
2024-01-23 21:06:08,021 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0
2024-01-23 21:06:08,023 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0
2024-01-23 21:06:08,024 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0
2024-01-23 21:06:08,025 - PROT.PROT.base.base_trainer - INFO - val_metric_species: 0
2024-01-23 21:06:08,026 - PROT.PROT.base.base_trainer - INFO - val_metric_expression: 0
2024-01-23 21:06:08,027 - PROT.PROT.base.base_trainer - INFO - val_metric_aggregation: 0.42857143602200914
2024-01-23 21:06:20,710 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2_singletask/0123-193707/checkpoints/checkpoint-epoch3.pth ...
2024-01-23 21:06:34,061 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2_singletask/0123-193707/checkpoints/model_best.pth
2024-01-23 21:06:38,628 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [0/1068 (0%)] Loss: 0.469637
2024-01-23 21:07:07,444 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [24/1068 (2%)] Loss: 0.469640
2024-01-23 21:07:36,167 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [48/1068 (4%)] Loss: 0.469642
2024-01-23 21:08:04,939 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [72/1068 (7%)] Loss: 0.469642
2024-01-23 21:08:33,754 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [96/1068 (9%)] Loss: 0.469641
2024-01-23 21:09:02,612 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [120/1068 (11%)] Loss: 0.469640
2024-01-23 21:09:31,508 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [144/1068 (13%)] Loss: 0.469639
2024-01-23 21:10:00,417 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [168/1068 (16%)] Loss: 0.469638
2024-01-23 21:10:29,337 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [192/1068 (18%)] Loss: 0.469638
2024-01-23 21:10:58,289 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [216/1068 (20%)] Loss: 0.469637
2024-01-23 21:11:27,245 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [240/1068 (22%)] Loss: 0.469637
2024-01-23 21:11:56,211 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [264/1068 (25%)] Loss: 0.469637
2024-01-23 21:12:25,190 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [288/1068 (27%)] Loss: 0.469637
2024-01-23 21:12:54,170 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [312/1068 (29%)] Loss: 0.469637
2024-01-23 21:13:23,155 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [336/1068 (31%)] Loss: 0.469637
2024-01-23 21:13:52,140 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [360/1068 (34%)] Loss: 0.469637
2024-01-23 21:14:21,132 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [384/1068 (36%)] Loss: 0.469637
2024-01-23 21:14:50,118 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [408/1068 (38%)] Loss: 0.469637
2024-01-23 21:15:19,097 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [432/1068 (40%)] Loss: 0.469637
2024-01-23 21:15:48,089 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [456/1068 (43%)] Loss: 0.469637
2024-01-23 21:16:17,079 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [480/1068 (45%)] Loss: 0.469637
2024-01-23 21:16:46,060 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [504/1068 (47%)] Loss: 0.469636
2024-01-23 21:17:15,041 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [528/1068 (49%)] Loss: 0.469637
2024-01-23 21:17:44,124 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [552/1068 (52%)] Loss: 0.469637
2024-01-23 21:18:13,120 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [576/1068 (54%)] Loss: 0.469637
2024-01-23 21:18:42,125 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [600/1068 (56%)] Loss: 0.469636
2024-01-23 21:19:11,126 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [624/1068 (58%)] Loss: 0.469637
2024-01-23 21:19:40,135 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [648/1068 (61%)] Loss: 0.469637
2024-01-23 21:20:09,138 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [672/1068 (63%)] Loss: 0.469636
2024-01-23 21:20:38,131 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [696/1068 (65%)] Loss: 0.469636
2024-01-23 21:21:07,128 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [720/1068 (67%)] Loss: 0.469637
2024-01-23 21:21:36,127 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [744/1068 (70%)] Loss: 0.469637
2024-01-23 21:22:05,134 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [768/1068 (72%)] Loss: 0.469637
2024-01-23 21:22:34,220 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [792/1068 (74%)] Loss: 0.469637
2024-01-23 21:23:03,222 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [816/1068 (76%)] Loss: 0.469637
2024-01-23 21:23:32,228 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [840/1068 (79%)] Loss: 0.469637
2024-01-23 21:24:01,228 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [864/1068 (81%)] Loss: 0.469637
2024-01-23 21:24:30,232 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [888/1068 (83%)] Loss: 0.469636
2024-01-23 21:24:59,246 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [912/1068 (85%)] Loss: 0.469636
2024-01-23 21:25:28,253 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [936/1068 (88%)] Loss: 0.469636
2024-01-23 21:25:57,256 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [960/1068 (90%)] Loss: 0.469636
2024-01-23 21:26:26,268 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [984/1068 (92%)] Loss: 0.469636
2024-01-23 21:26:55,272 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [1008/1068 (94%)] Loss: 0.469636
2024-01-23 21:27:24,323 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [1032/1068 (97%)] Loss: 0.469636
2024-01-23 21:27:53,422 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [1056/1068 (99%)] Loss: 0.469636
2024-01-23 21:28:27,617 - PROT.PROT.base.base_trainer - INFO - epoch          : 4
2024-01-23 21:28:27,618 - PROT.PROT.base.base_trainer - INFO - loss           : 0.46949934844787305
2024-01-23 21:28:27,619 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0
2024-01-23 21:28:27,621 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0
2024-01-23 21:28:27,622 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0
2024-01-23 21:28:27,623 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0
2024-01-23 21:28:27,624 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0
2024-01-23 21:28:27,626 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0
2024-01-23 21:28:27,627 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 0
2024-01-23 21:28:27,628 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 0
2024-01-23 21:28:27,630 - PROT.PROT.base.base_trainer - INFO - metric_tasa    : 0.0
2024-01-23 21:28:27,631 - PROT.PROT.base.base_trainer - INFO - metric_thsa    : 0.0
2024-01-23 21:28:27,632 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 0.0
2024-01-23 21:28:27,633 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0
2024-01-23 21:28:27,634 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0
2024-01-23 21:28:27,636 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0
2024-01-23 21:28:27,637 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0
2024-01-23 21:28:27,638 - PROT.PROT.base.base_trainer - INFO - metric_species : 0
2024-01-23 21:28:27,639 - PROT.PROT.base.base_trainer - INFO - metric_expression: 0
2024-01-23 21:28:27,640 - PROT.PROT.base.base_trainer - INFO - metric_aggregation: 0.29629630512661403
2024-01-23 21:28:27,641 - PROT.PROT.base.base_trainer - INFO - val_loss       : 0.46700926763670786
2024-01-23 21:28:27,643 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0
2024-01-23 21:28:27,644 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0
2024-01-23 21:28:27,645 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0
2024-01-23 21:28:27,646 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0
2024-01-23 21:28:27,647 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0
2024-01-23 21:28:27,649 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0
2024-01-23 21:28:27,650 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 0
2024-01-23 21:28:27,651 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 0
2024-01-23 21:28:27,652 - PROT.PROT.base.base_trainer - INFO - val_metric_tasa: 0.0
2024-01-23 21:28:27,657 - PROT.PROT.base.base_trainer - INFO - val_metric_thsa: 0.0
2024-01-23 21:28:27,660 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 0.0
2024-01-23 21:28:27,662 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0
2024-01-23 21:28:27,664 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0
2024-01-23 21:28:27,665 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0
2024-01-23 21:28:27,666 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0
2024-01-23 21:28:27,667 - PROT.PROT.base.base_trainer - INFO - val_metric_species: 0
2024-01-23 21:28:27,668 - PROT.PROT.base.base_trainer - INFO - val_metric_expression: 0
2024-01-23 21:28:27,669 - PROT.PROT.base.base_trainer - INFO - val_metric_aggregation: 0.4285714386829308
2024-01-23 21:28:41,185 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2_singletask/0123-193707/checkpoints/checkpoint-epoch4.pth ...
2024-01-23 21:28:57,170 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2_singletask/0123-193707/checkpoints/model_best.pth
2024-01-23 21:29:01,828 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [0/1068 (0%)] Loss: 0.469637
2024-01-23 21:29:30,511 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [24/1068 (2%)] Loss: 0.469642
2024-01-23 21:29:59,274 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [48/1068 (4%)] Loss: 0.469645
2024-01-23 21:30:28,062 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [72/1068 (7%)] Loss: 0.469645
2024-01-23 21:30:56,872 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [96/1068 (9%)] Loss: 0.469643
2024-01-23 21:31:25,728 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [120/1068 (11%)] Loss: 0.469641
2024-01-23 21:31:54,676 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [144/1068 (13%)] Loss: 0.469639
2024-01-23 21:32:23,817 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [168/1068 (16%)] Loss: 0.469638
2024-01-23 21:32:52,750 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [192/1068 (18%)] Loss: 0.469638
2024-01-23 21:33:21,695 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [216/1068 (20%)] Loss: 0.469637
2024-01-23 21:33:50,646 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [240/1068 (22%)] Loss: 0.469637
2024-01-23 21:34:19,623 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [264/1068 (25%)] Loss: 0.469637
2024-01-23 21:34:48,611 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [288/1068 (27%)] Loss: 0.469637
2024-01-23 21:35:17,590 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [312/1068 (29%)] Loss: 0.469637
2024-01-23 21:35:46,579 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [336/1068 (31%)] Loss: 0.469637
2024-01-23 21:36:15,584 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [360/1068 (34%)] Loss: 0.469637
2024-01-23 21:36:44,591 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [384/1068 (36%)] Loss: 0.469637
2024-01-23 21:37:13,602 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [408/1068 (38%)] Loss: 0.469637
2024-01-23 21:37:42,632 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [432/1068 (40%)] Loss: 0.469637
2024-01-23 21:38:11,651 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [456/1068 (43%)] Loss: 0.469637
2024-01-23 21:38:40,676 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [480/1068 (45%)] Loss: 0.469637
2024-01-23 21:39:09,710 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [504/1068 (47%)] Loss: 0.469637
2024-01-23 21:39:38,745 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [528/1068 (49%)] Loss: 0.469637
2024-01-23 21:40:07,778 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [552/1068 (52%)] Loss: 0.469637
2024-01-23 21:40:36,848 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [576/1068 (54%)] Loss: 0.469637
2024-01-23 21:41:05,888 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [600/1068 (56%)] Loss: 0.469637
2024-01-23 21:41:34,946 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [624/1068 (58%)] Loss: 0.469637
2024-01-23 21:42:03,999 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [648/1068 (61%)] Loss: 0.469637
2024-01-23 21:42:33,139 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [672/1068 (63%)] Loss: 0.469637
2024-01-23 21:43:02,262 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [696/1068 (65%)] Loss: 0.469637
2024-01-23 21:43:31,323 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [720/1068 (67%)] Loss: 0.469637
2024-01-23 21:44:00,382 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [744/1068 (70%)] Loss: 0.469637
2024-01-23 21:44:29,444 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [768/1068 (72%)] Loss: 0.469637
2024-01-23 21:44:58,536 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [792/1068 (74%)] Loss: 0.469637
2024-01-23 21:45:27,586 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [816/1068 (76%)] Loss: 0.469637
2024-01-23 21:45:56,628 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [840/1068 (79%)] Loss: 0.469637
2024-01-23 21:46:25,677 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [864/1068 (81%)] Loss: 0.469637
2024-01-23 21:46:54,723 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [888/1068 (83%)] Loss: 0.469637
2024-01-23 21:47:23,767 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [912/1068 (85%)] Loss: 0.469637
2024-01-23 21:47:52,898 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [936/1068 (88%)] Loss: 0.469637
2024-01-23 21:48:21,948 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [960/1068 (90%)] Loss: 0.469637
2024-01-23 21:48:50,999 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [984/1068 (92%)] Loss: 0.469637
2024-01-23 21:49:20,062 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [1008/1068 (94%)] Loss: 0.469637
2024-01-23 21:49:49,122 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [1032/1068 (97%)] Loss: 0.469637
2024-01-23 21:50:18,198 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [1056/1068 (99%)] Loss: 0.469637
2024-01-23 21:50:52,354 - PROT.PROT.base.base_trainer - INFO - epoch          : 5
2024-01-23 21:50:52,355 - PROT.PROT.base.base_trainer - INFO - loss           : 0.46949954093285395
2024-01-23 21:50:52,357 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0
2024-01-23 21:50:52,358 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0
2024-01-23 21:50:52,359 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0
2024-01-23 21:50:52,361 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0
2024-01-23 21:50:52,362 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0
2024-01-23 21:50:52,363 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0
2024-01-23 21:50:52,364 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 0
2024-01-23 21:50:52,365 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 0
2024-01-23 21:50:52,366 - PROT.PROT.base.base_trainer - INFO - metric_tasa    : 0.0
2024-01-23 21:50:52,368 - PROT.PROT.base.base_trainer - INFO - metric_thsa    : 0.0
2024-01-23 21:50:52,369 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 0.0
2024-01-23 21:50:52,370 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0
2024-01-23 21:50:52,371 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0
2024-01-23 21:50:52,372 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0
2024-01-23 21:50:52,373 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0
2024-01-23 21:50:52,374 - PROT.PROT.base.base_trainer - INFO - metric_species : 0
2024-01-23 21:50:52,375 - PROT.PROT.base.base_trainer - INFO - metric_expression: 0
2024-01-23 21:50:52,376 - PROT.PROT.base.base_trainer - INFO - metric_aggregation: 0.3481481571992238
2024-01-23 21:50:52,378 - PROT.PROT.base.base_trainer - INFO - val_loss       : 0.4670087567397526
2024-01-23 21:50:52,379 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0
2024-01-23 21:50:52,380 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0
2024-01-23 21:50:52,381 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0
2024-01-23 21:50:52,382 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0
2024-01-23 21:50:52,383 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0
2024-01-23 21:50:52,384 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0
2024-01-23 21:50:52,385 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 0
2024-01-23 21:50:52,387 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 0
2024-01-23 21:50:52,388 - PROT.PROT.base.base_trainer - INFO - val_metric_tasa: 0.0
2024-01-23 21:50:52,389 - PROT.PROT.base.base_trainer - INFO - val_metric_thsa: 0.0
2024-01-23 21:50:52,390 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 0.0
2024-01-23 21:50:52,391 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0
2024-01-23 21:50:52,392 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0
2024-01-23 21:50:52,393 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0
2024-01-23 21:50:52,394 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0
2024-01-23 21:50:52,395 - PROT.PROT.base.base_trainer - INFO - val_metric_species: 0
2024-01-23 21:50:52,396 - PROT.PROT.base.base_trainer - INFO - val_metric_expression: 0
2024-01-23 21:50:52,398 - PROT.PROT.base.base_trainer - INFO - val_metric_aggregation: 0.4285714381507465
2024-01-23 21:51:04,918 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2_singletask/0123-193707/checkpoints/checkpoint-epoch5.pth ...
2024-01-23 21:51:17,896 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2_singletask/0123-193707/checkpoints/model_best.pth
2024-01-23 21:51:22,498 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [0/1068 (0%)] Loss: 0.469637
2024-01-23 21:51:51,207 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [24/1068 (2%)] Loss: 0.469644
2024-01-23 21:52:19,939 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [48/1068 (4%)] Loss: 0.469648
2024-01-23 21:52:48,695 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [72/1068 (7%)] Loss: 0.469647
2024-01-23 21:53:17,457 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [96/1068 (9%)] Loss: 0.469644
2024-01-23 21:53:46,234 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [120/1068 (11%)] Loss: 0.469641
2024-01-23 21:54:15,016 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [144/1068 (13%)] Loss: 0.469639
2024-01-23 21:54:43,824 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [168/1068 (16%)] Loss: 0.469638
2024-01-23 21:55:12,630 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [192/1068 (18%)] Loss: 0.469637
2024-01-23 21:55:41,436 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [216/1068 (20%)] Loss: 0.469637
2024-01-23 21:56:10,248 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [240/1068 (22%)] Loss: 0.469637
2024-01-23 21:56:39,057 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [264/1068 (25%)] Loss: 0.469637
2024-01-23 21:57:07,870 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [288/1068 (27%)] Loss: 0.469637
2024-01-23 21:57:36,764 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [312/1068 (29%)] Loss: 0.469637
2024-01-23 21:58:05,562 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [336/1068 (31%)] Loss: 0.469636
2024-01-23 21:58:34,375 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [360/1068 (34%)] Loss: 0.469637
2024-01-23 21:59:03,181 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [384/1068 (36%)] Loss: 0.469637
2024-01-23 21:59:31,982 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [408/1068 (38%)] Loss: 0.469637
2024-01-23 22:00:00,787 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [432/1068 (40%)] Loss: 0.469637
2024-01-23 22:00:29,587 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [456/1068 (43%)] Loss: 0.469637
2024-01-23 22:00:58,383 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [480/1068 (45%)] Loss: 0.469637
2024-01-23 22:01:27,250 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [504/1068 (47%)] Loss: 0.469637
2024-01-23 22:01:56,095 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [528/1068 (49%)] Loss: 0.469637
2024-01-23 22:02:24,897 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [552/1068 (52%)] Loss: 0.469637
2024-01-23 22:02:53,772 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [576/1068 (54%)] Loss: 0.469637
2024-01-23 22:03:22,575 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [600/1068 (56%)] Loss: 0.469637
2024-01-23 22:03:51,365 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [624/1068 (58%)] Loss: 0.469637
2024-01-23 22:04:20,152 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [648/1068 (61%)] Loss: 0.469637
2024-01-23 22:04:48,945 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [672/1068 (63%)] Loss: 0.469637
2024-01-23 22:05:17,741 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [696/1068 (65%)] Loss: 0.469637
2024-01-23 22:05:46,535 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [720/1068 (67%)] Loss: 0.469637
2024-01-23 22:06:15,431 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [744/1068 (70%)] Loss: 0.469637
2024-01-23 22:06:44,294 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [768/1068 (72%)] Loss: 0.469637
2024-01-23 22:07:13,123 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [792/1068 (74%)] Loss: 0.469637
2024-01-23 22:07:41,903 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [816/1068 (76%)] Loss: 0.469637
2024-01-23 22:08:10,777 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [840/1068 (79%)] Loss: 0.469637
2024-01-23 22:08:39,563 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [864/1068 (81%)] Loss: 0.469637
2024-01-23 22:09:08,353 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [888/1068 (83%)] Loss: 0.469637
2024-01-23 22:09:37,146 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [912/1068 (85%)] Loss: 0.469637
2024-01-23 22:10:05,946 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [936/1068 (88%)] Loss: 0.469637
2024-01-23 22:10:34,745 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [960/1068 (90%)] Loss: 0.469637
2024-01-23 22:11:03,532 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [984/1068 (92%)] Loss: 0.469637
2024-01-23 22:11:32,333 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [1008/1068 (94%)] Loss: 0.469637
2024-01-23 22:12:01,126 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [1032/1068 (97%)] Loss: 0.469637
2024-01-23 22:12:29,917 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [1056/1068 (99%)] Loss: 0.469637
2024-01-23 22:13:04,001 - PROT.PROT.base.base_trainer - INFO - epoch          : 6
2024-01-23 22:13:04,001 - PROT.PROT.base.base_trainer - INFO - loss           : 0.4694997440695092
2024-01-23 22:13:04,003 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0
2024-01-23 22:13:04,004 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0
2024-01-23 22:13:04,005 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0
2024-01-23 22:13:04,007 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0
2024-01-23 22:13:04,008 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0
2024-01-23 22:13:04,009 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0
2024-01-23 22:13:04,011 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 0
2024-01-23 22:13:04,012 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 0
2024-01-23 22:13:04,013 - PROT.PROT.base.base_trainer - INFO - metric_tasa    : 0.0
2024-01-23 22:13:04,014 - PROT.PROT.base.base_trainer - INFO - metric_thsa    : 0.0
2024-01-23 22:13:04,015 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 0.0
2024-01-23 22:13:04,016 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0
2024-01-23 22:13:04,018 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0
2024-01-23 22:13:04,019 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0
2024-01-23 22:13:04,020 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0
2024-01-23 22:13:04,021 - PROT.PROT.base.base_trainer - INFO - metric_species : 0
2024-01-23 22:13:04,022 - PROT.PROT.base.base_trainer - INFO - metric_expression: 0
2024-01-23 22:13:04,023 - PROT.PROT.base.base_trainer - INFO - metric_aggregation: 0.3185185273488363
2024-01-23 22:13:04,024 - PROT.PROT.base.base_trainer - INFO - val_loss       : 0.4670083544084004
2024-01-23 22:13:04,026 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0
2024-01-23 22:13:04,027 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0
2024-01-23 22:13:04,028 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0
2024-01-23 22:13:04,029 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0
2024-01-23 22:13:04,030 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0
2024-01-23 22:13:04,032 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0
2024-01-23 22:13:04,033 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 0
2024-01-23 22:13:04,034 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 0
2024-01-23 22:13:04,035 - PROT.PROT.base.base_trainer - INFO - val_metric_tasa: 0.0
2024-01-23 22:13:04,036 - PROT.PROT.base.base_trainer - INFO - val_metric_thsa: 0.0
2024-01-23 22:13:04,038 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 0.0
2024-01-23 22:13:04,039 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0
2024-01-23 22:13:04,040 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0
2024-01-23 22:13:04,041 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0
2024-01-23 22:13:04,042 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0
2024-01-23 22:13:04,043 - PROT.PROT.base.base_trainer - INFO - val_metric_species: 0
2024-01-23 22:13:04,045 - PROT.PROT.base.base_trainer - INFO - val_metric_expression: 0
2024-01-23 22:13:04,046 - PROT.PROT.base.base_trainer - INFO - val_metric_aggregation: 0.42857144081166815
2024-01-23 22:13:17,384 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2_singletask/0123-193707/checkpoints/checkpoint-epoch6.pth ...
2024-01-23 22:13:30,859 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2_singletask/0123-193707/checkpoints/model_best.pth
2024-01-23 22:13:35,447 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [0/1068 (0%)] Loss: 0.469637
2024-01-23 22:14:03,965 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [24/1068 (2%)] Loss: 0.469648
2024-01-23 22:14:32,549 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [48/1068 (4%)] Loss: 0.469653
2024-01-23 22:15:01,147 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [72/1068 (7%)] Loss: 0.469650
2024-01-23 22:15:29,786 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [96/1068 (9%)] Loss: 0.469645
2024-01-23 22:15:58,456 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [120/1068 (11%)] Loss: 0.469641
2024-01-23 22:16:27,145 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [144/1068 (13%)] Loss: 0.469639
2024-01-23 22:16:55,846 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [168/1068 (16%)] Loss: 0.469637
2024-01-23 22:17:24,719 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [192/1068 (18%)] Loss: 0.469637
2024-01-23 22:17:53,454 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [216/1068 (20%)] Loss: 0.469637
2024-01-23 22:18:22,198 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [240/1068 (22%)] Loss: 0.469637
2024-01-23 22:18:50,951 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [264/1068 (25%)] Loss: 0.469636
2024-01-23 22:19:19,733 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [288/1068 (27%)] Loss: 0.469637
2024-01-23 22:19:48,505 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [312/1068 (29%)] Loss: 0.469637
2024-01-23 22:20:17,292 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [336/1068 (31%)] Loss: 0.469637
2024-01-23 22:20:46,077 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [360/1068 (34%)] Loss: 0.469637
2024-01-23 22:21:14,859 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [384/1068 (36%)] Loss: 0.469637
2024-01-23 22:21:43,643 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [408/1068 (38%)] Loss: 0.469637
2024-01-23 22:22:12,525 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [432/1068 (40%)] Loss: 0.469637
2024-01-23 22:22:41,301 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [456/1068 (43%)] Loss: 0.469637
2024-01-23 22:23:10,083 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [480/1068 (45%)] Loss: 0.469637
2024-01-23 22:23:38,868 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [504/1068 (47%)] Loss: 0.469637
2024-01-23 22:24:07,657 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [528/1068 (49%)] Loss: 0.469637
2024-01-23 22:24:36,452 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [552/1068 (52%)] Loss: 0.469637
2024-01-23 22:25:05,238 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [576/1068 (54%)] Loss: 0.469637
2024-01-23 22:25:34,031 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [600/1068 (56%)] Loss: 0.469637
2024-01-23 22:26:02,818 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [624/1068 (58%)] Loss: 0.469637
2024-01-23 22:26:31,606 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [648/1068 (61%)] Loss: 0.469637
2024-01-23 22:27:00,392 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [672/1068 (63%)] Loss: 0.469637
2024-01-23 22:27:29,256 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [696/1068 (65%)] Loss: 0.469637
2024-01-23 22:27:58,066 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [720/1068 (67%)] Loss: 0.469637
2024-01-23 22:28:26,850 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [744/1068 (70%)] Loss: 0.469637
2024-01-23 22:28:55,633 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [768/1068 (72%)] Loss: 0.469637
2024-01-23 22:29:24,415 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [792/1068 (74%)] Loss: 0.469637
2024-01-23 22:29:53,198 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [816/1068 (76%)] Loss: 0.469637
2024-01-23 22:30:21,978 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [840/1068 (79%)] Loss: 0.469637
2024-01-23 22:30:50,769 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [864/1068 (81%)] Loss: 0.469637
2024-01-23 22:31:19,555 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [888/1068 (83%)] Loss: 0.469637
2024-01-23 22:31:48,339 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [912/1068 (85%)] Loss: 0.469637
2024-01-23 22:32:17,135 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [936/1068 (88%)] Loss: 0.469637
2024-01-23 22:32:46,014 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [960/1068 (90%)] Loss: 0.469637
2024-01-23 22:33:14,812 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [984/1068 (92%)] Loss: 0.469637
2024-01-23 22:33:43,609 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [1008/1068 (94%)] Loss: 0.469637
2024-01-23 22:34:12,407 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [1032/1068 (97%)] Loss: 0.469637
2024-01-23 22:34:41,190 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [1056/1068 (99%)] Loss: 0.469637
2024-01-23 22:35:14,958 - PROT.PROT.base.base_trainer - INFO - epoch          : 7
2024-01-23 22:35:14,958 - PROT.PROT.base.base_trainer - INFO - loss           : 0.46949997544288635
2024-01-23 22:35:14,959 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0
2024-01-23 22:35:14,961 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0
2024-01-23 22:35:14,962 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0
2024-01-23 22:35:14,964 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0
2024-01-23 22:35:14,965 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0
2024-01-23 22:35:14,966 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0
2024-01-23 22:35:14,967 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 0
2024-01-23 22:35:14,968 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 0
2024-01-23 22:35:14,970 - PROT.PROT.base.base_trainer - INFO - metric_tasa    : 0.0
2024-01-23 22:35:14,971 - PROT.PROT.base.base_trainer - INFO - metric_thsa    : 0.0
2024-01-23 22:35:14,972 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 0.0
2024-01-23 22:35:14,973 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0
2024-01-23 22:35:14,974 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0
2024-01-23 22:35:14,976 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0
2024-01-23 22:35:14,977 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0
2024-01-23 22:35:14,978 - PROT.PROT.base.base_trainer - INFO - metric_species : 0
2024-01-23 22:35:14,979 - PROT.PROT.base.base_trainer - INFO - metric_expression: 0
2024-01-23 22:35:14,980 - PROT.PROT.base.base_trainer - INFO - metric_aggregation: 0.3407407508956061
2024-01-23 22:35:14,982 - PROT.PROT.base.base_trainer - INFO - val_loss       : 0.467007788164275
2024-01-23 22:35:14,983 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0
2024-01-23 22:35:14,984 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0
2024-01-23 22:35:14,985 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0
2024-01-23 22:35:14,986 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0
2024-01-23 22:35:14,987 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0
2024-01-23 22:35:14,989 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0
2024-01-23 22:35:14,990 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 0
2024-01-23 22:35:14,991 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 0
2024-01-23 22:35:14,992 - PROT.PROT.base.base_trainer - INFO - val_metric_tasa: 0.0
2024-01-23 22:35:14,993 - PROT.PROT.base.base_trainer - INFO - val_metric_thsa: 0.0
2024-01-23 22:35:14,994 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 0.0
2024-01-23 22:35:14,995 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0
2024-01-23 22:35:14,996 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0
2024-01-23 22:35:14,998 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0
2024-01-23 22:35:14,999 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0
2024-01-23 22:35:15,000 - PROT.PROT.base.base_trainer - INFO - val_metric_species: 0
2024-01-23 22:35:15,001 - PROT.PROT.base.base_trainer - INFO - val_metric_expression: 0
2024-01-23 22:35:15,002 - PROT.PROT.base.base_trainer - INFO - val_metric_aggregation: 0.4285714386829308
2024-01-23 22:35:27,698 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2_singletask/0123-193707/checkpoints/checkpoint-epoch7.pth ...
2024-01-23 22:35:43,362 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2_singletask/0123-193707/checkpoints/model_best.pth
2024-01-23 22:35:47,960 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [0/1068 (0%)] Loss: 0.469637
2024-01-23 22:36:16,480 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [24/1068 (2%)] Loss: 0.469652
2024-01-23 22:36:45,233 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [48/1068 (4%)] Loss: 0.469658
2024-01-23 22:37:13,849 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [72/1068 (7%)] Loss: 0.469653
2024-01-23 22:37:42,502 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [96/1068 (9%)] Loss: 0.469646
2024-01-23 22:38:11,216 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [120/1068 (11%)] Loss: 0.469641
2024-01-23 22:38:39,966 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [144/1068 (13%)] Loss: 0.469638
2024-01-23 22:39:08,726 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [168/1068 (16%)] Loss: 0.469637
2024-01-23 22:39:37,522 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [192/1068 (18%)] Loss: 0.469637
2024-01-23 22:40:06,326 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [216/1068 (20%)] Loss: 0.469637
2024-01-23 22:40:35,135 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [240/1068 (22%)] Loss: 0.469637
2024-01-23 22:41:03,951 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [264/1068 (25%)] Loss: 0.469637
2024-01-23 22:41:32,870 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [288/1068 (27%)] Loss: 0.469637
2024-01-23 22:42:01,698 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [312/1068 (29%)] Loss: 0.469637
2024-01-23 22:42:30,562 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [336/1068 (31%)] Loss: 0.469637
2024-01-23 22:42:59,426 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [360/1068 (34%)] Loss: 0.469637
2024-01-23 22:43:28,301 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [384/1068 (36%)] Loss: 0.469637
2024-01-23 22:43:57,182 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [408/1068 (38%)] Loss: 0.469636
2024-01-23 22:44:26,061 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [432/1068 (40%)] Loss: 0.469636
2024-01-23 22:44:54,926 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [456/1068 (43%)] Loss: 0.469637
2024-01-23 22:45:23,720 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [480/1068 (45%)] Loss: 0.469637
2024-01-23 22:45:52,619 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [504/1068 (47%)] Loss: 0.469637
2024-01-23 22:46:21,527 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [528/1068 (49%)] Loss: 0.469637
2024-01-23 22:46:50,522 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [552/1068 (52%)] Loss: 0.469637
2024-01-23 22:47:19,422 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [576/1068 (54%)] Loss: 0.469637
2024-01-23 22:47:48,332 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [600/1068 (56%)] Loss: 0.469637
2024-01-23 22:48:17,239 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [624/1068 (58%)] Loss: 0.469637
2024-01-23 22:48:46,149 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [648/1068 (61%)] Loss: 0.469637
2024-01-23 22:49:15,111 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [672/1068 (63%)] Loss: 0.469637
2024-01-23 22:49:44,017 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [696/1068 (65%)] Loss: 0.469637
2024-01-23 22:50:12,931 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [720/1068 (67%)] Loss: 0.469637
2024-01-23 22:50:41,853 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [744/1068 (70%)] Loss: 0.469637
2024-01-23 22:51:10,778 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [768/1068 (72%)] Loss: 0.469637
2024-01-23 22:51:39,701 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [792/1068 (74%)] Loss: 0.469637
2024-01-23 22:52:08,720 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [816/1068 (76%)] Loss: 0.469637
2024-01-23 22:52:37,634 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [840/1068 (79%)] Loss: 0.469637
2024-01-23 22:53:06,554 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [864/1068 (81%)] Loss: 0.469637
2024-01-23 22:53:35,482 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [888/1068 (83%)] Loss: 0.469637
2024-01-23 22:54:04,405 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [912/1068 (85%)] Loss: 0.469637
2024-01-23 22:54:33,318 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [936/1068 (88%)] Loss: 0.469637
2024-01-23 22:55:02,251 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [960/1068 (90%)] Loss: 0.469637
2024-01-23 22:55:31,180 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [984/1068 (92%)] Loss: 0.469637
2024-01-23 22:56:00,113 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [1008/1068 (94%)] Loss: 0.469637
2024-01-23 22:56:29,043 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [1032/1068 (97%)] Loss: 0.469637
2024-01-23 22:56:57,951 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [1056/1068 (99%)] Loss: 0.469637
2024-01-23 22:57:32,323 - PROT.PROT.base.base_trainer - INFO - epoch          : 8
2024-01-23 22:57:32,324 - PROT.PROT.base.base_trainer - INFO - loss           : 0.46950024332830204
2024-01-23 22:57:32,325 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0
2024-01-23 22:57:32,326 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0
2024-01-23 22:57:32,327 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0
2024-01-23 22:57:32,329 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0
2024-01-23 22:57:32,330 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0
2024-01-23 22:57:32,331 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0
2024-01-23 22:57:32,332 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 0
2024-01-23 22:57:32,334 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 0
2024-01-23 22:57:32,335 - PROT.PROT.base.base_trainer - INFO - metric_tasa    : 0.0
2024-01-23 22:57:32,336 - PROT.PROT.base.base_trainer - INFO - metric_thsa    : 0.0
2024-01-23 22:57:32,337 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 0.0
2024-01-23 22:57:32,338 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0
2024-01-23 22:57:32,339 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0
2024-01-23 22:57:32,341 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0
2024-01-23 22:57:32,342 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0
2024-01-23 22:57:32,343 - PROT.PROT.base.base_trainer - INFO - metric_species : 0
2024-01-23 22:57:32,344 - PROT.PROT.base.base_trainer - INFO - metric_expression: 0
2024-01-23 22:57:32,346 - PROT.PROT.base.base_trainer - INFO - metric_aggregation: 0.35555556416511536
2024-01-23 22:57:32,347 - PROT.PROT.base.base_trainer - INFO - val_loss       : 0.46700718786035267
2024-01-23 22:57:32,348 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0
2024-01-23 22:57:32,349 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0
2024-01-23 22:57:32,350 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0
2024-01-23 22:57:32,351 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0
2024-01-23 22:57:32,352 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0
2024-01-23 22:57:32,353 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0
2024-01-23 22:57:32,354 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 0
2024-01-23 22:57:32,355 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 0
2024-01-23 22:57:32,356 - PROT.PROT.base.base_trainer - INFO - val_metric_tasa: 0.0
2024-01-23 22:57:32,358 - PROT.PROT.base.base_trainer - INFO - val_metric_thsa: 0.0
2024-01-23 22:57:32,359 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 0.0
2024-01-23 22:57:32,360 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0
2024-01-23 22:57:32,361 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0
2024-01-23 22:57:32,362 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0
2024-01-23 22:57:32,363 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0
2024-01-23 22:57:32,364 - PROT.PROT.base.base_trainer - INFO - val_metric_species: 0
2024-01-23 22:57:32,365 - PROT.PROT.base.base_trainer - INFO - val_metric_expression: 0
2024-01-23 22:57:32,367 - PROT.PROT.base.base_trainer - INFO - val_metric_aggregation: 0.42857144134385244
2024-01-23 22:57:59,414 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2_singletask/0123-193707/checkpoints/checkpoint-epoch8.pth ...
2024-01-23 22:58:25,587 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2_singletask/0123-193707/checkpoints/model_best.pth
2024-01-23 22:58:30,682 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [0/1068 (0%)] Loss: 0.469637
2024-01-23 22:58:59,058 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [24/1068 (2%)] Loss: 0.469658
2024-01-23 22:59:27,565 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [48/1068 (4%)] Loss: 0.469665
2024-01-23 22:59:56,220 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [72/1068 (7%)] Loss: 0.469656
2024-01-23 23:00:24,842 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [96/1068 (9%)] Loss: 0.469646
2024-01-23 23:00:53,574 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [120/1068 (11%)] Loss: 0.469640
2024-01-23 23:01:22,455 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [144/1068 (13%)] Loss: 0.469637
2024-01-23 23:01:51,203 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [168/1068 (16%)] Loss: 0.469637
2024-01-23 23:02:19,977 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [192/1068 (18%)] Loss: 0.469637
2024-01-23 23:02:48,872 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [216/1068 (20%)] Loss: 0.469637
2024-01-23 23:03:17,789 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [240/1068 (22%)] Loss: 0.469637
2024-01-23 23:03:46,694 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [264/1068 (25%)] Loss: 0.469637
2024-01-23 23:04:15,564 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [288/1068 (27%)] Loss: 0.469637
2024-01-23 23:04:44,449 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [312/1068 (29%)] Loss: 0.469637
2024-01-23 23:05:13,345 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [336/1068 (31%)] Loss: 0.469637
2024-01-23 23:05:42,244 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [360/1068 (34%)] Loss: 0.469637
2024-01-23 23:06:11,141 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [384/1068 (36%)] Loss: 0.469637
2024-01-23 23:06:40,131 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [408/1068 (38%)] Loss: 0.469637
2024-01-23 23:07:09,034 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [432/1068 (40%)] Loss: 0.469637
2024-01-23 23:07:37,938 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [456/1068 (43%)] Loss: 0.469637
2024-01-23 23:08:06,844 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [480/1068 (45%)] Loss: 0.469637
2024-01-23 23:08:35,751 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [504/1068 (47%)] Loss: 0.469637
2024-01-23 23:09:04,651 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [528/1068 (49%)] Loss: 0.469637
2024-01-23 23:09:33,551 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [552/1068 (52%)] Loss: 0.469637
2024-01-23 23:10:02,445 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [576/1068 (54%)] Loss: 0.469637
2024-01-23 23:10:31,341 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [600/1068 (56%)] Loss: 0.469637
2024-01-23 23:11:00,236 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [624/1068 (58%)] Loss: 0.469637
2024-01-23 23:11:29,136 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [648/1068 (61%)] Loss: 0.469637
2024-01-23 23:11:58,127 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [672/1068 (63%)] Loss: 0.469637
2024-01-23 23:12:27,025 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [696/1068 (65%)] Loss: 0.469637
2024-01-23 23:12:55,920 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [720/1068 (67%)] Loss: 0.469637
2024-01-23 23:13:24,815 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [744/1068 (70%)] Loss: 0.469637
2024-01-23 23:13:53,713 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [768/1068 (72%)] Loss: 0.469637
2024-01-23 23:14:22,611 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [792/1068 (74%)] Loss: 0.469637
2024-01-23 23:14:54,858 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [816/1068 (76%)] Loss: 0.469637
2024-01-23 23:15:23,720 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [840/1068 (79%)] Loss: 0.469637
2024-01-23 23:15:52,577 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [864/1068 (81%)] Loss: 0.469637
2024-01-23 23:16:21,441 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [888/1068 (83%)] Loss: 0.469637
2024-01-23 23:16:50,345 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [912/1068 (85%)] Loss: 0.469637
2024-01-23 23:17:19,229 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [936/1068 (88%)] Loss: 0.469637
2024-01-23 23:17:48,123 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [960/1068 (90%)] Loss: 0.469637
2024-01-23 23:18:17,003 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [984/1068 (92%)] Loss: 0.469637
2024-01-23 23:18:45,893 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [1008/1068 (94%)] Loss: 0.469637
2024-01-23 23:19:14,784 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [1032/1068 (97%)] Loss: 0.469637
2024-01-23 23:19:43,672 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [1056/1068 (99%)] Loss: 0.469637
2024-01-23 23:20:17,352 - PROT.PROT.base.base_trainer - INFO - epoch          : 9
2024-01-23 23:20:17,353 - PROT.PROT.base.base_trainer - INFO - loss           : 0.46950053506116707
2024-01-23 23:20:17,355 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0
2024-01-23 23:20:17,356 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0
2024-01-23 23:20:17,357 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0
2024-01-23 23:20:17,359 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0
2024-01-23 23:20:17,360 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0
2024-01-23 23:20:17,361 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0
2024-01-23 23:20:17,363 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 0
2024-01-23 23:20:17,364 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 0
2024-01-23 23:20:17,365 - PROT.PROT.base.base_trainer - INFO - metric_tasa    : 0.0
2024-01-23 23:20:17,366 - PROT.PROT.base.base_trainer - INFO - metric_thsa    : 0.0
2024-01-23 23:20:17,367 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 0.0
2024-01-23 23:20:17,369 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0
2024-01-23 23:20:17,370 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0
2024-01-23 23:20:17,371 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0
2024-01-23 23:20:17,372 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0
2024-01-23 23:20:17,373 - PROT.PROT.base.base_trainer - INFO - metric_species : 0
2024-01-23 23:20:17,374 - PROT.PROT.base.base_trainer - INFO - metric_expression: 0
2024-01-23 23:20:17,376 - PROT.PROT.base.base_trainer - INFO - metric_aggregation: 0.4370370460881127
2024-01-23 23:20:17,377 - PROT.PROT.base.base_trainer - INFO - val_loss       : 0.4670065524322646
2024-01-23 23:20:17,378 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0
2024-01-23 23:20:17,379 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0
2024-01-23 23:20:17,380 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0
2024-01-23 23:20:17,382 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0
2024-01-23 23:20:17,383 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0
2024-01-23 23:20:17,384 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0
2024-01-23 23:20:17,385 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 0
2024-01-23 23:20:17,386 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 0
2024-01-23 23:20:17,387 - PROT.PROT.base.base_trainer - INFO - val_metric_tasa: 0.0
2024-01-23 23:20:17,388 - PROT.PROT.base.base_trainer - INFO - val_metric_thsa: 0.0
2024-01-23 23:20:17,389 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 0.0
2024-01-23 23:20:17,390 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0
2024-01-23 23:20:17,392 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0
2024-01-23 23:20:17,393 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0
2024-01-23 23:20:17,394 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0
2024-01-23 23:20:17,395 - PROT.PROT.base.base_trainer - INFO - val_metric_species: 0
2024-01-23 23:20:17,396 - PROT.PROT.base.base_trainer - INFO - val_metric_expression: 0
2024-01-23 23:20:17,397 - PROT.PROT.base.base_trainer - INFO - val_metric_aggregation: 0.4285714381507465
2024-01-23 23:20:30,110 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2_singletask/0123-193707/checkpoints/checkpoint-epoch9.pth ...
2024-01-23 23:20:42,966 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2_singletask/0123-193707/checkpoints/model_best.pth
2024-01-23 23:20:47,688 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [0/1068 (0%)] Loss: 0.469637
2024-01-23 23:21:16,323 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [24/1068 (2%)] Loss: 0.469666
2024-01-23 23:21:44,955 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [48/1068 (4%)] Loss: 0.469672
2024-01-23 23:22:13,636 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [72/1068 (7%)] Loss: 0.469658
2024-01-23 23:22:42,368 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [96/1068 (9%)] Loss: 0.469645
2024-01-23 23:23:11,120 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [120/1068 (11%)] Loss: 0.469638
2024-01-23 23:23:39,904 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [144/1068 (13%)] Loss: 0.469637
2024-01-23 23:24:08,694 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [168/1068 (16%)] Loss: 0.469637
2024-01-23 23:24:37,511 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [192/1068 (18%)] Loss: 0.469637
2024-01-23 23:25:06,329 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [216/1068 (20%)] Loss: 0.469637
2024-01-23 23:25:35,164 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [240/1068 (22%)] Loss: 0.469637
2024-01-23 23:26:04,154 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [264/1068 (25%)] Loss: 0.469637
2024-01-23 23:26:32,985 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [288/1068 (27%)] Loss: 0.469637
2024-01-23 23:27:01,825 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [312/1068 (29%)] Loss: 0.469637
2024-01-23 23:27:30,675 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [336/1068 (31%)] Loss: 0.469637
2024-01-23 23:27:59,523 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [360/1068 (34%)] Loss: 0.469637
2024-01-23 23:28:28,375 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [384/1068 (36%)] Loss: 0.469637
2024-01-23 23:28:57,253 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [408/1068 (38%)] Loss: 0.469637
2024-01-23 23:29:26,149 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [432/1068 (40%)] Loss: 0.469637
2024-01-23 23:29:55,055 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [456/1068 (43%)] Loss: 0.469637
2024-01-23 23:30:23,960 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [480/1068 (45%)] Loss: 0.469637
2024-01-23 23:30:52,867 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [504/1068 (47%)] Loss: 0.469636
2024-01-23 23:31:21,863 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [528/1068 (49%)] Loss: 0.469637
2024-01-23 23:31:50,755 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [552/1068 (52%)] Loss: 0.469637
2024-01-23 23:32:19,671 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [576/1068 (54%)] Loss: 0.469636
2024-01-23 23:32:48,573 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [600/1068 (56%)] Loss: 0.469636
2024-01-23 23:33:17,481 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [624/1068 (58%)] Loss: 0.469636
2024-01-23 23:33:46,382 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [648/1068 (61%)] Loss: 0.469636
2024-01-23 23:34:15,280 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [672/1068 (63%)] Loss: 0.469636
2024-01-23 23:34:44,183 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [696/1068 (65%)] Loss: 0.469636
2024-01-23 23:35:13,088 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [720/1068 (67%)] Loss: 0.469636
2024-01-23 23:35:41,997 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [744/1068 (70%)] Loss: 0.469636
2024-01-23 23:36:10,995 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [768/1068 (72%)] Loss: 0.469636
2024-01-23 23:36:39,894 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [792/1068 (74%)] Loss: 0.469636
2024-01-23 23:37:09,235 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [816/1068 (76%)] Loss: 0.469636
2024-01-23 23:37:38,130 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [840/1068 (79%)] Loss: 0.469636
2024-01-23 23:38:07,038 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [864/1068 (81%)] Loss: 0.469636
2024-01-23 23:38:35,942 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [888/1068 (83%)] Loss: 0.469636
2024-01-23 23:39:04,866 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [912/1068 (85%)] Loss: 0.469636
2024-01-23 23:39:34,126 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [936/1068 (88%)] Loss: 0.469636
2024-01-23 23:40:03,039 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [960/1068 (90%)] Loss: 0.469636
2024-01-23 23:40:31,950 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [984/1068 (92%)] Loss: 0.469636
2024-01-23 23:41:00,864 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [1008/1068 (94%)] Loss: 0.469636
2024-01-23 23:41:29,875 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [1032/1068 (97%)] Loss: 0.469636
2024-01-23 23:41:58,787 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [1056/1068 (99%)] Loss: 0.469636
2024-01-23 23:42:32,999 - PROT.PROT.base.base_trainer - INFO - epoch          : 10
2024-01-23 23:42:33,000 - PROT.PROT.base.base_trainer - INFO - loss           : 0.469500862271656
2024-01-23 23:42:33,001 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0
2024-01-23 23:42:33,002 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0
2024-01-23 23:42:33,004 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0
2024-01-23 23:42:33,005 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0
2024-01-23 23:42:33,006 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0
2024-01-23 23:42:33,007 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0
2024-01-23 23:42:33,009 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 0
2024-01-23 23:42:33,010 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 0
2024-01-23 23:42:33,011 - PROT.PROT.base.base_trainer - INFO - metric_tasa    : 0.0
2024-01-23 23:42:33,013 - PROT.PROT.base.base_trainer - INFO - metric_thsa    : 0.0
2024-01-23 23:42:33,014 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 0.0
2024-01-23 23:42:33,015 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0
2024-01-23 23:42:33,016 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0
2024-01-23 23:42:33,017 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0
2024-01-23 23:42:33,018 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0
2024-01-23 23:42:33,019 - PROT.PROT.base.base_trainer - INFO - metric_species : 0
2024-01-23 23:42:33,020 - PROT.PROT.base.base_trainer - INFO - metric_expression: 0
2024-01-23 23:42:33,022 - PROT.PROT.base.base_trainer - INFO - metric_aggregation: 0.26666667461395266
2024-01-23 23:42:33,023 - PROT.PROT.base.base_trainer - INFO - val_loss       : 0.4670059872525079
2024-01-23 23:42:33,024 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0
2024-01-23 23:42:33,025 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0
2024-01-23 23:42:33,026 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0
2024-01-23 23:42:33,027 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0
2024-01-23 23:42:33,028 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0
2024-01-23 23:42:33,029 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0
2024-01-23 23:42:33,030 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 0
2024-01-23 23:42:33,031 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 0
2024-01-23 23:42:33,032 - PROT.PROT.base.base_trainer - INFO - val_metric_tasa: 0.0
2024-01-23 23:42:33,033 - PROT.PROT.base.base_trainer - INFO - val_metric_thsa: 0.0
2024-01-23 23:42:33,035 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 0.0
2024-01-23 23:42:33,036 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0
2024-01-23 23:42:33,037 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0
2024-01-23 23:42:33,038 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0
2024-01-23 23:42:33,039 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0
2024-01-23 23:42:33,040 - PROT.PROT.base.base_trainer - INFO - val_metric_species: 0
2024-01-23 23:42:33,041 - PROT.PROT.base.base_trainer - INFO - val_metric_expression: 0
2024-01-23 23:42:33,042 - PROT.PROT.base.base_trainer - INFO - val_metric_aggregation: 0.42857143602200914
2024-01-23 23:42:45,602 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2_singletask/0123-193707/checkpoints/checkpoint-epoch10.pth ...
2024-01-23 23:43:01,098 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2_singletask/0123-193707/checkpoints/model_best.pth
2024-01-23 23:43:05,298 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [0/1068 (0%)] Loss: 0.469638
2024-01-23 23:43:33,900 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [24/1068 (2%)] Loss: 0.469676
2024-01-23 23:44:02,546 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [48/1068 (4%)] Loss: 0.469681
2024-01-23 23:44:31,209 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [72/1068 (7%)] Loss: 0.469659
2024-01-23 23:44:59,931 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [96/1068 (9%)] Loss: 0.469643
2024-01-23 23:45:28,685 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [120/1068 (11%)] Loss: 0.469637
2024-01-23 23:45:57,616 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [144/1068 (13%)] Loss: 0.469637
2024-01-23 23:46:26,418 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [168/1068 (16%)] Loss: 0.469637
2024-01-23 23:46:55,273 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [192/1068 (18%)] Loss: 0.469637
2024-01-23 23:47:24,123 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [216/1068 (20%)] Loss: 0.469637
2024-01-23 23:47:52,957 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [240/1068 (22%)] Loss: 0.469637
2024-01-23 23:48:21,797 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [264/1068 (25%)] Loss: 0.469637
2024-01-23 23:48:50,663 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [288/1068 (27%)] Loss: 0.469637
2024-01-23 23:49:19,568 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [312/1068 (29%)] Loss: 0.469636
2024-01-23 23:49:48,434 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [336/1068 (31%)] Loss: 0.469637
2024-01-23 23:50:17,304 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [360/1068 (34%)] Loss: 0.469636
2024-01-23 23:50:46,184 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [384/1068 (36%)] Loss: 0.469637
2024-01-23 23:51:15,091 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [408/1068 (38%)] Loss: 0.469637
2024-01-23 23:51:44,017 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [432/1068 (40%)] Loss: 0.469636
2024-01-23 23:52:12,940 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [456/1068 (43%)] Loss: 0.469637
2024-01-23 23:52:41,863 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [480/1068 (45%)] Loss: 0.469637
2024-01-23 23:53:10,780 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [504/1068 (47%)] Loss: 0.469637
2024-01-23 23:53:39,716 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [528/1068 (49%)] Loss: 0.469637
2024-01-23 23:54:08,631 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [552/1068 (52%)] Loss: 0.469636
2024-01-23 23:54:37,556 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [576/1068 (54%)] Loss: 0.469637
2024-01-23 23:55:06,478 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [600/1068 (56%)] Loss: 0.469637
2024-01-23 23:55:35,395 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [624/1068 (58%)] Loss: 0.469637
2024-01-23 23:56:04,398 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [648/1068 (61%)] Loss: 0.469637
2024-01-23 23:56:33,301 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [672/1068 (63%)] Loss: 0.469637
2024-01-23 23:57:02,222 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [696/1068 (65%)] Loss: 0.469637
2024-01-23 23:57:31,145 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [720/1068 (67%)] Loss: 0.469637
2024-01-23 23:58:00,066 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [744/1068 (70%)] Loss: 0.469637
2024-01-23 23:58:29,007 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [768/1068 (72%)] Loss: 0.469637
2024-01-23 23:58:57,940 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [792/1068 (74%)] Loss: 0.469637
2024-01-23 23:59:26,861 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [816/1068 (76%)] Loss: 0.469637
2024-01-23 23:59:59,367 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [840/1068 (79%)] Loss: 0.469637
2024-01-24 00:00:28,238 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [864/1068 (81%)] Loss: 0.469637
2024-01-24 00:00:57,084 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [888/1068 (83%)] Loss: 0.469637
2024-01-24 00:01:26,018 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [912/1068 (85%)] Loss: 0.469637
2024-01-24 00:01:54,893 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [936/1068 (88%)] Loss: 0.469637
2024-01-24 00:02:23,795 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [960/1068 (90%)] Loss: 0.469637
2024-01-24 00:02:52,696 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [984/1068 (92%)] Loss: 0.469637
2024-01-24 00:03:21,603 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [1008/1068 (94%)] Loss: 0.469637
2024-01-24 00:03:50,512 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [1032/1068 (97%)] Loss: 0.469637
2024-01-24 00:04:19,417 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [1056/1068 (99%)] Loss: 0.469637
2024-01-24 00:04:53,058 - PROT.PROT.base.base_trainer - INFO - epoch          : 11
2024-01-24 00:04:53,059 - PROT.PROT.base.base_trainer - INFO - loss           : 0.4695013081322915
2024-01-24 00:04:53,061 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0
2024-01-24 00:04:53,062 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0
2024-01-24 00:04:53,063 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0
2024-01-24 00:04:53,065 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0
2024-01-24 00:04:53,066 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0
2024-01-24 00:04:53,067 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0
2024-01-24 00:04:53,068 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 0
2024-01-24 00:04:53,069 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 0
2024-01-24 00:04:53,071 - PROT.PROT.base.base_trainer - INFO - metric_tasa    : 0.0
2024-01-24 00:04:53,072 - PROT.PROT.base.base_trainer - INFO - metric_thsa    : 0.0
2024-01-24 00:04:53,073 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 0.0
2024-01-24 00:04:53,074 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0
2024-01-24 00:04:53,075 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0
2024-01-24 00:04:53,077 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0
2024-01-24 00:04:53,078 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0
2024-01-24 00:04:53,079 - PROT.PROT.base.base_trainer - INFO - metric_species : 0
2024-01-24 00:04:53,080 - PROT.PROT.base.base_trainer - INFO - metric_expression: 0
2024-01-24 00:04:53,081 - PROT.PROT.base.base_trainer - INFO - metric_aggregation: 0.37037037942144607
2024-01-24 00:04:53,082 - PROT.PROT.base.base_trainer - INFO - val_loss       : 0.46700532947267803
2024-01-24 00:04:53,084 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0
2024-01-24 00:04:53,085 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0
2024-01-24 00:04:53,086 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0
2024-01-24 00:04:53,087 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0
2024-01-24 00:04:53,088 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0
2024-01-24 00:04:53,089 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0
2024-01-24 00:04:53,091 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 0
2024-01-24 00:04:53,092 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 0
2024-01-24 00:04:53,093 - PROT.PROT.base.base_trainer - INFO - val_metric_tasa: 0.0
2024-01-24 00:04:53,094 - PROT.PROT.base.base_trainer - INFO - val_metric_thsa: 0.0
2024-01-24 00:04:53,095 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 0.0
2024-01-24 00:04:53,096 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0
2024-01-24 00:04:53,097 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0
2024-01-24 00:04:53,098 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0
2024-01-24 00:04:53,100 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0
2024-01-24 00:04:53,101 - PROT.PROT.base.base_trainer - INFO - val_metric_species: 0
2024-01-24 00:04:53,102 - PROT.PROT.base.base_trainer - INFO - val_metric_expression: 0
2024-01-24 00:04:53,103 - PROT.PROT.base.base_trainer - INFO - val_metric_aggregation: 0.42857143921511515
2024-01-24 00:05:05,896 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2_singletask/0123-193707/checkpoints/checkpoint-epoch11.pth ...
2024-01-24 00:05:19,702 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2_singletask/0123-193707/checkpoints/model_best.pth
2024-01-24 00:05:24,278 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [0/1068 (0%)] Loss: 0.469638
2024-01-24 00:05:52,785 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [24/1068 (2%)] Loss: 0.469689
2024-01-24 00:06:21,330 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [48/1068 (4%)] Loss: 0.469689
2024-01-24 00:06:50,004 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [72/1068 (7%)] Loss: 0.469658
2024-01-24 00:07:18,744 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [96/1068 (9%)] Loss: 0.469640
2024-01-24 00:07:47,504 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [120/1068 (11%)] Loss: 0.469637
2024-01-24 00:08:16,281 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [144/1068 (13%)] Loss: 0.469637
2024-01-24 00:08:45,075 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [168/1068 (16%)] Loss: 0.469637
2024-01-24 00:09:13,882 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [192/1068 (18%)] Loss: 0.469637
2024-01-24 00:09:42,705 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [216/1068 (20%)] Loss: 0.469637
2024-01-24 00:10:11,583 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [240/1068 (22%)] Loss: 0.469637
2024-01-24 00:10:40,513 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [264/1068 (25%)] Loss: 0.469637
2024-01-24 00:11:09,361 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [288/1068 (27%)] Loss: 0.469637
2024-01-24 00:11:38,216 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [312/1068 (29%)] Loss: 0.469637
2024-01-24 00:12:07,065 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [336/1068 (31%)] Loss: 0.469637
2024-01-24 00:12:35,920 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [360/1068 (34%)] Loss: 0.469637
2024-01-24 00:13:04,804 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [384/1068 (36%)] Loss: 0.469636
2024-01-24 00:13:33,692 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [408/1068 (38%)] Loss: 0.469637
2024-01-24 00:14:02,586 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [432/1068 (40%)] Loss: 0.469637
2024-01-24 00:14:31,486 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [456/1068 (43%)] Loss: 0.469637
2024-01-24 00:15:00,389 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [480/1068 (45%)] Loss: 0.469637
2024-01-24 00:15:29,302 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [504/1068 (47%)] Loss: 0.469637
2024-01-24 00:15:58,282 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [528/1068 (49%)] Loss: 0.469637
2024-01-24 00:16:27,184 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [552/1068 (52%)] Loss: 0.469637
2024-01-24 00:16:56,087 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [576/1068 (54%)] Loss: 0.469636
2024-01-24 00:17:24,991 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [600/1068 (56%)] Loss: 0.469637
2024-01-24 00:17:53,898 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [624/1068 (58%)] Loss: 0.469637
2024-01-24 00:18:22,805 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [648/1068 (61%)] Loss: 0.469637
2024-01-24 00:18:51,717 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [672/1068 (63%)] Loss: 0.469637
2024-01-24 00:19:20,625 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [696/1068 (65%)] Loss: 0.469637
2024-01-24 00:19:49,535 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [720/1068 (67%)] Loss: 0.469637
2024-01-24 00:20:18,443 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [744/1068 (70%)] Loss: 0.469637
2024-01-24 00:20:47,367 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [768/1068 (72%)] Loss: 0.469637
2024-01-24 00:21:16,355 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [792/1068 (74%)] Loss: 0.469637
2024-01-24 00:21:45,256 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [816/1068 (76%)] Loss: 0.469637
2024-01-24 00:22:14,166 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [840/1068 (79%)] Loss: 0.469637
2024-01-24 00:22:43,092 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [864/1068 (81%)] Loss: 0.469637
2024-01-24 00:23:12,015 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [888/1068 (83%)] Loss: 0.469637
2024-01-24 00:23:40,936 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [912/1068 (85%)] Loss: 0.469637
2024-01-24 00:24:09,857 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [936/1068 (88%)] Loss: 0.469637
2024-01-24 00:24:38,781 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [960/1068 (90%)] Loss: 0.469637
2024-01-24 00:25:07,713 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [984/1068 (92%)] Loss: 0.469637
2024-01-24 00:25:36,644 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [1008/1068 (94%)] Loss: 0.469637
2024-01-24 00:26:05,651 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [1032/1068 (97%)] Loss: 0.469637
2024-01-24 00:26:34,575 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [1056/1068 (99%)] Loss: 0.469637
2024-01-24 00:27:08,760 - PROT.PROT.base.base_trainer - INFO - epoch          : 12
2024-01-24 00:27:08,761 - PROT.PROT.base.base_trainer - INFO - loss           : 0.46950172609169083
2024-01-24 00:27:08,763 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0
2024-01-24 00:27:08,764 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0
2024-01-24 00:27:08,765 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0
2024-01-24 00:27:08,767 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0
2024-01-24 00:27:08,768 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0
2024-01-24 00:27:08,769 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0
2024-01-24 00:27:08,770 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 0
2024-01-24 00:27:08,771 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 0
2024-01-24 00:27:08,773 - PROT.PROT.base.base_trainer - INFO - metric_tasa    : 0.0
2024-01-24 00:27:08,774 - PROT.PROT.base.base_trainer - INFO - metric_thsa    : 0.0
2024-01-24 00:27:08,775 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 0.0
2024-01-24 00:27:08,776 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0
2024-01-24 00:27:08,777 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0
2024-01-24 00:27:08,778 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0
2024-01-24 00:27:08,780 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0
2024-01-24 00:27:08,781 - PROT.PROT.base.base_trainer - INFO - metric_species : 0
2024-01-24 00:27:08,782 - PROT.PROT.base.base_trainer - INFO - metric_expression: 0
2024-01-24 00:27:08,783 - PROT.PROT.base.base_trainer - INFO - metric_aggregation: 0.31111111839612327
2024-01-24 00:27:08,784 - PROT.PROT.base.base_trainer - INFO - val_loss       : 0.4670046716928482
2024-01-24 00:27:08,786 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0
2024-01-24 00:27:08,787 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0
2024-01-24 00:27:08,788 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0
2024-01-24 00:27:08,789 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0
2024-01-24 00:27:08,790 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0
2024-01-24 00:27:08,791 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0
2024-01-24 00:27:08,793 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 0
2024-01-24 00:27:08,794 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 0
2024-01-24 00:27:08,795 - PROT.PROT.base.base_trainer - INFO - val_metric_tasa: 0.0
2024-01-24 00:27:08,797 - PROT.PROT.base.base_trainer - INFO - val_metric_thsa: 0.0
2024-01-24 00:27:08,798 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 0.0
2024-01-24 00:27:08,799 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0
2024-01-24 00:27:08,800 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0
2024-01-24 00:27:08,801 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0
2024-01-24 00:27:08,803 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0
2024-01-24 00:27:08,804 - PROT.PROT.base.base_trainer - INFO - val_metric_species: 0
2024-01-24 00:27:08,805 - PROT.PROT.base.base_trainer - INFO - val_metric_expression: 0
2024-01-24 00:27:08,806 - PROT.PROT.base.base_trainer - INFO - val_metric_aggregation: 0.42857144081166815
2024-01-24 00:27:21,540 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2_singletask/0123-193707/checkpoints/checkpoint-epoch12.pth ...
2024-01-24 00:27:34,597 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2_singletask/0123-193707/checkpoints/model_best.pth
2024-01-24 00:27:39,100 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [0/1068 (0%)] Loss: 0.469639
2024-01-24 00:28:07,726 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [24/1068 (2%)] Loss: 0.469703
2024-01-24 00:28:36,389 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [48/1068 (4%)] Loss: 0.469696
2024-01-24 00:29:05,086 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [72/1068 (7%)] Loss: 0.469656
2024-01-24 00:29:33,811 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [96/1068 (9%)] Loss: 0.469638
2024-01-24 00:30:02,725 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [120/1068 (11%)] Loss: 0.469637
2024-01-24 00:30:31,511 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [144/1068 (13%)] Loss: 0.469638
2024-01-24 00:31:00,319 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [168/1068 (16%)] Loss: 0.469637
2024-01-24 00:31:29,143 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [192/1068 (18%)] Loss: 0.469637
2024-01-24 00:31:57,975 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [216/1068 (20%)] Loss: 0.469637
2024-01-24 00:32:26,808 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [240/1068 (22%)] Loss: 0.469637
2024-01-24 00:32:55,655 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [264/1068 (25%)] Loss: 0.469637
2024-01-24 00:33:24,518 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [288/1068 (27%)] Loss: 0.469637
2024-01-24 00:33:53,402 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [312/1068 (29%)] Loss: 0.469637
2024-01-24 00:34:22,302 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [336/1068 (31%)] Loss: 0.469637
2024-01-24 00:34:51,209 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [360/1068 (34%)] Loss: 0.469637
2024-01-24 00:35:20,218 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [384/1068 (36%)] Loss: 0.469637
2024-01-24 00:35:49,129 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [408/1068 (38%)] Loss: 0.469637
2024-01-24 00:36:18,045 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [432/1068 (40%)] Loss: 0.469636
2024-01-24 00:36:46,957 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [456/1068 (43%)] Loss: 0.469637
2024-01-24 00:37:15,867 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [480/1068 (45%)] Loss: 0.469636
2024-01-24 00:37:44,774 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [504/1068 (47%)] Loss: 0.469637
2024-01-24 00:38:13,694 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [528/1068 (49%)] Loss: 0.469637
2024-01-24 00:38:42,609 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [552/1068 (52%)] Loss: 0.469637
2024-01-24 00:39:11,533 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [576/1068 (54%)] Loss: 0.469637
2024-01-24 00:39:40,453 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [600/1068 (56%)] Loss: 0.469637
2024-01-24 00:40:09,469 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [624/1068 (58%)] Loss: 0.469637
2024-01-24 00:40:38,386 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [648/1068 (61%)] Loss: 0.469637
2024-01-24 00:41:07,315 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [672/1068 (63%)] Loss: 0.469637
2024-01-24 00:41:36,242 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [696/1068 (65%)] Loss: 0.469637
2024-01-24 00:42:05,165 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [720/1068 (67%)] Loss: 0.469637
2024-01-24 00:42:34,098 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [744/1068 (70%)] Loss: 0.469637
2024-01-24 00:43:03,029 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [768/1068 (72%)] Loss: 0.469637
2024-01-24 00:43:31,963 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [792/1068 (74%)] Loss: 0.469637
2024-01-24 00:44:00,880 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [816/1068 (76%)] Loss: 0.469637
2024-01-24 00:44:29,807 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [840/1068 (79%)] Loss: 0.469637
2024-01-24 00:44:58,729 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [864/1068 (81%)] Loss: 0.469637
2024-01-24 00:45:27,741 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [888/1068 (83%)] Loss: 0.469637
2024-01-24 00:45:56,655 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [912/1068 (85%)] Loss: 0.469637
2024-01-24 00:46:25,566 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [936/1068 (88%)] Loss: 0.469637
2024-01-24 00:46:54,491 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [960/1068 (90%)] Loss: 0.469637
2024-01-24 00:47:23,415 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [984/1068 (92%)] Loss: 0.469637
2024-01-24 00:47:52,328 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [1008/1068 (94%)] Loss: 0.469637
2024-01-24 00:48:21,281 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [1032/1068 (97%)] Loss: 0.469637
2024-01-24 00:48:58,026 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [1056/1068 (99%)] Loss: 0.469637
2024-01-24 00:49:32,001 - PROT.PROT.base.base_trainer - INFO - epoch          : 13
2024-01-24 00:49:32,002 - PROT.PROT.base.base_trainer - INFO - loss           : 0.46950214136720375
2024-01-24 00:49:32,003 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0
2024-01-24 00:49:32,004 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0
2024-01-24 00:49:32,006 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0
2024-01-24 00:49:32,007 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0
2024-01-24 00:49:32,008 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0
2024-01-24 00:49:32,009 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0
2024-01-24 00:49:32,010 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 0
2024-01-24 00:49:32,012 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 0
2024-01-24 00:49:32,013 - PROT.PROT.base.base_trainer - INFO - metric_tasa    : 0.0
2024-01-24 00:49:32,014 - PROT.PROT.base.base_trainer - INFO - metric_thsa    : 0.0
2024-01-24 00:49:32,015 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 0.0
2024-01-24 00:49:32,016 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0
2024-01-24 00:49:32,017 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0
2024-01-24 00:49:32,019 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0
2024-01-24 00:49:32,020 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0
2024-01-24 00:49:32,021 - PROT.PROT.base.base_trainer - INFO - metric_species : 0
2024-01-24 00:49:32,022 - PROT.PROT.base.base_trainer - INFO - metric_expression: 0
2024-01-24 00:49:32,024 - PROT.PROT.base.base_trainer - INFO - metric_aggregation: 0.355555565489663
2024-01-24 00:49:32,025 - PROT.PROT.base.base_trainer - INFO - val_loss       : 0.46700416398899897
2024-01-24 00:49:32,026 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0
2024-01-24 00:49:32,027 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0
2024-01-24 00:49:32,028 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0
2024-01-24 00:49:32,030 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0
2024-01-24 00:49:32,031 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0
2024-01-24 00:49:32,032 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0
2024-01-24 00:49:32,033 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 0
2024-01-24 00:49:32,034 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 0
2024-01-24 00:49:32,035 - PROT.PROT.base.base_trainer - INFO - val_metric_tasa: 0.0
2024-01-24 00:49:32,036 - PROT.PROT.base.base_trainer - INFO - val_metric_thsa: 0.0
2024-01-24 00:49:32,038 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 0.0
2024-01-24 00:49:32,039 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0
2024-01-24 00:49:32,040 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0
2024-01-24 00:49:32,041 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0
2024-01-24 00:49:32,042 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0
2024-01-24 00:49:32,044 - PROT.PROT.base.base_trainer - INFO - val_metric_species: 0
2024-01-24 00:49:32,045 - PROT.PROT.base.base_trainer - INFO - val_metric_expression: 0
2024-01-24 00:49:32,046 - PROT.PROT.base.base_trainer - INFO - val_metric_aggregation: 0.42857143921511515
2024-01-24 00:49:44,798 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2_singletask/0123-193707/checkpoints/checkpoint-epoch13.pth ...
2024-01-24 00:50:06,581 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2_singletask/0123-193707/checkpoints/model_best.pth
2024-01-24 00:50:12,853 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [0/1068 (0%)] Loss: 0.469639
2024-01-24 00:50:41,424 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [24/1068 (2%)] Loss: 0.469718
2024-01-24 00:51:10,002 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [48/1068 (4%)] Loss: 0.469703
2024-01-24 00:51:38,689 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [72/1068 (7%)] Loss: 0.469653
2024-01-24 00:52:07,445 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [96/1068 (9%)] Loss: 0.469637
2024-01-24 00:52:36,241 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [120/1068 (11%)] Loss: 0.469638
2024-01-24 00:53:04,993 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [144/1068 (13%)] Loss: 0.469639
2024-01-24 00:53:33,798 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [168/1068 (16%)] Loss: 0.469637
2024-01-24 00:54:02,661 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [192/1068 (18%)] Loss: 0.469637
2024-01-24 00:54:31,498 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [216/1068 (20%)] Loss: 0.469637
2024-01-24 00:55:00,463 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [240/1068 (22%)] Loss: 0.469637
2024-01-24 00:55:29,293 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [264/1068 (25%)] Loss: 0.469637
2024-01-24 00:55:58,133 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [288/1068 (27%)] Loss: 0.469637
2024-01-24 00:56:27,033 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [312/1068 (29%)] Loss: 0.469637
2024-01-24 00:56:55,972 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [336/1068 (31%)] Loss: 0.469637
2024-01-24 00:57:24,916 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [360/1068 (34%)] Loss: 0.469637
2024-01-24 00:57:53,869 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [384/1068 (36%)] Loss: 0.469637
2024-01-24 00:58:22,835 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [408/1068 (38%)] Loss: 0.469637
2024-01-24 00:58:51,763 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [432/1068 (40%)] Loss: 0.469637
2024-01-24 00:59:20,694 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [456/1068 (43%)] Loss: 0.469637
2024-01-24 00:59:49,584 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [480/1068 (45%)] Loss: 0.469637
2024-01-24 01:00:18,569 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [504/1068 (47%)] Loss: 0.469637
2024-01-24 01:00:47,461 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [528/1068 (49%)] Loss: 0.469637
2024-01-24 01:01:16,380 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [552/1068 (52%)] Loss: 0.469637
2024-01-24 01:01:45,292 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [576/1068 (54%)] Loss: 0.469637
2024-01-24 01:02:14,209 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [600/1068 (56%)] Loss: 0.469637
2024-01-24 01:02:43,146 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [624/1068 (58%)] Loss: 0.469637
2024-01-24 01:03:12,057 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [648/1068 (61%)] Loss: 0.469637
2024-01-24 01:03:40,980 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [672/1068 (63%)] Loss: 0.469637
2024-01-24 01:04:09,897 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [696/1068 (65%)] Loss: 0.469637
2024-01-24 01:04:38,818 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [720/1068 (67%)] Loss: 0.469637
2024-01-24 01:05:07,738 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [744/1068 (70%)] Loss: 0.469637
2024-01-24 01:05:36,737 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [768/1068 (72%)] Loss: 0.469637
2024-01-24 01:06:05,645 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [792/1068 (74%)] Loss: 0.469637
2024-01-24 01:06:34,549 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [816/1068 (76%)] Loss: 0.469637
2024-01-24 01:07:03,474 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [840/1068 (79%)] Loss: 0.469637
2024-01-24 01:07:32,390 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [864/1068 (81%)] Loss: 0.469637
2024-01-24 01:08:01,315 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [888/1068 (83%)] Loss: 0.469637
2024-01-24 01:08:30,233 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [912/1068 (85%)] Loss: 0.469637
2024-01-24 01:08:59,152 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [936/1068 (88%)] Loss: 0.469637
2024-01-24 01:09:28,064 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [960/1068 (90%)] Loss: 0.469637
2024-01-24 01:09:56,985 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [984/1068 (92%)] Loss: 0.469637
2024-01-24 01:10:26,000 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [1008/1068 (94%)] Loss: 0.469637
2024-01-24 01:10:54,912 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [1032/1068 (97%)] Loss: 0.469637
2024-01-24 01:11:23,829 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [1056/1068 (99%)] Loss: 0.469637
2024-01-24 01:11:57,766 - PROT.PROT.base.base_trainer - INFO - epoch          : 14
2024-01-24 01:11:57,767 - PROT.PROT.base.base_trainer - INFO - loss           : 0.4695026282130218
2024-01-24 01:11:57,768 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0
2024-01-24 01:11:57,769 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0
2024-01-24 01:11:57,771 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0
2024-01-24 01:11:57,772 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0
2024-01-24 01:11:57,773 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0
2024-01-24 01:11:57,774 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0
2024-01-24 01:11:57,775 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 0
2024-01-24 01:11:57,777 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 0
2024-01-24 01:11:57,778 - PROT.PROT.base.base_trainer - INFO - metric_tasa    : 0.0
2024-01-24 01:11:57,779 - PROT.PROT.base.base_trainer - INFO - metric_thsa    : 0.0
2024-01-24 01:11:57,780 - PROT.PROT.base.base_trainer - INFO - metric_lhp     : 0.0
2024-01-24 01:11:57,781 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_mcc: 0
2024-01-24 01:11:57,782 - PROT.PROT.base.base_trainer - INFO - metric_hp_loc_fnr: 0
2024-01-24 01:11:57,784 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_mcc: 0
2024-01-24 01:11:57,785 - PROT.PROT.base.base_trainer - INFO - metric_lhp_loc_fnr: 0
2024-01-24 01:11:57,786 - PROT.PROT.base.base_trainer - INFO - metric_species : 0
2024-01-24 01:11:57,787 - PROT.PROT.base.base_trainer - INFO - metric_expression: 0
2024-01-24 01:11:57,788 - PROT.PROT.base.base_trainer - INFO - metric_aggregation: 0.3777777883741591
2024-01-24 01:11:57,789 - PROT.PROT.base.base_trainer - INFO - val_loss       : 0.46700373185532434
2024-01-24 01:11:57,790 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0
2024-01-24 01:11:57,791 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0
2024-01-24 01:11:57,792 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0
2024-01-24 01:11:57,794 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0
2024-01-24 01:11:57,795 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0
2024-01-24 01:11:57,796 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0
2024-01-24 01:11:57,797 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 0
2024-01-24 01:11:57,798 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 0
2024-01-24 01:11:57,799 - PROT.PROT.base.base_trainer - INFO - val_metric_tasa: 0.0
2024-01-24 01:11:57,800 - PROT.PROT.base.base_trainer - INFO - val_metric_thsa: 0.0
2024-01-24 01:11:57,801 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp : 0.0
2024-01-24 01:11:57,802 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_mcc: 0
2024-01-24 01:11:57,803 - PROT.PROT.base.base_trainer - INFO - val_metric_hp_loc_fnr: 0
2024-01-24 01:11:57,804 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_mcc: 0
2024-01-24 01:11:57,805 - PROT.PROT.base.base_trainer - INFO - val_metric_lhp_loc_fnr: 0
2024-01-24 01:11:57,806 - PROT.PROT.base.base_trainer - INFO - val_metric_species: 0
2024-01-24 01:11:57,808 - PROT.PROT.base.base_trainer - INFO - val_metric_expression: 0
2024-01-24 01:11:57,809 - PROT.PROT.base.base_trainer - INFO - val_metric_aggregation: 0.42857143974729944
2024-01-24 01:12:10,975 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/ESM2_singletask/0123-193707/checkpoints/checkpoint-epoch14.pth ...
2024-01-24 01:12:26,177 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/ESM2_singletask/0123-193707/checkpoints/model_best.pth
2024-01-24 01:12:26,448 - PROT.PROT.main - INFO - Initialising evaluation
2024-01-24 01:12:27,373 - PROT.PROT.base.base_eval - INFO - Starting evaluating...
2024-01-24 01:13:54,316 - PROT.PROT.base.base_eval - INFO - metric_ss8: 0.0
2024-01-24 01:13:54,318 - PROT.PROT.base.base_eval - INFO - metric_ss3: 0.0
2024-01-24 01:13:54,319 - PROT.PROT.base.base_eval - INFO - metric_dis_mcc: 0.0
2024-01-24 01:13:54,320 - PROT.PROT.base.base_eval - INFO - metric_dis_fnr: 0.0
2024-01-24 01:13:54,321 - PROT.PROT.base.base_eval - INFO - metric_rsa: 0.0
2024-01-24 01:13:54,322 - PROT.PROT.base.base_eval - INFO - metric_asa: 0.0
2024-01-24 01:13:54,323 - PROT.PROT.base.base_eval - INFO - metric_phi: 0.0
2024-01-24 01:13:54,324 - PROT.PROT.base.base_eval - INFO - metric_psi: 0.0
2024-01-24 01:13:54,325 - PROT.PROT.base.base_eval - INFO - metric_tasa: 0.0
2024-01-24 01:13:54,326 - PROT.PROT.base.base_eval - INFO - metric_thsa: 0.0
2024-01-24 01:13:54,327 - PROT.PROT.base.base_eval - INFO - metric_lhp: 0.0
2024-01-24 01:13:54,328 - PROT.PROT.base.base_eval - INFO - metric_hp_loc_mcc: 0.0
2024-01-24 01:13:54,329 - PROT.PROT.base.base_eval - INFO - metric_hp_loc_fnr: 0.0
2024-01-24 01:13:54,331 - PROT.PROT.base.base_eval - INFO - metric_lhp_loc_mcc: 0.0
2024-01-24 01:13:54,332 - PROT.PROT.base.base_eval - INFO - metric_lhp_loc_fnr: 0.0
2024-01-24 01:13:54,333 - PROT.PROT.base.base_eval - INFO - metric_species: 0.0
2024-01-24 01:13:54,334 - PROT.PROT.base.base_eval - INFO - metric_expression: 0.0
2024-01-24 01:13:54,335 - PROT.PROT.base.base_eval - INFO - metric_aggregation: 0.3610108401155644
2024-01-24 01:13:54,339 - PROT.PROT.main - INFO - Finished!
done
