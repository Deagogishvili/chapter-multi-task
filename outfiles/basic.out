Requirement already satisfied: click in /scistor/informatica/dgi460/anaconda3/envs/dl2022/lib/python3.10/site-packages (8.1.3)
Requirement already satisfied: fair-esm in /scistor/informatica/dgi460/anaconda3/envs/dl2022/lib/python3.10/site-packages (2.0.0)
Processing /scistor/informatica/dgi460/PROT/PROT
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Building wheels for collected packages: PROT
  Building wheel for PROT (setup.py): started
  Building wheel for PROT (setup.py): finished with status 'done'
  Created wheel for PROT: filename=PROT-0.0.1-py3-none-any.whl size=108691 sha256=de402d02cfbb13695f8644c0913b1218800f4d254c7d0889c920f01ba08061a5
  Stored in directory: /scratch/674824/pip-ephem-wheel-cache-24r86ca0/wheels/f3/f3/d5/e95d019bbe728e863c8658a0c362e103b5264b28afecea62b9
Successfully built PROT
Installing collected packages: PROT
  Attempting uninstall: PROT
    Found existing installation: PROT 0.0.1
    Uninstalling PROT-0.0.1:
      Successfully uninstalled PROT-0.0.1
Successfully installed PROT-0.0.1
2024-02-18 08:25:00,096 - PROT.PROT.main - INFO - Building: PROT.models.ESM2_original_extended
FINETUNING CHOSEN: LoRA
Gradient Checkpointing 2
2024-02-18 08:25:05,925 - PROT.PROT.models.ESM2_original_extended.model - INFO - <init>: 
ESM2_original_extended(
  (embedding): ESM2Embedding(
    (model): ESM2(
      (embed_tokens): Embedding(33, 1280, padding_idx=1)
      (layers): ModuleList(
        (0): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (12): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (13): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (14): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (15): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (16): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (17): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (18): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (19): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (20): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (21): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (22): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (23): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (24): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (25): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (26): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (27): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (28): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (29): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (30): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (31): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (32): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (contact_head): ContactPredictionHead(
        (regression): Linear(in_features=660, out_features=1, bias=True)
        (activation): Sigmoid()
      )
      (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (lm_head): RobertaLMHead(
        (dense): Linear(in_features=1280, out_features=1280, bias=True)
        (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (conv): ModuleList(
    (0): Sequential(
      (0): Dropout(p=0.5, inplace=False)
      (1): Conv1d(1280, 32, kernel_size=(129,), stride=(1,), padding=(64,))
      (2): ReLU()
    )
    (1): Sequential(
      (0): Dropout(p=0.5, inplace=False)
      (1): Conv1d(1280, 32, kernel_size=(257,), stride=(1,), padding=(128,))
      (2): ReLU()
    )
  )
  (batch_norm): BatchNorm1d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (lstm): LSTM(1344, 1024, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)
  (lstm_dropout_layer): Dropout(p=0.5, inplace=False)
  (ss8): Sequential(
    (0): Linear(in_features=2048, out_features=8, bias=True)
  )
  (ss3): Sequential(
    (0): Linear(in_features=2048, out_features=3, bias=True)
  )
  (disorder): Sequential(
    (0): Linear(in_features=2048, out_features=2, bias=True)
  )
  (rsa): Sequential(
    (0): Linear(in_features=2048, out_features=1, bias=True)
    (1): Sigmoid()
  )
  (phi): Sequential(
    (0): Linear(in_features=2048, out_features=2, bias=True)
    (1): Tanh()
  )
  (psi): Sequential(
    (0): Linear(in_features=2048, out_features=2, bias=True)
    (1): Tanh()
  )
  (tasa): Sequential(
    (0): Linear(in_features=2048, out_features=1, bias=True)
  )
  (thsa): Sequential(
    (0): Linear(in_features=2048, out_features=1, bias=True)
  )
  (lhp): Sequential(
    (0): Linear(in_features=2048, out_features=1, bias=True)
  )
  (hp_loc): Sequential(
    (0): Linear(in_features=2048, out_features=2, bias=True)
  )
  (lhp_loc): Sequential(
    (0): Linear(in_features=2048, out_features=2, bias=True)
  )
  (species): Sequential(
    (0): Linear(in_features=2048, out_features=10, bias=True)
  )
  (expression): Sequential(
    (0): Linear(in_features=2048, out_features=2, bias=True)
  )
  (aggregation): Sequential(
    (0): Linear(in_features=2048, out_features=2, bias=True)
  )
)
Trainable parameters: 61504231
2024-02-18 08:25:05,936 - PROT.PROT.main - INFO - Using devices [0] of available devices [0]
Using devices [0] of available devices [0]
2024-02-18 08:25:07,243 - PROT.PROT.main - INFO - Building: torch.optim.Adam
2024-02-18 08:25:07,247 - PROT.PROT.main - INFO - Building: PROT.data_loader.augmentation.sparse_token
FINETUNING CHOSEN: no finetuning
Gradient Checkpointing None
2024-02-18 08:25:08,595 - PROT.PROT.main - INFO - Building: PROT.data_loader.data_loaders.NSPDataLoader
2024-02-18 08:29:16,164 - PROT.PROT.main - INFO - Getting loss and metric function handles
2024-02-18 08:29:16,166 - PROT.PROT.main - INFO - Initialising trainer
GRADIENT ACCUMULATION 6
2024-02-18 08:29:16,178 - PROT.PROT.base.base_trainer - INFO - Starting training...
Multi Task Loss
SS8 : 1 // SS3: 5 // DIS: 5 // RSA: 100 // PHI: 5 // PSI: 5
2024-02-18 08:29:20,231 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [0/10308 (0%)] Loss: 17.523422
2024-02-18 08:33:47,359 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [900/10308 (9%)] Loss: 8.051588
2024-02-18 08:38:09,767 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [1800/10308 (17%)] Loss: 7.895588
2024-02-18 08:42:33,013 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [2700/10308 (26%)] Loss: 7.333611
2024-02-18 08:47:02,574 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [3600/10308 (35%)] Loss: 8.163786
2024-02-18 08:51:26,135 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [4500/10308 (44%)] Loss: 6.744476
2024-02-18 08:55:48,341 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [5400/10308 (52%)] Loss: 7.743315
2024-02-18 09:00:13,261 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [6300/10308 (61%)] Loss: 9.359033
2024-02-18 09:04:37,656 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [7200/10308 (70%)] Loss: 6.557192
2024-02-18 09:09:07,607 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [8100/10308 (79%)] Loss: 7.555952
2024-02-18 09:13:28,906 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [9000/10308 (87%)] Loss: 7.488065
2024-02-18 09:17:55,128 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [9900/10308 (96%)] Loss: 6.675498
2024-02-18 09:21:16,434 - PROT.PROT.base.base_trainer - INFO - epoch          : 0
2024-02-18 09:21:16,438 - PROT.PROT.base.base_trainer - INFO - loss           : 7.539672561370121
2024-02-18 09:21:16,439 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.6789512590815624
2024-02-18 09:21:16,440 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8104840119679769
2024-02-18 09:21:16,442 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.4555217094408969
2024-02-18 09:21:16,443 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.048785453545860946
2024-02-18 09:21:16,444 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.7209473732703676
2024-02-18 09:21:16,446 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.748436643431584
2024-02-18 09:21:16,447 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 25.134789784749348
2024-02-18 09:21:16,448 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 32.18378512064616
2024-02-18 09:21:16,449 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.08206445968459
2024-02-18 09:21:16,450 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7602521517619876
2024-02-18 09:21:16,451 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8696553743413453
2024-02-18 09:21:16,453 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6051523468407332
2024-02-18 09:21:16,454 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.009925228735600422
2024-02-18 09:21:16,455 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8124748485994515
2024-02-18 09:21:16,456 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8251952309010213
2024-02-18 09:21:16,457 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.52035814017827
2024-02-18 09:21:16,459 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.174832196253252
2024-02-18 09:21:36,665 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/Basic_w_LoRa/0218-082916/checkpoints/checkpoint-epoch0.pth ...
2024-02-18 09:22:01,791 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/Basic_w_LoRa/0218-082916/checkpoints/model_best.pth
2024-02-18 09:22:04,809 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [0/10308 (0%)] Loss: 6.647789
2024-02-18 09:26:27,688 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [900/10308 (9%)] Loss: 6.819804
2024-02-18 09:30:48,743 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [1800/10308 (17%)] Loss: 6.979955
2024-02-18 09:35:18,437 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [2700/10308 (26%)] Loss: 6.813818
2024-02-18 09:39:46,497 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [3600/10308 (35%)] Loss: 7.388371
2024-02-18 09:44:19,868 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [4500/10308 (44%)] Loss: 6.584901
2024-02-18 09:48:29,876 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [5400/10308 (52%)] Loss: 9.216902
2024-02-18 09:53:02,576 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [6300/10308 (61%)] Loss: 11.002660
2024-02-18 09:57:34,663 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [7200/10308 (70%)] Loss: 6.670053
2024-02-18 10:02:05,427 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [8100/10308 (79%)] Loss: 7.423117
2024-02-18 10:06:33,961 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [9000/10308 (87%)] Loss: 6.136168
2024-02-18 10:10:55,532 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [9900/10308 (96%)] Loss: 6.767922
2024-02-18 10:14:02,857 - PROT.PROT.base.base_trainer - INFO - epoch          : 1
2024-02-18 10:14:02,860 - PROT.PROT.base.base_trainer - INFO - loss           : 7.097171063888612
2024-02-18 10:14:02,861 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7365314811468124
2024-02-18 10:14:02,863 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8493794500827789
2024-02-18 10:14:02,864 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.6162448339164257
2024-02-18 10:14:02,866 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.01035529658353577
2024-02-18 10:14:02,867 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.7846516321102778
2024-02-18 10:14:02,868 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.798958366115888
2024-02-18 10:14:02,869 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 17.203240791956585
2024-02-18 10:14:02,871 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 25.681264797846477
2024-02-18 10:14:02,872 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.049520502231218
2024-02-18 10:14:02,873 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7644203737433106
2024-02-18 10:14:02,874 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.870600078818543
2024-02-18 10:14:02,876 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6252412084039864
2024-02-18 10:14:02,877 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.014835290301924509
2024-02-18 10:14:02,878 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.813969762452854
2024-02-18 10:14:02,879 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8272353925388237
2024-02-18 10:14:02,881 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.369091086721948
2024-02-18 10:14:02,882 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.855600707205458
2024-02-18 10:14:21,129 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/Basic_w_LoRa/0218-082916/checkpoints/checkpoint-epoch1.pth ...
2024-02-18 10:14:46,715 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/Basic_w_LoRa/0218-082916/checkpoints/model_best.pth
2024-02-18 10:14:48,513 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [0/10308 (0%)] Loss: 7.729181
2024-02-18 10:19:09,970 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [900/10308 (9%)] Loss: 7.058786
2024-02-18 10:23:29,987 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [1800/10308 (17%)] Loss: 6.554353
2024-02-18 10:28:03,662 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [2700/10308 (26%)] Loss: 6.717754
2024-02-18 10:32:29,104 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [3600/10308 (35%)] Loss: 7.695297
2024-02-18 10:36:56,779 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [4500/10308 (44%)] Loss: 6.339155
2024-02-18 10:41:41,239 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [5400/10308 (52%)] Loss: 6.603058
2024-02-18 10:46:11,572 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [6300/10308 (61%)] Loss: 6.756050
2024-02-18 10:50:45,417 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [7200/10308 (70%)] Loss: 6.479969
2024-02-18 10:55:18,501 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [8100/10308 (79%)] Loss: 7.260557
2024-02-18 10:59:37,969 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [9000/10308 (87%)] Loss: 8.253798
2024-02-18 11:04:00,771 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [9900/10308 (96%)] Loss: 6.613512
2024-02-18 11:07:15,495 - PROT.PROT.base.base_trainer - INFO - epoch          : 2
2024-02-18 11:07:15,497 - PROT.PROT.base.base_trainer - INFO - loss           : 7.038502015721421
2024-02-18 11:07:15,499 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7638488560914993
2024-02-18 11:07:15,501 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8825979828834534
2024-02-18 11:07:15,502 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.5766359257201353
2024-02-18 11:07:15,504 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.009375467013645297
2024-02-18 11:07:15,505 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8136072407166163
2024-02-18 11:07:15,506 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8275846640268961
2024-02-18 11:07:15,507 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 16.21010732650757
2024-02-18 11:07:15,509 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 22.341843128204346
2024-02-18 11:07:15,510 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.021930649711637
2024-02-18 11:07:15,511 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7657151677511715
2024-02-18 11:07:15,512 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8734478396243275
2024-02-18 11:07:15,513 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6176684520998197
2024-02-18 11:07:15,515 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.013549687008363129
2024-02-18 11:07:15,516 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8170871608151721
2024-02-18 11:07:15,517 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8306355057387335
2024-02-18 11:07:15,519 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.196266015957203
2024-02-18 11:07:15,520 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.62215227042617
2024-02-18 11:07:34,821 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/Basic_w_LoRa/0218-082916/checkpoints/checkpoint-epoch2.pth ...
2024-02-18 11:08:03,444 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/Basic_w_LoRa/0218-082916/checkpoints/model_best.pth
2024-02-18 11:08:05,178 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [0/10308 (0%)] Loss: 7.051402
2024-02-18 11:12:31,031 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [900/10308 (9%)] Loss: 6.317558
2024-02-18 11:17:00,715 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [1800/10308 (17%)] Loss: 5.831420
2024-02-18 11:21:39,064 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [2700/10308 (26%)] Loss: 6.212336
2024-02-18 11:26:14,630 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [3600/10308 (35%)] Loss: 6.711697
2024-02-18 11:30:34,169 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [4500/10308 (44%)] Loss: 5.941008
2024-02-18 11:34:58,183 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [5400/10308 (52%)] Loss: 7.476400
2024-02-18 11:39:20,743 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [6300/10308 (61%)] Loss: 7.349868
2024-02-18 11:43:45,244 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [7200/10308 (70%)] Loss: 6.484851
2024-02-18 11:48:09,943 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [8100/10308 (79%)] Loss: 5.873474
2024-02-18 11:52:24,369 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [9000/10308 (87%)] Loss: 6.383379
2024-02-18 11:56:51,314 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [9900/10308 (96%)] Loss: 6.188421
2024-02-18 12:00:01,647 - PROT.PROT.base.base_trainer - INFO - epoch          : 3
2024-02-18 12:00:01,648 - PROT.PROT.base.base_trainer - INFO - loss           : 7.011905117773348
2024-02-18 12:00:01,650 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.8017186224460602
2024-02-18 12:00:01,651 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.897160475452741
2024-02-18 12:00:01,652 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.6058263046046098
2024-02-18 12:00:01,653 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.009287489364699772
2024-02-18 12:00:01,655 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8452470501263937
2024-02-18 12:00:01,656 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8617724676926931
2024-02-18 12:00:01,657 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 15.004105567932129
2024-02-18 12:00:01,658 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 19.982930898666382
2024-02-18 12:00:01,659 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.039431081926691
2024-02-18 12:00:01,660 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7661890760339054
2024-02-18 12:00:01,661 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.872480901844827
2024-02-18 12:00:01,662 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.5968842950834442
2024-02-18 12:00:01,664 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.008102227512826951
2024-02-18 12:00:01,665 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.818371607918581
2024-02-18 12:00:01,666 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8310567284642111
2024-02-18 12:00:01,667 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.294706626131966
2024-02-18 12:00:01,668 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.614271220245925
2024-02-18 12:00:18,987 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/Basic_w_LoRa/0218-082916/checkpoints/checkpoint-epoch3.pth ...
2024-02-18 12:00:21,806 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [0/10308 (0%)] Loss: 6.217879
2024-02-18 12:04:37,169 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [900/10308 (9%)] Loss: 7.496691
2024-02-18 12:09:04,562 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [1800/10308 (17%)] Loss: 7.027324
2024-02-18 12:13:38,295 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [2700/10308 (26%)] Loss: 6.123812
2024-02-18 12:18:05,559 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [3600/10308 (35%)] Loss: 6.424133
2024-02-18 12:22:32,789 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [4500/10308 (44%)] Loss: 7.353292
2024-02-18 12:26:48,114 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [5400/10308 (52%)] Loss: 6.268771
2024-02-18 12:31:19,577 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [6300/10308 (61%)] Loss: 7.970775
2024-02-18 12:35:33,258 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [7200/10308 (70%)] Loss: 7.100159
2024-02-18 12:40:10,313 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [8100/10308 (79%)] Loss: 7.969993
2024-02-18 12:44:33,896 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [9000/10308 (87%)] Loss: 7.851323
2024-02-18 12:48:59,565 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [9900/10308 (96%)] Loss: 6.115769
2024-02-18 12:52:02,119 - PROT.PROT.base.base_trainer - INFO - epoch          : 4
2024-02-18 12:52:02,123 - PROT.PROT.base.base_trainer - INFO - loss           : 6.995611584628413
2024-02-18 12:52:02,124 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7573873649040858
2024-02-18 12:52:02,125 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.864044855038325
2024-02-18 12:52:02,127 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.5867720370491346
2024-02-18 12:52:02,128 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.011245561394995699
2024-02-18 12:52:02,129 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8187207480271658
2024-02-18 12:52:02,131 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8279902040958405
2024-02-18 12:52:02,132 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 17.51705304781596
2024-02-18 12:52:02,133 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 24.155994017918903
2024-02-18 12:52:02,134 - PROT.PROT.base.base_trainer - INFO - val_loss       : 6.993677525502729
2024-02-18 12:52:02,135 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7683238854487444
2024-02-18 12:52:02,136 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.876764549100531
2024-02-18 12:52:02,138 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6312804582504714
2024-02-18 12:52:02,139 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.014635798575466574
2024-02-18 12:52:02,140 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.818187690309053
2024-02-18 12:52:02,141 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.830550145289115
2024-02-18 12:52:02,142 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.188235451814432
2024-02-18 12:52:02,143 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.510480773404954
2024-02-18 12:52:20,673 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/Basic_w_LoRa/0218-082916/checkpoints/checkpoint-epoch4.pth ...
2024-02-18 12:52:46,334 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/Basic_w_LoRa/0218-082916/checkpoints/model_best.pth
2024-02-18 12:52:47,620 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [0/10308 (0%)] Loss: 6.375889
2024-02-18 12:57:14,116 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [900/10308 (9%)] Loss: 7.695300
2024-02-18 13:01:34,030 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [1800/10308 (17%)] Loss: 7.747902
2024-02-18 13:06:04,490 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [2700/10308 (26%)] Loss: 6.873129
2024-02-18 13:10:43,230 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [3600/10308 (35%)] Loss: 6.339979
2024-02-18 13:15:21,345 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [4500/10308 (44%)] Loss: 6.631601
2024-02-18 13:19:50,421 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [5400/10308 (52%)] Loss: 6.071669
2024-02-18 13:24:23,127 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [6300/10308 (61%)] Loss: 6.770918
2024-02-18 13:28:47,287 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [7200/10308 (70%)] Loss: 6.865509
2024-02-18 13:33:15,185 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [8100/10308 (79%)] Loss: 6.720920
2024-02-18 13:37:29,938 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [9000/10308 (87%)] Loss: 6.387968
2024-02-18 13:41:53,483 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [9900/10308 (96%)] Loss: 6.600082
2024-02-18 13:45:04,373 - PROT.PROT.base.base_trainer - INFO - epoch          : 5
2024-02-18 13:45:04,374 - PROT.PROT.base.base_trainer - INFO - loss           : 6.9977231276467915
2024-02-18 13:45:04,376 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.8085350195566813
2024-02-18 13:45:04,377 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8922871351242065
2024-02-18 13:45:04,379 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.6522588438043991
2024-02-18 13:45:04,380 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.011057197077510258
2024-02-18 13:45:04,382 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.817412942647934
2024-02-18 13:45:04,383 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8259821534156799
2024-02-18 13:45:04,384 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 14.361337184906006
2024-02-18 13:45:04,386 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 20.903027216593426
2024-02-18 13:45:04,387 - PROT.PROT.base.base_trainer - INFO - val_loss       : 6.980751606811016
2024-02-18 13:45:04,388 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.769132423334896
2024-02-18 13:45:04,389 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8765393990432204
2024-02-18 13:45:04,391 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6319518574716255
2024-02-18 13:45:04,392 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.016309179468948148
2024-02-18 13:45:04,394 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8202782377765627
2024-02-18 13:45:04,395 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8325557412916443
2024-02-18 13:45:04,396 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.212122493124536
2024-02-18 13:45:04,398 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.508972759176444
2024-02-18 13:45:22,947 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/Basic_w_LoRa/0218-082916/checkpoints/checkpoint-epoch5.pth ...
2024-02-18 13:45:42,803 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/Basic_w_LoRa/0218-082916/checkpoints/model_best.pth
2024-02-18 13:45:44,899 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [0/10308 (0%)] Loss: 6.454462
2024-02-18 13:50:19,922 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [900/10308 (9%)] Loss: 6.205725
2024-02-18 13:54:48,255 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [1800/10308 (17%)] Loss: 7.972680
2024-02-18 13:59:12,184 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [2700/10308 (26%)] Loss: 6.889342
2024-02-18 14:03:31,288 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [3600/10308 (35%)] Loss: 7.729867
2024-02-18 14:07:58,930 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [4500/10308 (44%)] Loss: 7.297596
2024-02-18 14:12:23,179 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [5400/10308 (52%)] Loss: 6.773050
2024-02-18 14:16:40,515 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [6300/10308 (61%)] Loss: 7.218473
2024-02-18 14:21:06,778 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [7200/10308 (70%)] Loss: 6.429537
2024-02-18 14:25:29,228 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [8100/10308 (79%)] Loss: 6.685029
2024-02-18 14:29:52,274 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [9000/10308 (87%)] Loss: 6.655039
2024-02-18 14:34:24,063 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [9900/10308 (96%)] Loss: 6.281198
2024-02-18 14:37:32,982 - PROT.PROT.base.base_trainer - INFO - epoch          : 6
2024-02-18 14:37:32,985 - PROT.PROT.base.base_trainer - INFO - loss           : 6.978080112633834
2024-02-18 14:37:32,987 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7804273168245951
2024-02-18 14:37:32,989 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8817229817310969
2024-02-18 14:37:32,990 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.628778800368309
2024-02-18 14:37:32,992 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.013379734164724747
2024-02-18 14:37:32,994 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8281216571728388
2024-02-18 14:37:32,995 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.836137463649114
2024-02-18 14:37:33,001 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 16.07026735941569
2024-02-18 14:37:33,005 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 22.77567704518636
2024-02-18 14:37:33,009 - PROT.PROT.base.base_trainer - INFO - val_loss       : 6.9984426911906565
2024-02-18 14:37:33,012 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.768294717223002
2024-02-18 14:37:33,017 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8749534906716364
2024-02-18 14:37:33,022 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6397314608764293
2024-02-18 14:37:33,025 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.016408409968957587
2024-02-18 14:37:33,030 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8184096670238734
2024-02-18 14:37:33,034 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.830643755263508
2024-02-18 14:37:33,037 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.227434902613453
2024-02-18 14:37:33,041 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.550232348846773
2024-02-18 14:37:52,727 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/Basic_w_LoRa/0218-082916/checkpoints/checkpoint-epoch6.pth ...
2024-02-18 14:37:55,001 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [0/10308 (0%)] Loss: 5.904562
2024-02-18 14:42:07,261 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [900/10308 (9%)] Loss: 7.215017
2024-02-18 14:46:28,768 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [1800/10308 (17%)] Loss: 8.075268
2024-02-18 14:50:54,508 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [2700/10308 (26%)] Loss: 7.320044
2024-02-18 14:55:23,694 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [3600/10308 (35%)] Loss: 7.235100
2024-02-18 14:59:52,858 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [4500/10308 (44%)] Loss: 7.277459
2024-02-18 15:04:29,208 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [5400/10308 (52%)] Loss: 6.651893
2024-02-18 15:09:00,862 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [6300/10308 (61%)] Loss: 6.768810
2024-02-18 15:13:25,452 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [7200/10308 (70%)] Loss: 6.162798
2024-02-18 15:17:49,262 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [8100/10308 (79%)] Loss: 6.558480
2024-02-18 15:22:22,376 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [9000/10308 (87%)] Loss: 6.351769
2024-02-18 15:26:41,506 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [9900/10308 (96%)] Loss: 6.402721
2024-02-18 15:29:54,957 - PROT.PROT.base.base_trainer - INFO - epoch          : 7
2024-02-18 15:29:54,961 - PROT.PROT.base.base_trainer - INFO - loss           : 6.982653662322355
2024-02-18 15:29:54,962 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.791122113664945
2024-02-18 15:29:54,964 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8912960837284724
2024-02-18 15:29:54,965 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.6625110308329264
2024-02-18 15:29:54,966 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.015036917383743761
2024-02-18 15:29:54,968 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8226025352875391
2024-02-18 15:29:54,969 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8339542100826899
2024-02-18 15:29:54,970 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 15.95473019282023
2024-02-18 15:29:54,971 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 21.96074612935384
2024-02-18 15:29:54,973 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.0135936358758
2024-02-18 15:29:54,974 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7670555895544946
2024-02-18 15:29:54,975 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8743167566857215
2024-02-18 15:29:54,976 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6048659806957051
2024-02-18 15:29:54,977 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.010044408033922404
2024-02-18 15:29:54,979 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8169439720931528
2024-02-18 15:29:54,980 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8296072285113739
2024-02-18 15:29:54,981 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.258585766232763
2024-02-18 15:29:54,982 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.62726603222949
2024-02-18 15:30:12,990 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/Basic_w_LoRa/0218-082916/checkpoints/checkpoint-epoch7.pth ...
2024-02-18 15:30:16,199 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [0/10308 (0%)] Loss: 6.700768
2024-02-18 15:34:40,434 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [900/10308 (9%)] Loss: 6.500779
2024-02-18 15:39:11,799 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [1800/10308 (17%)] Loss: 5.981882
2024-02-18 15:43:39,715 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [2700/10308 (26%)] Loss: 7.294991
2024-02-18 15:47:54,697 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [3600/10308 (35%)] Loss: 7.730559
2024-02-18 15:52:24,776 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [4500/10308 (44%)] Loss: 6.408312
2024-02-18 15:56:54,435 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [5400/10308 (52%)] Loss: 6.802169
2024-02-18 16:01:25,209 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [6300/10308 (61%)] Loss: 6.236368
2024-02-18 16:05:55,361 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [7200/10308 (70%)] Loss: 6.600419
2024-02-18 16:10:20,112 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [8100/10308 (79%)] Loss: 6.581095
2024-02-18 16:14:45,613 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [9000/10308 (87%)] Loss: 6.561049
2024-02-18 16:19:10,976 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [9900/10308 (96%)] Loss: 6.857080
2024-02-18 16:22:22,449 - PROT.PROT.base.base_trainer - INFO - epoch          : 8
2024-02-18 16:22:22,453 - PROT.PROT.base.base_trainer - INFO - loss           : 6.98191216120136
2024-02-18 16:22:22,455 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7887337952852249
2024-02-18 16:22:22,456 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8905500322580338
2024-02-18 16:22:22,457 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.71914342045784
2024-02-18 16:22:22,459 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.009701040631625801
2024-02-18 16:22:22,460 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8348956455787023
2024-02-18 16:22:22,462 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8432754476865133
2024-02-18 16:22:22,463 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 16.254844188690186
2024-02-18 16:22:22,464 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 21.86875645319621
2024-02-18 16:22:22,465 - PROT.PROT.base.base_trainer - INFO - val_loss       : 6.977473098853418
2024-02-18 16:22:22,466 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.767590512208833
2024-02-18 16:22:22,468 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8758134692357475
2024-02-18 16:22:22,469 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6240170052261315
2024-02-18 16:22:22,470 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.013424786312481547
2024-02-18 16:22:22,471 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8181565392941127
2024-02-18 16:22:22,472 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8308107176610025
2024-02-18 16:22:22,474 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.299804798351442
2024-02-18 16:22:22,475 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.542448791630594
2024-02-18 16:22:40,619 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/Basic_w_LoRa/0218-082916/checkpoints/checkpoint-epoch8.pth ...
2024-02-18 16:23:01,199 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/Basic_w_LoRa/0218-082916/checkpoints/model_best.pth
2024-02-18 16:23:03,353 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [0/10308 (0%)] Loss: 7.026542
2024-02-18 16:27:39,880 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [900/10308 (9%)] Loss: 6.700439
2024-02-18 16:32:01,275 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [1800/10308 (17%)] Loss: 6.491854
2024-02-18 16:36:25,591 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [2700/10308 (26%)] Loss: 8.817749
2024-02-18 16:40:47,155 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [3600/10308 (35%)] Loss: 7.467299
2024-02-18 16:45:06,952 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [4500/10308 (44%)] Loss: 8.524896
2024-02-18 16:49:33,329 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [5400/10308 (52%)] Loss: 7.270236
2024-02-18 16:54:11,630 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [6300/10308 (61%)] Loss: 7.051520
2024-02-18 16:58:27,175 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [7200/10308 (70%)] Loss: 6.304942
2024-02-18 17:02:56,452 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [8100/10308 (79%)] Loss: 6.698780
2024-02-18 17:07:20,271 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [9000/10308 (87%)] Loss: 6.716433
2024-02-18 17:11:47,384 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [9900/10308 (96%)] Loss: 6.401723
2024-02-18 17:14:57,770 - PROT.PROT.base.base_trainer - INFO - epoch          : 9
2024-02-18 17:14:57,774 - PROT.PROT.base.base_trainer - INFO - loss           : 6.968097094075555
2024-02-18 17:14:57,776 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7412406355142593
2024-02-18 17:14:57,777 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.862032155195872
2024-02-18 17:14:57,778 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.656170167028904
2024-02-18 17:14:57,780 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.0046439145420057075
2024-02-18 17:14:57,781 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.7959844768047333
2024-02-18 17:14:57,782 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8117917676766714
2024-02-18 17:14:57,784 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 18.22399679819743
2024-02-18 17:14:57,785 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 25.751975218454998
2024-02-18 17:14:57,786 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.031757378490209
2024-02-18 17:14:57,787 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7660641236938673
2024-02-18 17:14:57,789 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8747267010467079
2024-02-18 17:14:57,790 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6066873693307028
2024-02-18 17:14:57,791 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.011507262893122486
2024-02-18 17:14:57,792 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8171794721341221
2024-02-18 17:14:57,793 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8300515922453131
2024-02-18 17:14:57,795 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.370753219646723
2024-02-18 17:14:57,796 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.61460928195517
2024-02-18 17:15:16,254 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/Basic_w_LoRa/0218-082916/checkpoints/checkpoint-epoch9.pth ...
2024-02-18 17:15:19,698 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [0/10308 (0%)] Loss: 7.385347
2024-02-18 17:19:51,725 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [900/10308 (9%)] Loss: 7.372568
2024-02-18 17:24:26,875 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [1800/10308 (17%)] Loss: 6.118829
2024-02-18 17:28:42,529 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [2700/10308 (26%)] Loss: 8.110199
2024-02-18 17:33:13,246 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [3600/10308 (35%)] Loss: 6.805285
2024-02-18 17:37:29,473 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [4500/10308 (44%)] Loss: 7.813675
2024-02-18 17:41:59,258 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [5400/10308 (52%)] Loss: 7.835834
2024-02-18 17:46:33,686 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [6300/10308 (61%)] Loss: 6.695072
2024-02-18 17:50:57,828 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [7200/10308 (70%)] Loss: 7.143284
2024-02-18 17:55:28,170 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [8100/10308 (79%)] Loss: 6.839274
2024-02-18 17:59:51,345 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [9000/10308 (87%)] Loss: 6.589686
2024-02-18 18:04:02,200 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [9900/10308 (96%)] Loss: 6.993692
2024-02-18 18:07:13,979 - PROT.PROT.base.base_trainer - INFO - epoch          : 10
2024-02-18 18:07:13,980 - PROT.PROT.base.base_trainer - INFO - loss           : 6.9636157500079925
2024-02-18 18:07:13,982 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7584879944721857
2024-02-18 18:07:13,983 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8666998594999313
2024-02-18 18:07:13,984 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.49606577927867573
2024-02-18 18:07:13,985 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.011806463667502006
2024-02-18 18:07:13,987 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8081097702185313
2024-02-18 18:07:13,988 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8172911703586578
2024-02-18 18:07:13,989 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 17.852859099706013
2024-02-18 18:07:13,990 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 23.81692083676656
2024-02-18 18:07:13,991 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.039676615232911
2024-02-18 18:07:13,992 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7669082759051306
2024-02-18 18:07:13,994 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8753140492852763
2024-02-18 18:07:13,995 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6263281127980934
2024-02-18 18:07:13,996 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.015050955840907957
2024-02-18 18:07:13,997 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8171685743815784
2024-02-18 18:07:13,998 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8302874114240667
2024-02-18 18:07:14,000 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.31986097392121
2024-02-18 18:07:14,001 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.6450214825873
2024-02-18 18:07:39,044 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/Basic_w_LoRa/0218-082916/checkpoints/checkpoint-epoch10.pth ...
2024-02-18 18:07:41,709 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [0/10308 (0%)] Loss: 6.638505
2024-02-18 18:11:51,771 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [900/10308 (9%)] Loss: 6.684176
2024-02-18 18:16:11,727 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [1800/10308 (17%)] Loss: 7.742834
2024-02-18 18:20:32,726 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [2700/10308 (26%)] Loss: 7.183053
2024-02-18 18:25:02,450 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [3600/10308 (35%)] Loss: 7.266506
2024-02-18 18:29:27,543 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [4500/10308 (44%)] Loss: 6.684893
2024-02-18 18:33:51,119 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [5400/10308 (52%)] Loss: 6.742899
2024-02-18 18:38:20,681 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [6300/10308 (61%)] Loss: 6.359199
2024-02-18 18:42:50,970 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [7200/10308 (70%)] Loss: 6.389852
2024-02-18 18:47:30,102 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [8100/10308 (79%)] Loss: 9.006622
2024-02-18 18:52:00,613 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [9000/10308 (87%)] Loss: 7.091704
2024-02-18 18:56:25,486 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [9900/10308 (96%)] Loss: 7.326991
2024-02-18 18:59:33,586 - PROT.PROT.base.base_trainer - INFO - epoch          : 11
2024-02-18 18:59:33,590 - PROT.PROT.base.base_trainer - INFO - loss           : 6.970611567870173
2024-02-18 18:59:33,592 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7739842434724172
2024-02-18 18:59:33,593 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8721008400122324
2024-02-18 18:59:33,594 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.7199741477767626
2024-02-18 18:59:33,596 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.00995668608811684
2024-02-18 18:59:33,597 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.794485072294871
2024-02-18 18:59:33,598 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8002013017733892
2024-02-18 18:59:33,599 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 17.512220462163288
2024-02-18 18:59:33,601 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 24.744664192199707
2024-02-18 18:59:33,602 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.008526004988329
2024-02-18 18:59:33,603 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7686224191611103
2024-02-18 18:59:33,605 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8750469717592331
2024-02-18 18:59:33,606 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6164635355956157
2024-02-18 18:59:33,607 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.012381584993761198
2024-02-18 18:59:33,608 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8185797025796672
2024-02-18 18:59:33,609 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8309729102572831
2024-02-18 18:59:33,610 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.263676968887722
2024-02-18 18:59:33,612 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.957563150412923
2024-02-18 18:59:51,704 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/Basic_w_LoRa/0218-082916/checkpoints/checkpoint-epoch11.pth ...
2024-02-18 18:59:54,269 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [0/10308 (0%)] Loss: 6.776002
2024-02-18 19:04:09,595 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [900/10308 (9%)] Loss: 7.169574
2024-02-18 19:08:26,458 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [1800/10308 (17%)] Loss: 7.119022
2024-02-18 19:12:52,434 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [2700/10308 (26%)] Loss: 6.513350
2024-02-18 19:17:24,589 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [3600/10308 (35%)] Loss: 5.824964
2024-02-18 19:21:58,375 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [4500/10308 (44%)] Loss: 6.592083
2024-02-18 19:26:19,944 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [5400/10308 (52%)] Loss: 7.835861
2024-02-18 19:30:42,426 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [6300/10308 (61%)] Loss: 6.599029
2024-02-18 19:35:01,036 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [7200/10308 (70%)] Loss: 7.085428
2024-02-18 19:39:31,005 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [8100/10308 (79%)] Loss: 6.590352
2024-02-18 19:43:59,974 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [9000/10308 (87%)] Loss: 6.845844
2024-02-18 19:48:29,229 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [9900/10308 (96%)] Loss: 5.879897
2024-02-18 19:51:41,673 - PROT.PROT.base.base_trainer - INFO - epoch          : 12
2024-02-18 19:51:41,677 - PROT.PROT.base.base_trainer - INFO - loss           : 6.9581983315234455
2024-02-18 19:51:41,678 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7746509412924448
2024-02-18 19:51:41,679 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8831630100806555
2024-02-18 19:51:41,681 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.627068734417359
2024-02-18 19:51:41,682 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.006715467267592127
2024-02-18 19:51:41,683 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8361524244149526
2024-02-18 19:51:41,685 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.843630795677503
2024-02-18 19:51:41,686 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 16.129319588343304
2024-02-18 19:51:41,687 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 21.512434403101604
2024-02-18 19:51:41,688 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.006606877070071
2024-02-18 19:51:41,689 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7680663492846753
2024-02-18 19:51:41,691 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8758726776525984
2024-02-18 19:51:41,692 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.608871103618605
2024-02-18 19:51:41,693 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.014024620359150067
2024-02-18 19:51:41,694 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8188910456821048
2024-02-18 19:51:41,695 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8309205673717485
2024-02-18 19:51:41,696 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.18128228979357
2024-02-18 19:51:41,697 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.400040320364752
2024-02-18 19:51:59,375 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/Basic_w_LoRa/0218-082916/checkpoints/checkpoint-epoch12.pth ...
2024-02-18 19:52:01,447 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [0/10308 (0%)] Loss: 6.245995
2024-02-18 19:56:24,122 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [900/10308 (9%)] Loss: 6.979877
2024-02-18 20:00:53,090 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [1800/10308 (17%)] Loss: 6.266007
2024-02-18 20:05:17,448 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [2700/10308 (26%)] Loss: 6.880942
2024-02-18 20:09:35,433 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [3600/10308 (35%)] Loss: 7.411246
2024-02-18 20:13:58,573 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [4500/10308 (44%)] Loss: 7.852392
2024-02-18 20:18:26,114 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [5400/10308 (52%)] Loss: 7.868612
2024-02-18 20:22:46,497 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [6300/10308 (61%)] Loss: 7.404534
2024-02-18 20:27:19,577 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [7200/10308 (70%)] Loss: 6.135218
2024-02-18 20:31:43,436 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [8100/10308 (79%)] Loss: 6.343340
2024-02-18 20:36:17,170 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [9000/10308 (87%)] Loss: 6.302959
2024-02-18 20:40:47,337 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [9900/10308 (96%)] Loss: 7.296939
2024-02-18 20:43:53,826 - PROT.PROT.base.base_trainer - INFO - epoch          : 13
2024-02-18 20:43:53,830 - PROT.PROT.base.base_trainer - INFO - loss           : 6.961936291735912
2024-02-18 20:43:53,832 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7605493168036143
2024-02-18 20:43:53,833 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.879321942726771
2024-02-18 20:43:53,835 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.6903273090720177
2024-02-18 20:43:53,836 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.009557116417757546
2024-02-18 20:43:53,837 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8080338835716248
2024-02-18 20:43:53,838 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.820936049024264
2024-02-18 20:43:53,840 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 17.69844635327657
2024-02-18 20:43:53,841 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 24.28904088338216
2024-02-18 20:43:53,842 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.0548038438677345
2024-02-18 20:43:53,843 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7651759979909637
2024-02-18 20:43:53,844 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8737103823586144
2024-02-18 20:43:53,845 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.5969450867501159
2024-02-18 20:43:53,847 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.010819018418502021
2024-02-18 20:43:53,848 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8157937936897207
2024-02-18 20:43:53,849 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8286327430462925
2024-02-18 20:43:53,850 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.394812341105894
2024-02-18 20:43:53,852 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.71025444748657
2024-02-18 20:44:12,039 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/Basic_w_LoRa/0218-082916/checkpoints/checkpoint-epoch13.pth ...
2024-02-18 20:44:14,232 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [0/10308 (0%)] Loss: 6.538985
2024-02-18 20:48:34,325 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [900/10308 (9%)] Loss: 6.325106
2024-02-18 20:52:59,309 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [1800/10308 (17%)] Loss: 7.551751
2024-02-18 20:57:43,503 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [2700/10308 (26%)] Loss: 6.585066
2024-02-18 21:02:09,959 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [3600/10308 (35%)] Loss: 7.660400
2024-02-18 21:06:29,514 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [4500/10308 (44%)] Loss: 7.061779
2024-02-18 21:10:59,041 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [5400/10308 (52%)] Loss: 6.068921
2024-02-18 21:15:23,635 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [6300/10308 (61%)] Loss: 7.480529
2024-02-18 21:19:41,757 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [7200/10308 (70%)] Loss: 6.674863
2024-02-18 21:24:14,908 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [8100/10308 (79%)] Loss: 8.159125
2024-02-18 21:28:34,887 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [9000/10308 (87%)] Loss: 6.566160
2024-02-18 21:33:05,571 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [9900/10308 (96%)] Loss: 7.443002
2024-02-18 21:36:12,559 - PROT.PROT.base.base_trainer - INFO - epoch          : 14
2024-02-18 21:36:12,560 - PROT.PROT.base.base_trainer - INFO - loss           : 6.944822067569442
2024-02-18 21:36:12,561 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7584403902292252
2024-02-18 21:36:12,563 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8734058986107508
2024-02-18 21:36:12,564 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.6362052230397239
2024-02-18 21:36:12,566 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.0050538678575928015
2024-02-18 21:36:12,567 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8008477240800858
2024-02-18 21:36:12,568 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8111384312311808
2024-02-18 21:36:12,569 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 18.060685237248737
2024-02-18 21:36:12,571 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 24.30496374766032
2024-02-18 21:36:12,572 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.019153025757343
2024-02-18 21:36:12,573 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7662718943784158
2024-02-18 21:36:12,575 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8746148696464806
2024-02-18 21:36:12,576 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.5984160466161116
2024-02-18 21:36:12,577 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.011775065436392226
2024-02-18 21:36:12,578 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8178857434059861
2024-02-18 21:36:12,579 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.830354364925645
2024-02-18 21:36:12,581 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.241724501676664
2024-02-18 21:36:12,582 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 22.46412465669132
2024-02-18 21:36:30,490 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/Basic_w_LoRa/0218-082916/checkpoints/checkpoint-epoch14.pth ...
2024-02-18 21:36:30,994 - PROT.PROT.main - INFO - Initialising evaluation
2024-02-18 21:36:32,094 - PROT.PROT.base.base_eval - INFO - Starting evaluating...
2024-02-18 21:36:38,234 - PROT.PROT.base.base_eval - INFO - metric_ss8: 0.6828385761805943
2024-02-18 21:36:38,234 - PROT.PROT.base.base_eval - INFO - metric_ss3: 0.7918718457221985
2024-02-18 21:36:38,236 - PROT.PROT.base.base_eval - INFO - metric_dis_mcc: 0.579194324357169
2024-02-18 21:36:38,237 - PROT.PROT.base.base_eval - INFO - metric_dis_fnr: 0.029367842578462193
2024-02-18 21:36:38,239 - PROT.PROT.base.base_eval - INFO - metric_rsa: 0.7198604600770133
2024-02-18 21:36:38,241 - PROT.PROT.base.base_eval - INFO - metric_asa: 0.7349313412393842
2024-02-18 21:36:38,242 - PROT.PROT.base.base_eval - INFO - metric_phi: 20.390434537615096
2024-02-18 21:36:38,244 - PROT.PROT.base.base_eval - INFO - metric_psi: 31.775617871965682
2024-02-18 21:36:39,407 - PROT.PROT.base.base_eval - INFO - Starting evaluating...
2024-02-18 21:37:44,595 - PROT.PROT.base.base_eval - INFO - metric_ss8: 0.7368209832593015
2024-02-18 21:37:44,599 - PROT.PROT.base.base_eval - INFO - metric_ss3: 0.8677459421910738
2024-02-18 21:37:44,601 - PROT.PROT.base.base_eval - INFO - metric_dis_mcc: 0.1003705888315169
2024-02-18 21:37:44,602 - PROT.PROT.base.base_eval - INFO - metric_dis_fnr: 0.012254194111416214
2024-02-18 21:37:44,603 - PROT.PROT.base.base_eval - INFO - metric_rsa: 0.8155690442051804
2024-02-18 21:37:44,605 - PROT.PROT.base.base_eval - INFO - metric_asa: 0.8282322378186454
2024-02-18 21:37:44,606 - PROT.PROT.base.base_eval - INFO - metric_phi: 18.934288063941644
2024-02-18 21:37:44,607 - PROT.PROT.base.base_eval - INFO - metric_psi: 25.558280571162353
2024-02-18 21:37:45,654 - PROT.PROT.base.base_eval - INFO - Starting evaluating...
2024-02-18 21:38:03,169 - PROT.PROT.base.base_eval - INFO - metric_ss8: 0.7634996719982313
2024-02-18 21:38:03,169 - PROT.PROT.base.base_eval - INFO - metric_ss3: 0.8689758228219073
2024-02-18 21:38:03,171 - PROT.PROT.base.base_eval - INFO - metric_dis_mcc: 0.6496214951348046
2024-02-18 21:38:03,173 - PROT.PROT.base.base_eval - INFO - metric_dis_fnr: 0.014343561158697728
2024-02-18 21:38:03,174 - PROT.PROT.base.base_eval - INFO - metric_rsa: 0.7940453093984853
2024-02-18 21:38:03,175 - PROT.PROT.base.base_eval - INFO - metric_asa: 0.8128769553225973
2024-02-18 21:38:03,177 - PROT.PROT.base.base_eval - INFO - metric_phi: 16.37333303534466
2024-02-18 21:38:03,178 - PROT.PROT.base.base_eval - INFO - metric_psi: 23.590082442242167
2024-02-18 21:38:03,181 - PROT.PROT.main - INFO - Finished!
done
