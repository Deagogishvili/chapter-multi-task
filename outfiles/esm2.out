Requirement already satisfied: click in /scistor/informatica/dgi460/anaconda3/envs/dl2022/lib/python3.10/site-packages (8.1.3)
Requirement already satisfied: fair-esm in /scistor/informatica/dgi460/anaconda3/envs/dl2022/lib/python3.10/site-packages (2.0.0)
Processing /scistor/informatica/dgi460/PROT/PROT
  Preparing metadata (setup.py): started
  Preparing metadata (setup.py): finished with status 'done'
Building wheels for collected packages: PROT
  Building wheel for PROT (setup.py): started
  Building wheel for PROT (setup.py): finished with status 'done'
  Created wheel for PROT: filename=PROT-0.0.1-py3-none-any.whl size=111690 sha256=7dd4fba1be4ee661b1e46f6b3c82196f1286f5ed271cdca199e79acf41504129
  Stored in directory: /scratch/680966/pip-ephem-wheel-cache-1bfbsj92/wheels/f3/f3/d5/e95d019bbe728e863c8658a0c362e103b5264b28afecea62b9
Successfully built PROT
Installing collected packages: PROT
  Attempting uninstall: PROT
    Found existing installation: PROT 0.0.1
    Uninstalling PROT-0.0.1:
      Successfully uninstalled PROT-0.0.1
Successfully installed PROT-0.0.1
2024-02-24 14:09:12,299 - PROT.PROT.main - INFO - Building: PROT.models.ESM2_multitask
FINETUNING CHOSEN: LoRA
Gradient Checkpointing 2
2024-02-24 14:09:34,359 - PROT.PROT.models.ESM2_multitask.model - INFO - Params to learn:
2024-02-24 14:09:34,360 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.0.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,370 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.0.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,376 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.0.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,381 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.0.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,387 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.1.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,392 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.1.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,397 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.1.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,402 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.1.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,407 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.2.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,412 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.2.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,417 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.2.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,423 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.2.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,428 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.3.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,434 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.3.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,439 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.3.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,445 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.3.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,450 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.4.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,457 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.4.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,462 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.4.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,467 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.4.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,473 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.5.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,478 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.5.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,484 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.5.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,489 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.5.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,495 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.6.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,499 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.6.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,506 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.6.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,512 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.6.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,519 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.7.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,523 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.7.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,530 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.7.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,535 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.7.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,542 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.8.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,548 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.8.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,553 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.8.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,559 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.8.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,563 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.9.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,569 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.9.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,575 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.9.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,580 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.9.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,584 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.10.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,589 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.10.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,593 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.10.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,597 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.10.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,603 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.11.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,607 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.11.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,614 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.11.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,619 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.11.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,624 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.12.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,629 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.12.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,635 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.12.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,640 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.12.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,646 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.13.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,651 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.13.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,656 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.13.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,662 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.13.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,667 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.14.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,672 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.14.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,678 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.14.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,683 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.14.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,688 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.15.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,694 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.15.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,699 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.15.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,706 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.15.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,711 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.16.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,716 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.16.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,722 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.16.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,727 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.16.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,732 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.17.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,738 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.17.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,744 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.17.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,749 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.17.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,755 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.18.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,760 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.18.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,764 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.18.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,770 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.18.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,775 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.19.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,780 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.19.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,786 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.19.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,793 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.19.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,800 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.20.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,805 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.20.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,810 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.20.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,816 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.20.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,823 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.21.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,829 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.21.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,835 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.21.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,840 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.21.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,845 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.22.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,849 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.22.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,854 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.22.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,861 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.22.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,866 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.23.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,871 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.23.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,877 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.23.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,883 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.23.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,890 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.24.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,895 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.24.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,901 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.24.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,906 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.24.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,911 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.25.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,917 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.25.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,922 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.25.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,928 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.25.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,933 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.26.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,938 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.26.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,944 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.26.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,949 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.26.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,954 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.27.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,959 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.27.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,964 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.27.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,970 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.27.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,976 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.28.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:34,982 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.28.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:34,987 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.28.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:34,992 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.28.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:34,998 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.29.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:35,003 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.29.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:35,008 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.29.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:35,014 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.29.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:35,020 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.30.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:35,025 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.30.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:35,031 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.30.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:35,036 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.30.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:35,041 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.31.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:35,047 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.31.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:35,053 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.31.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:35,058 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.31.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:35,064 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.32.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:35,070 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.32.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:35,077 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.32.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:35,083 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.32.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:35,093 - PROT.PROT.models.ESM2_multitask.model - INFO - 	ss8.0.weight
2024-02-24 14:09:35,100 - PROT.PROT.models.ESM2_multitask.model - INFO - 	ss8.0.bias
2024-02-24 14:09:35,105 - PROT.PROT.models.ESM2_multitask.model - INFO - 	ss3.0.weight
2024-02-24 14:09:35,111 - PROT.PROT.models.ESM2_multitask.model - INFO - 	ss3.0.bias
2024-02-24 14:09:35,117 - PROT.PROT.models.ESM2_multitask.model - INFO - 	disorder.0.weight
2024-02-24 14:09:35,123 - PROT.PROT.models.ESM2_multitask.model - INFO - 	disorder.0.bias
2024-02-24 14:09:35,128 - PROT.PROT.models.ESM2_multitask.model - INFO - 	rsa.0.weight
2024-02-24 14:09:35,133 - PROT.PROT.models.ESM2_multitask.model - INFO - 	rsa.0.bias
2024-02-24 14:09:35,139 - PROT.PROT.models.ESM2_multitask.model - INFO - 	phi.0.weight
2024-02-24 14:09:35,144 - PROT.PROT.models.ESM2_multitask.model - INFO - 	phi.0.bias
2024-02-24 14:09:35,150 - PROT.PROT.models.ESM2_multitask.model - INFO - 	psi.0.weight
2024-02-24 14:09:35,155 - PROT.PROT.models.ESM2_multitask.model - INFO - 	psi.0.bias
2024-02-24 14:09:35,161 - PROT.PROT.models.ESM2_multitask.model - INFO - 	tasa.0.weight
2024-02-24 14:09:35,166 - PROT.PROT.models.ESM2_multitask.model - INFO - 	tasa.0.bias
2024-02-24 14:09:35,172 - PROT.PROT.models.ESM2_multitask.model - INFO - 	thsa.0.weight
2024-02-24 14:09:35,178 - PROT.PROT.models.ESM2_multitask.model - INFO - 	thsa.0.bias
2024-02-24 14:09:35,183 - PROT.PROT.models.ESM2_multitask.model - INFO - 	lhp.0.weight
2024-02-24 14:09:35,188 - PROT.PROT.models.ESM2_multitask.model - INFO - 	lhp.0.bias
2024-02-24 14:09:35,193 - PROT.PROT.models.ESM2_multitask.model - INFO - 	hp_loc.0.weight
2024-02-24 14:09:35,199 - PROT.PROT.models.ESM2_multitask.model - INFO - 	hp_loc.0.bias
2024-02-24 14:09:35,204 - PROT.PROT.models.ESM2_multitask.model - INFO - 	lhp_loc.0.weight
2024-02-24 14:09:35,210 - PROT.PROT.models.ESM2_multitask.model - INFO - 	lhp_loc.0.bias
2024-02-24 14:09:35,216 - PROT.PROT.models.ESM2_multitask.model - INFO - 	species.0.weight
2024-02-24 14:09:35,221 - PROT.PROT.models.ESM2_multitask.model - INFO - 	species.0.bias
2024-02-24 14:09:35,226 - PROT.PROT.models.ESM2_multitask.model - INFO - 	expression.0.weight
2024-02-24 14:09:35,232 - PROT.PROT.models.ESM2_multitask.model - INFO - 	expression.0.bias
2024-02-24 14:09:35,237 - PROT.PROT.models.ESM2_multitask.model - INFO - 	aggregation.0.weight
2024-02-24 14:09:35,242 - PROT.PROT.models.ESM2_multitask.model - INFO - 	aggregation.0.bias
2024-02-24 14:09:35,249 - PROT.PROT.models.ESM2_multitask.model - INFO - <init>: 
ESM2_multitask(
  (embedding): ESM2Embedding(
    (model): ESM2(
      (embed_tokens): Embedding(33, 1280, padding_idx=1)
      (layers): ModuleList(
        (0): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (6): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (7): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (8): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (9): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (10): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (11): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (12): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (13): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (14): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (15): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (16): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (17): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (18): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (19): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (20): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (21): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (22): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (23): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (24): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (25): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (26): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (27): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (28): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (29): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (30): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (31): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
        (32): TransformerLoRALayer(
          (self_attn): MultiheadAttention(
            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (k_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (v_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (q_LoRA): LowRankProjection(1280, 1280, rank=6, alpha=6)
            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)
            (rot_emb): RotaryEmbedding()
          )
          (self_attn_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
          (fc1_LoRA): LowRankProjection(1280, 5120, rank=6, alpha=6)
          (fc1): Linear(in_features=1280, out_features=5120, bias=True)
          (fc2_LoRA): LowRankProjection(5120, 1280, rank=6, alpha=6)
          (fc2): Linear(in_features=5120, out_features=1280, bias=True)
          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
        )
      )
      (contact_head): ContactPredictionHead(
        (regression): Linear(in_features=660, out_features=1, bias=True)
        (activation): Sigmoid()
      )
      (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      (lm_head): RobertaLMHead(
        (dense): Linear(in_features=1280, out_features=1280, bias=True)
        (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
  (ss8): Sequential(
    (0): Linear(in_features=1280, out_features=8, bias=True)
  )
  (ss3): Sequential(
    (0): Linear(in_features=1280, out_features=3, bias=True)
  )
  (disorder): Sequential(
    (0): Linear(in_features=1280, out_features=2, bias=True)
  )
  (rsa): Sequential(
    (0): Linear(in_features=1280, out_features=1, bias=True)
    (1): Sigmoid()
  )
  (phi): Sequential(
    (0): Linear(in_features=1280, out_features=2, bias=True)
    (1): Tanh()
  )
  (psi): Sequential(
    (0): Linear(in_features=1280, out_features=2, bias=True)
    (1): Tanh()
  )
  (tasa): Sequential(
    (0): Linear(in_features=1280, out_features=1, bias=True)
  )
  (thsa): Sequential(
    (0): Linear(in_features=1280, out_features=1, bias=True)
  )
  (lhp): Sequential(
    (0): Linear(in_features=1280, out_features=1, bias=True)
  )
  (hp_loc): Sequential(
    (0): Linear(in_features=1280, out_features=2, bias=True)
  )
  (lhp_loc): Sequential(
    (0): Linear(in_features=1280, out_features=2, bias=True)
  )
  (species): Sequential(
    (0): Linear(in_features=1280, out_features=10, bias=True)
  )
  (expression): Sequential(
    (0): Linear(in_features=1280, out_features=2, bias=True)
  )
  (aggregation): Sequential(
    (0): Linear(in_features=1280, out_features=2, bias=True)
  )
)
Trainable parameters: 1063719
2024-02-24 14:09:35,266 - PROT.PROT.main - INFO - Using devices [0] of available devices [0]
Using devices [0] of available devices [0]
2024-02-24 14:09:43,397 - PROT.PROT.main - INFO - Building: torch.optim.Adam
2024-02-24 14:09:43,400 - PROT.PROT.models.ESM2_multitask.model - INFO - Params to learn:
2024-02-24 14:09:43,402 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.0.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,404 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.0.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,405 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.0.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,407 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.0.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,409 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.1.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,410 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.1.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,412 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.1.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,414 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.1.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,416 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.2.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,417 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.2.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,419 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.2.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,420 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.2.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,422 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.3.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,424 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.3.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,425 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.3.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,426 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.3.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,428 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.4.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,429 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.4.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,431 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.4.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,432 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.4.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,434 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.5.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,436 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.5.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,437 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.5.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,439 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.5.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,440 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.6.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,442 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.6.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,443 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.6.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,444 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.6.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,446 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.7.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,448 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.7.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,449 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.7.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,451 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.7.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,452 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.8.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,454 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.8.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,455 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.8.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,456 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.8.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,458 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.9.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,459 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.9.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,461 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.9.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,462 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.9.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,464 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.10.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,465 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.10.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,466 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.10.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,467 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.10.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,469 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.11.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,470 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.11.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,471 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.11.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,472 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.11.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,474 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.12.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,475 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.12.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,476 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.12.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,477 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.12.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,479 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.13.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,481 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.13.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,482 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.13.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,483 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.13.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,485 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.14.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,487 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.14.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,488 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.14.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,490 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.14.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,493 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.15.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,495 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.15.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,497 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.15.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,499 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.15.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,500 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.16.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,501 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.16.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,503 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.16.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,505 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.16.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,507 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.17.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,510 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.17.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,513 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.17.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,517 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.17.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,520 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.18.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,524 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.18.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,526 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.18.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,530 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.18.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,533 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.19.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,537 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.19.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,539 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.19.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,543 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.19.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,546 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.20.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,550 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.20.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,553 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.20.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,556 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.20.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,558 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.21.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,561 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.21.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,563 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.21.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,567 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.21.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,570 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.22.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,571 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.22.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,574 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.22.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,577 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.22.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,581 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.23.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,583 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.23.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,586 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.23.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,590 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.23.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,593 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.24.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,597 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.24.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,601 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.24.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,604 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.24.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,607 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.25.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,609 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.25.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,611 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.25.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,613 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.25.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,614 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.26.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,615 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.26.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,617 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.26.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,618 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.26.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,621 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.27.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,622 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.27.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,623 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.27.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,626 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.27.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,630 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.28.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,632 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.28.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,635 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.28.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,638 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.28.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,640 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.29.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,641 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.29.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,643 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.29.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,644 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.29.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,648 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.30.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,651 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.30.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,652 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.30.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,654 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.30.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,655 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.31.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,657 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.31.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,658 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.31.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,660 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.31.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,662 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.32.self_attn.k_LoRA.LoRA_A
2024-02-24 14:09:43,664 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.32.self_attn.k_LoRA.LoRA_B
2024-02-24 14:09:43,666 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.32.self_attn.v_LoRA.LoRA_A
2024-02-24 14:09:43,668 - PROT.PROT.models.ESM2_multitask.model - INFO - 	embedding.model.layers.32.self_attn.v_LoRA.LoRA_B
2024-02-24 14:09:43,670 - PROT.PROT.models.ESM2_multitask.model - INFO - 	ss8.0.weight
2024-02-24 14:09:43,672 - PROT.PROT.models.ESM2_multitask.model - INFO - 	ss8.0.bias
2024-02-24 14:09:43,673 - PROT.PROT.models.ESM2_multitask.model - INFO - 	ss3.0.weight
2024-02-24 14:09:43,674 - PROT.PROT.models.ESM2_multitask.model - INFO - 	ss3.0.bias
2024-02-24 14:09:43,676 - PROT.PROT.models.ESM2_multitask.model - INFO - 	disorder.0.weight
2024-02-24 14:09:43,677 - PROT.PROT.models.ESM2_multitask.model - INFO - 	disorder.0.bias
2024-02-24 14:09:43,679 - PROT.PROT.models.ESM2_multitask.model - INFO - 	rsa.0.weight
2024-02-24 14:09:43,681 - PROT.PROT.models.ESM2_multitask.model - INFO - 	rsa.0.bias
2024-02-24 14:09:43,683 - PROT.PROT.models.ESM2_multitask.model - INFO - 	phi.0.weight
2024-02-24 14:09:43,684 - PROT.PROT.models.ESM2_multitask.model - INFO - 	phi.0.bias
2024-02-24 14:09:43,686 - PROT.PROT.models.ESM2_multitask.model - INFO - 	psi.0.weight
2024-02-24 14:09:43,687 - PROT.PROT.models.ESM2_multitask.model - INFO - 	psi.0.bias
2024-02-24 14:09:43,689 - PROT.PROT.models.ESM2_multitask.model - INFO - 	tasa.0.weight
2024-02-24 14:09:43,691 - PROT.PROT.models.ESM2_multitask.model - INFO - 	tasa.0.bias
2024-02-24 14:09:43,692 - PROT.PROT.models.ESM2_multitask.model - INFO - 	thsa.0.weight
2024-02-24 14:09:43,694 - PROT.PROT.models.ESM2_multitask.model - INFO - 	thsa.0.bias
2024-02-24 14:09:43,696 - PROT.PROT.models.ESM2_multitask.model - INFO - 	lhp.0.weight
2024-02-24 14:09:43,697 - PROT.PROT.models.ESM2_multitask.model - INFO - 	lhp.0.bias
2024-02-24 14:09:43,699 - PROT.PROT.models.ESM2_multitask.model - INFO - 	hp_loc.0.weight
2024-02-24 14:09:43,701 - PROT.PROT.models.ESM2_multitask.model - INFO - 	hp_loc.0.bias
2024-02-24 14:09:43,702 - PROT.PROT.models.ESM2_multitask.model - INFO - 	lhp_loc.0.weight
2024-02-24 14:09:43,703 - PROT.PROT.models.ESM2_multitask.model - INFO - 	lhp_loc.0.bias
2024-02-24 14:09:43,705 - PROT.PROT.models.ESM2_multitask.model - INFO - 	species.0.weight
2024-02-24 14:09:43,707 - PROT.PROT.models.ESM2_multitask.model - INFO - 	species.0.bias
2024-02-24 14:09:43,708 - PROT.PROT.models.ESM2_multitask.model - INFO - 	expression.0.weight
2024-02-24 14:09:43,710 - PROT.PROT.models.ESM2_multitask.model - INFO - 	expression.0.bias
2024-02-24 14:09:43,711 - PROT.PROT.models.ESM2_multitask.model - INFO - 	aggregation.0.weight
2024-02-24 14:09:43,713 - PROT.PROT.models.ESM2_multitask.model - INFO - 	aggregation.0.bias
2024-02-24 14:09:43,715 - PROT.PROT.main - INFO - Building: PROT.data_loader.augmentation.sparse_token
FINETUNING CHOSEN: no finetuning
Gradient Checkpointing None
2024-02-24 14:09:51,005 - PROT.PROT.main - INFO - Building: PROT.data_loader.data_loaders.NSPDataLoader
2024-02-24 14:13:48,522 - PROT.PROT.main - INFO - Getting loss and metric function handles
2024-02-24 14:13:48,524 - PROT.PROT.main - INFO - Initialising trainer
GRADIENT ACCUMULATION 6
2024-02-24 14:13:48,544 - PROT.PROT.base.base_trainer - INFO - Starting training...
Multi Task Loss
SS8 : 1 // SS3: 5 // DIS: 5 // RSA: 100 // PHI: 5 // PSI: 5
2024-02-24 14:13:55,032 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [0/10308 (0%)] Loss: 18.115793
2024-02-24 14:17:48,986 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [900/10308 (9%)] Loss: 11.165866
2024-02-24 14:21:49,786 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [1800/10308 (17%)] Loss: 9.779931
2024-02-24 14:25:42,957 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [2700/10308 (26%)] Loss: 8.323317
2024-02-24 14:29:38,304 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [3600/10308 (35%)] Loss: 8.624589
2024-02-24 14:33:33,480 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [4500/10308 (44%)] Loss: 8.327094
2024-02-24 14:37:30,168 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [5400/10308 (52%)] Loss: 7.713501
2024-02-24 14:41:22,459 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [6300/10308 (61%)] Loss: 8.199673
2024-02-24 14:45:21,525 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [7200/10308 (70%)] Loss: 7.711205
2024-02-24 14:49:04,692 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [8100/10308 (79%)] Loss: 7.125294
2024-02-24 14:53:01,630 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [9000/10308 (87%)] Loss: 6.496620
2024-02-24 14:56:49,959 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 0 [9900/10308 (96%)] Loss: 7.594860
2024-02-24 14:59:35,802 - PROT.PROT.base.base_trainer - INFO - epoch          : 0
2024-02-24 14:59:35,803 - PROT.PROT.base.base_trainer - INFO - loss           : 8.589665628650131
2024-02-24 14:59:35,809 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.6236426383256912
2024-02-24 14:59:35,815 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.7428041473031044
2024-02-24 14:59:35,819 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.5477143964833684
2024-02-24 14:59:35,822 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.08316821668025416
2024-02-24 14:59:35,823 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.6759779155254364
2024-02-24 14:59:35,825 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.702446182568868
2024-02-24 14:59:35,827 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 25.533004919687908
2024-02-24 14:59:35,828 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 34.929515520731606
2024-02-24 14:59:35,829 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.40634188968757
2024-02-24 14:59:35,831 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7407668453960841
2024-02-24 14:59:35,835 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8549524179463897
2024-02-24 14:59:35,838 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.5893162963018371
2024-02-24 14:59:35,841 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.012460747918108053
2024-02-24 14:59:35,843 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.7823302080490493
2024-02-24 14:59:35,844 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.7988962207773075
2024-02-24 14:59:35,846 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 17.4018186009678
2024-02-24 14:59:35,849 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 24.87452549248164
2024-02-24 14:59:58,060 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/esm2/0224-141348/checkpoints/checkpoint-epoch0.pth ...
2024-02-24 15:00:22,367 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/esm2/0224-141348/checkpoints/model_best.pth
2024-02-24 15:00:23,379 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [0/10308 (0%)] Loss: 7.102773
2024-02-24 15:04:12,384 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [900/10308 (9%)] Loss: 6.552651
2024-02-24 15:08:03,941 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [1800/10308 (17%)] Loss: 7.594704
2024-02-24 15:12:02,346 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [2700/10308 (26%)] Loss: 8.017306
2024-02-24 15:15:57,402 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [3600/10308 (35%)] Loss: 7.907477
2024-02-24 15:19:59,810 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [4500/10308 (44%)] Loss: 7.697969
2024-02-24 15:23:56,399 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [5400/10308 (52%)] Loss: 7.999687
2024-02-24 15:27:49,943 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [6300/10308 (61%)] Loss: 7.233879
2024-02-24 15:31:55,098 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [7200/10308 (70%)] Loss: 7.056090
2024-02-24 15:35:47,079 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [8100/10308 (79%)] Loss: 8.790389
2024-02-24 15:39:44,407 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [9000/10308 (87%)] Loss: 7.216467
2024-02-24 15:43:39,496 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 1 [9900/10308 (96%)] Loss: 7.336596
2024-02-24 15:46:28,002 - PROT.PROT.base.base_trainer - INFO - epoch          : 1
2024-02-24 15:46:28,006 - PROT.PROT.base.base_trainer - INFO - loss           : 7.324551914262466
2024-02-24 15:46:28,013 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7266990393400192
2024-02-24 15:46:28,020 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8371924261252085
2024-02-24 15:46:28,028 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.5088566554089388
2024-02-24 15:46:28,036 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.019394260496483184
2024-02-24 15:46:28,043 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.7796091139316559
2024-02-24 15:46:28,047 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.7964350432157516
2024-02-24 15:46:28,053 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 17.487934192021687
2024-02-24 15:46:28,059 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 24.646286884943645
2024-02-24 15:46:28,064 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.270042388641526
2024-02-24 15:46:28,069 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7506924774154086
2024-02-24 15:46:28,075 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8610248240157687
2024-02-24 15:46:28,081 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.5754005296470283
2024-02-24 15:46:28,086 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.011452732101573378
2024-02-24 15:46:28,092 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.7945247409088585
2024-02-24 15:46:28,098 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8088627489510497
2024-02-24 15:46:28,104 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 17.00775102143798
2024-02-24 15:46:28,109 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 24.103260193363766
2024-02-24 15:46:49,397 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/esm2/0224-141348/checkpoints/checkpoint-epoch1.pth ...
2024-02-24 15:47:14,435 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/esm2/0224-141348/checkpoints/model_best.pth
2024-02-24 15:47:15,922 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [0/10308 (0%)] Loss: 7.144250
2024-02-24 15:51:05,046 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [900/10308 (9%)] Loss: 7.458909
2024-02-24 15:55:06,825 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [1800/10308 (17%)] Loss: 7.462125
2024-02-24 15:58:57,617 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [2700/10308 (26%)] Loss: 6.761779
2024-02-24 16:03:00,668 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [3600/10308 (35%)] Loss: 7.078034
2024-02-24 16:07:13,175 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [4500/10308 (44%)] Loss: 7.384952
2024-02-24 16:11:03,697 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [5400/10308 (52%)] Loss: 8.514411
2024-02-24 16:15:00,864 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [6300/10308 (61%)] Loss: 7.077474
2024-02-24 16:18:55,516 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [7200/10308 (70%)] Loss: 7.690649
2024-02-24 16:22:55,657 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [8100/10308 (79%)] Loss: 7.465357
2024-02-24 16:26:39,359 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [9000/10308 (87%)] Loss: 6.774988
2024-02-24 16:30:30,334 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 2 [9900/10308 (96%)] Loss: 7.412054
2024-02-24 16:33:14,751 - PROT.PROT.base.base_trainer - INFO - epoch          : 2
2024-02-24 16:33:14,755 - PROT.PROT.base.base_trainer - INFO - loss           : 7.244757600647719
2024-02-24 16:33:14,762 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7478476613759995
2024-02-24 16:33:14,770 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.860484262307485
2024-02-24 16:33:14,778 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.5856843578318754
2024-02-24 16:33:14,785 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.012166124734600695
2024-02-24 16:33:14,792 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.7671482612689337
2024-02-24 16:33:14,798 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.7921172678470612
2024-02-24 16:33:14,804 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 16.936747074127197
2024-02-24 16:33:14,809 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 24.46628014246623
2024-02-24 16:33:14,815 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.200635289793965
2024-02-24 16:33:14,820 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7548352973707487
2024-02-24 16:33:14,825 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8653614339573357
2024-02-24 16:33:14,830 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6061106648320174
2024-02-24 16:33:14,835 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.010818268666026695
2024-02-24 16:33:14,842 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.7995725189407813
2024-02-24 16:33:14,849 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8138659071878314
2024-02-24 16:33:14,855 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.91845928171024
2024-02-24 16:33:14,861 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.815132283636565
2024-02-24 16:33:37,455 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/esm2/0224-141348/checkpoints/checkpoint-epoch2.pth ...
2024-02-24 16:34:09,464 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/esm2/0224-141348/checkpoints/model_best.pth
2024-02-24 16:34:11,582 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [0/10308 (0%)] Loss: 7.025341
2024-02-24 16:38:09,159 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [900/10308 (9%)] Loss: 6.637883
2024-02-24 16:42:00,610 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [1800/10308 (17%)] Loss: 7.033560
2024-02-24 16:46:05,779 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [2700/10308 (26%)] Loss: 7.022506
2024-02-24 16:49:52,365 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [3600/10308 (35%)] Loss: 7.366221
2024-02-24 16:53:49,066 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [4500/10308 (44%)] Loss: 6.463048
2024-02-24 16:57:40,600 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [5400/10308 (52%)] Loss: 7.070384
2024-02-24 17:01:28,849 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [6300/10308 (61%)] Loss: 6.610994
2024-02-24 17:05:28,135 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [7200/10308 (70%)] Loss: 6.830008
2024-02-24 17:09:27,499 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [8100/10308 (79%)] Loss: 6.965880
2024-02-24 17:13:13,620 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [9000/10308 (87%)] Loss: 6.455054
2024-02-24 17:17:10,914 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 3 [9900/10308 (96%)] Loss: 6.641123
2024-02-24 17:19:59,007 - PROT.PROT.base.base_trainer - INFO - epoch          : 3
2024-02-24 17:19:59,011 - PROT.PROT.base.base_trainer - INFO - loss           : 7.207758497733781
2024-02-24 17:19:59,019 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7731903791427612
2024-02-24 17:19:59,027 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8852841456731161
2024-02-24 17:19:59,036 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.6116568769017855
2024-02-24 17:19:59,043 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.013754620197384307
2024-02-24 17:19:59,050 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.806306799252828
2024-02-24 17:19:59,055 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8219922532637914
2024-02-24 17:19:59,061 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 15.505714416503906
2024-02-24 17:19:59,068 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 21.2827787399292
2024-02-24 17:19:59,073 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.198224094960962
2024-02-24 17:19:59,079 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7547388409996385
2024-02-24 17:19:59,084 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8652536225714806
2024-02-24 17:19:59,090 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.5960232053687486
2024-02-24 17:19:59,095 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.013743345741684312
2024-02-24 17:19:59,101 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8024158129173011
2024-02-24 17:19:59,106 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8164909194976201
2024-02-24 17:19:59,112 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.808258867791658
2024-02-24 17:19:59,117 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.710006787768148
2024-02-24 17:20:27,585 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/esm2/0224-141348/checkpoints/checkpoint-epoch3.pth ...
2024-02-24 17:20:55,137 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/esm2/0224-141348/checkpoints/model_best.pth
2024-02-24 17:20:56,875 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [0/10308 (0%)] Loss: 7.121641
2024-02-24 17:24:57,836 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [900/10308 (9%)] Loss: 7.027973
2024-02-24 17:28:53,877 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [1800/10308 (17%)] Loss: 7.830867
2024-02-24 17:33:03,146 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [2700/10308 (26%)] Loss: 6.679676
2024-02-24 17:37:01,549 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [3600/10308 (35%)] Loss: 6.359759
2024-02-24 17:40:51,863 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [4500/10308 (44%)] Loss: 7.020953
2024-02-24 17:44:39,521 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [5400/10308 (52%)] Loss: 7.544752
2024-02-24 17:48:32,879 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [6300/10308 (61%)] Loss: 6.857681
2024-02-24 17:52:29,552 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [7200/10308 (70%)] Loss: 7.484266
2024-02-24 17:56:26,850 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [8100/10308 (79%)] Loss: 8.513001
2024-02-24 18:00:13,835 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [9000/10308 (87%)] Loss: 6.773790
2024-02-24 18:04:08,665 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 4 [9900/10308 (96%)] Loss: 6.484339
2024-02-24 18:06:53,873 - PROT.PROT.base.base_trainer - INFO - epoch          : 4
2024-02-24 18:06:53,877 - PROT.PROT.base.base_trainer - INFO - loss           : 7.188483010998472
2024-02-24 18:06:53,884 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7575046320756277
2024-02-24 18:06:53,895 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8690759340922037
2024-02-24 18:06:53,903 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.6655804614226023
2024-02-24 18:06:53,911 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.008689770610847821
2024-02-24 18:06:53,917 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.7943035066127777
2024-02-24 18:06:53,923 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8090716203053793
2024-02-24 18:06:53,928 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 17.182021935780842
2024-02-24 18:06:53,934 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 24.213478724161785
2024-02-24 18:06:53,939 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.1926325097735075
2024-02-24 18:06:53,945 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7542142843848225
2024-02-24 18:06:53,950 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8654321315543678
2024-02-24 18:06:53,955 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6056819941717067
2024-02-24 18:06:53,961 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.012887319937718661
2024-02-24 18:06:53,966 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8032334060906484
2024-02-24 18:06:53,973 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8175104615653133
2024-02-24 18:06:53,978 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.800646776642747
2024-02-24 18:06:53,984 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.786061716255666
2024-02-24 18:07:20,247 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/esm2/0224-141348/checkpoints/checkpoint-epoch4.pth ...
2024-02-24 18:07:51,030 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/esm2/0224-141348/checkpoints/model_best.pth
2024-02-24 18:07:52,533 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [0/10308 (0%)] Loss: 7.046811
2024-02-24 18:11:49,920 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [900/10308 (9%)] Loss: 7.723793
2024-02-24 18:15:50,102 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [1800/10308 (17%)] Loss: 7.017856
2024-02-24 18:19:44,629 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [2700/10308 (26%)] Loss: 7.290160
2024-02-24 18:23:34,081 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [3600/10308 (35%)] Loss: 6.232493
2024-02-24 18:27:22,431 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [4500/10308 (44%)] Loss: 7.672451
2024-02-24 18:31:19,505 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [5400/10308 (52%)] Loss: 6.793922
2024-02-24 18:35:02,321 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [6300/10308 (61%)] Loss: 7.063569
2024-02-24 18:39:02,630 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [7200/10308 (70%)] Loss: 7.053759
2024-02-24 18:42:58,296 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [8100/10308 (79%)] Loss: 6.636302
2024-02-24 18:46:54,507 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [9000/10308 (87%)] Loss: 7.495145
2024-02-24 18:50:53,341 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 5 [9900/10308 (96%)] Loss: 6.991561
2024-02-24 18:53:43,698 - PROT.PROT.base.base_trainer - INFO - epoch          : 5
2024-02-24 18:53:43,736 - PROT.PROT.base.base_trainer - INFO - loss           : 7.170280085104126
2024-02-24 18:53:43,743 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7707534730434418
2024-02-24 18:53:43,751 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8755135436852773
2024-02-24 18:53:43,759 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.5683841925735275
2024-02-24 18:53:43,768 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.014448326410880933
2024-02-24 18:53:43,774 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8097355216741562
2024-02-24 18:53:43,780 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.822766641775767
2024-02-24 18:53:43,785 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 16.35551079114278
2024-02-24 18:53:43,789 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 22.28730058670044
2024-02-24 18:53:43,791 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.166887614119976
2024-02-24 18:53:43,793 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7565868780621743
2024-02-24 18:53:43,795 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8666020967203752
2024-02-24 18:53:43,797 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6224967052488213
2024-02-24 18:53:43,798 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.01695267093235565
2024-02-24 18:53:43,799 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8052038827505499
2024-02-24 18:53:43,800 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.818885105892301
2024-02-24 18:53:43,802 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.73500549309368
2024-02-24 18:53:43,803 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.60705026401365
2024-02-24 18:54:08,554 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/esm2/0224-141348/checkpoints/checkpoint-epoch5.pth ...
2024-02-24 18:54:35,015 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/esm2/0224-141348/checkpoints/model_best.pth
2024-02-24 18:54:36,723 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [0/10308 (0%)] Loss: 6.779857
2024-02-24 18:58:40,010 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [900/10308 (9%)] Loss: 8.511395
2024-02-24 19:02:33,218 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [1800/10308 (17%)] Loss: 7.865260
2024-02-24 19:06:23,372 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [2700/10308 (26%)] Loss: 6.842913
2024-02-24 19:10:23,880 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [3600/10308 (35%)] Loss: 6.816368
2024-02-24 19:14:19,494 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [4500/10308 (44%)] Loss: 7.327765
2024-02-24 19:18:13,210 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [5400/10308 (52%)] Loss: 7.248468
2024-02-24 19:22:06,817 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [6300/10308 (61%)] Loss: 6.660727
2024-02-24 19:25:57,488 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [7200/10308 (70%)] Loss: 6.693526
2024-02-24 19:29:56,803 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [8100/10308 (79%)] Loss: 7.627052
2024-02-24 19:33:47,435 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [9000/10308 (87%)] Loss: 6.765369
2024-02-24 19:37:36,433 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 6 [9900/10308 (96%)] Loss: 7.415771
2024-02-24 19:40:26,173 - PROT.PROT.base.base_trainer - INFO - epoch          : 6
2024-02-24 19:40:26,177 - PROT.PROT.base.base_trainer - INFO - loss           : 7.166595537501031
2024-02-24 19:40:26,183 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7727368275324503
2024-02-24 19:40:26,192 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8681228955586752
2024-02-24 19:40:26,200 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.4986113924533129
2024-02-24 19:40:26,208 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.01727153950681289
2024-02-24 19:40:26,213 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8092453529437383
2024-02-24 19:40:26,219 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.821285143494606
2024-02-24 19:40:26,224 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 16.62682302792867
2024-02-24 19:40:26,231 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 22.88830312093099
2024-02-24 19:40:26,237 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.168245682417247
2024-02-24 19:40:26,242 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7573369077210936
2024-02-24 19:40:26,247 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.86736468239464
2024-02-24 19:40:26,251 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6020870471649847
2024-02-24 19:40:26,257 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.012666933487553485
2024-02-24 19:40:26,261 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8025154858497676
2024-02-24 19:40:26,267 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8173423370971891
2024-02-24 19:40:26,272 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.652820518535883
2024-02-24 19:40:26,277 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.470732442567268
2024-02-24 19:40:49,490 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/esm2/0224-141348/checkpoints/checkpoint-epoch6.pth ...
2024-02-24 19:40:52,349 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [0/10308 (0%)] Loss: 6.714916
2024-02-24 19:44:52,426 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [900/10308 (9%)] Loss: 7.457338
2024-02-24 19:48:47,032 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [1800/10308 (17%)] Loss: 6.258616
2024-02-24 19:52:31,665 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [2700/10308 (26%)] Loss: 6.444286
2024-02-24 19:56:25,655 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [3600/10308 (35%)] Loss: 6.253822
2024-02-24 20:00:24,815 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [4500/10308 (44%)] Loss: 6.819139
2024-02-24 20:04:13,253 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [5400/10308 (52%)] Loss: 6.316023
2024-02-24 20:07:59,465 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [6300/10308 (61%)] Loss: 7.380191
2024-02-24 20:11:57,118 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [7200/10308 (70%)] Loss: 7.195103
2024-02-24 20:16:03,016 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [8100/10308 (79%)] Loss: 6.950284
2024-02-24 20:20:04,965 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [9000/10308 (87%)] Loss: 7.763778
2024-02-24 20:24:03,925 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 7 [9900/10308 (96%)] Loss: 7.184973
2024-02-24 20:26:51,533 - PROT.PROT.base.base_trainer - INFO - epoch          : 7
2024-02-24 20:26:51,537 - PROT.PROT.base.base_trainer - INFO - loss           : 7.155390981884806
2024-02-24 20:26:51,543 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7701477209726969
2024-02-24 20:26:51,549 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8811593453089396
2024-02-24 20:26:51,557 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.5961206052452326
2024-02-24 20:26:51,565 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.012532445951364934
2024-02-24 20:26:51,570 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8229891111453375
2024-02-24 20:26:51,575 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8314000219106674
2024-02-24 20:26:51,581 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 16.332430919011433
2024-02-24 20:26:51,586 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 22.880348682403564
2024-02-24 20:26:51,592 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.1663735783847935
2024-02-24 20:26:51,597 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7576082021107973
2024-02-24 20:26:51,603 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8678038748208007
2024-02-24 20:26:51,608 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.5993550358584144
2024-02-24 20:26:51,613 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.0126075001212904
2024-02-24 20:26:51,619 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8050325925279808
2024-02-24 20:26:51,624 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8187433778139938
2024-02-24 20:26:51,630 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.802294428498104
2024-02-24 20:26:51,635 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.61697809458659
2024-02-24 20:27:12,675 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/esm2/0224-141348/checkpoints/checkpoint-epoch7.pth ...
2024-02-24 20:27:44,855 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/esm2/0224-141348/checkpoints/model_best.pth
2024-02-24 20:27:46,565 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [0/10308 (0%)] Loss: 6.475063
2024-02-24 20:31:48,620 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [900/10308 (9%)] Loss: 6.451890
2024-02-24 20:35:50,172 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [1800/10308 (17%)] Loss: 7.270342
2024-02-24 20:39:51,384 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [2700/10308 (26%)] Loss: 7.644993
2024-02-24 20:43:40,727 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [3600/10308 (35%)] Loss: 7.085643
2024-02-24 20:47:32,286 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [4500/10308 (44%)] Loss: 6.287829
2024-02-24 20:51:21,230 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [5400/10308 (52%)] Loss: 6.986277
2024-02-24 20:55:12,401 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [6300/10308 (61%)] Loss: 7.421759
2024-02-24 20:58:57,649 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [7200/10308 (70%)] Loss: 6.749977
2024-02-24 21:02:53,094 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [8100/10308 (79%)] Loss: 7.323147
2024-02-24 21:06:50,952 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [9000/10308 (87%)] Loss: 7.741516
2024-02-24 21:10:46,179 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 8 [9900/10308 (96%)] Loss: 6.570879
2024-02-24 21:13:43,831 - PROT.PROT.base.base_trainer - INFO - epoch          : 8
2024-02-24 21:13:43,836 - PROT.PROT.base.base_trainer - INFO - loss           : 7.151250331766879
2024-02-24 21:13:43,842 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7637484868367513
2024-02-24 21:13:43,851 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.866581529378891
2024-02-24 21:13:43,859 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.6605889946222305
2024-02-24 21:13:43,866 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.010267245466820896
2024-02-24 21:13:43,873 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8093244334061941
2024-02-24 21:13:43,879 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8229485402504603
2024-02-24 21:13:43,885 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 17.132532755533855
2024-02-24 21:13:43,892 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 23.139724254608154
2024-02-24 21:13:43,904 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.16272192687566
2024-02-24 21:13:43,909 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.757842151551229
2024-02-24 21:13:43,916 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8671168818025131
2024-02-24 21:13:43,923 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6016123233280554
2024-02-24 21:13:43,929 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.011684708304555461
2024-02-24 21:13:43,938 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.803535521360341
2024-02-24 21:13:43,948 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8173374951545602
2024-02-24 21:13:43,958 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.657769819027383
2024-02-24 21:13:43,969 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.449405360485795
2024-02-24 21:14:09,028 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/esm2/0224-141348/checkpoints/checkpoint-epoch8.pth ...
2024-02-24 21:14:37,955 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/esm2/0224-141348/checkpoints/model_best.pth
2024-02-24 21:14:39,546 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [0/10308 (0%)] Loss: 5.986254
2024-02-24 21:18:31,196 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [900/10308 (9%)] Loss: 7.612273
2024-02-24 21:22:33,886 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [1800/10308 (17%)] Loss: 6.972088
2024-02-24 21:26:28,051 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [2700/10308 (26%)] Loss: 7.654994
2024-02-24 21:30:16,918 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [3600/10308 (35%)] Loss: 7.782744
2024-02-24 21:34:15,368 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [4500/10308 (44%)] Loss: 6.575539
2024-02-24 21:38:22,609 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [5400/10308 (52%)] Loss: 8.351490
2024-02-24 21:42:30,366 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [6300/10308 (61%)] Loss: 7.271519
2024-02-24 21:46:20,485 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [7200/10308 (70%)] Loss: 7.384973
2024-02-24 21:50:16,427 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [8100/10308 (79%)] Loss: 6.440675
2024-02-24 21:54:15,984 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [9000/10308 (87%)] Loss: 6.692022
2024-02-24 21:58:19,934 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 9 [9900/10308 (96%)] Loss: 6.698033
2024-02-24 22:01:12,350 - PROT.PROT.base.base_trainer - INFO - epoch          : 9
2024-02-24 22:01:12,351 - PROT.PROT.base.base_trainer - INFO - loss           : 7.143187146313602
2024-02-24 22:01:12,358 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.752467542886734
2024-02-24 22:01:12,366 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8596705347299576
2024-02-24 22:01:12,373 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.6106641764442126
2024-02-24 22:01:12,381 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.008023400771586845
2024-02-24 22:01:12,387 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.7998935580253601
2024-02-24 22:01:12,393 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8125814745823542
2024-02-24 22:01:12,399 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 16.72653341293335
2024-02-24 22:01:12,405 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 23.75853657722473
2024-02-24 22:01:12,410 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.132909847801462
2024-02-24 22:01:12,415 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7592881980637343
2024-02-24 22:01:12,421 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8686055715893467
2024-02-24 22:01:12,427 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6113151887541712
2024-02-24 22:01:12,432 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.013897648503350067
2024-02-24 22:01:12,438 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8034342138639675
2024-02-24 22:01:12,443 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8184070935768395
2024-02-24 22:01:12,449 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.603196620941162
2024-02-24 22:01:12,454 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.365145180058214
2024-02-24 22:01:37,587 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/esm2/0224-141348/checkpoints/checkpoint-epoch9.pth ...
2024-02-24 22:02:03,462 - PROT.PROT.base.base_trainer - INFO - Saving current best: saved/esm2/0224-141348/checkpoints/model_best.pth
2024-02-24 22:02:04,914 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [0/10308 (0%)] Loss: 7.339490
2024-02-24 22:06:03,984 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [900/10308 (9%)] Loss: 6.110760
2024-02-24 22:09:53,989 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [1800/10308 (17%)] Loss: 6.527623
2024-02-24 22:13:49,090 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [2700/10308 (26%)] Loss: 7.102221
2024-02-24 22:17:46,231 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [3600/10308 (35%)] Loss: 7.425904
2024-02-24 22:21:49,121 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [4500/10308 (44%)] Loss: 7.797857
2024-02-24 22:25:36,581 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [5400/10308 (52%)] Loss: 6.804088
2024-02-24 22:29:38,968 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [6300/10308 (61%)] Loss: 7.065597
2024-02-24 22:33:32,978 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [7200/10308 (70%)] Loss: 5.939116
2024-02-24 22:37:24,933 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [8100/10308 (79%)] Loss: 6.898686
2024-02-24 22:41:19,003 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [9000/10308 (87%)] Loss: 7.023707
2024-02-24 22:45:10,432 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 10 [9900/10308 (96%)] Loss: 6.662947
2024-02-24 22:48:02,726 - PROT.PROT.base.base_trainer - INFO - epoch          : 10
2024-02-24 22:48:02,730 - PROT.PROT.base.base_trainer - INFO - loss           : 7.148285734336538
2024-02-24 22:48:02,739 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7693252811829249
2024-02-24 22:48:02,748 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8809538086255392
2024-02-24 22:48:02,757 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.5933395822842916
2024-02-24 22:48:02,766 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.011765886602612833
2024-02-24 22:48:02,772 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8221724728743235
2024-02-24 22:48:02,777 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8304256498813629
2024-02-24 22:48:02,782 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 15.997829834620157
2024-02-24 22:48:02,788 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 22.09773127237956
2024-02-24 22:48:02,793 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.163923816927245
2024-02-24 22:48:02,799 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7579625711889725
2024-02-24 22:48:02,805 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8679815900281787
2024-02-24 22:48:02,809 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.5933481923240567
2024-02-24 22:48:02,816 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.011007749980721087
2024-02-24 22:48:02,821 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8038974484614341
2024-02-24 22:48:02,825 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8184594858396537
2024-02-24 22:48:02,832 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.71443820615536
2024-02-24 22:48:02,837 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.406858326324237
2024-02-24 22:48:26,194 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/esm2/0224-141348/checkpoints/checkpoint-epoch10.pth ...
2024-02-24 22:48:28,382 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [0/10308 (0%)] Loss: 6.382936
2024-02-24 22:52:22,499 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [900/10308 (9%)] Loss: 7.281025
2024-02-24 22:56:15,507 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [1800/10308 (17%)] Loss: 6.713120
2024-02-24 23:00:13,509 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [2700/10308 (26%)] Loss: 7.639332
2024-02-24 23:04:10,191 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [3600/10308 (35%)] Loss: 7.331603
2024-02-24 23:08:03,902 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [4500/10308 (44%)] Loss: 9.699711
2024-02-24 23:12:01,909 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [5400/10308 (52%)] Loss: 7.144632
2024-02-24 23:16:03,514 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [6300/10308 (61%)] Loss: 7.226829
2024-02-24 23:20:04,900 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [7200/10308 (70%)] Loss: 7.562508
2024-02-24 23:24:02,448 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [8100/10308 (79%)] Loss: 7.659032
2024-02-24 23:27:51,237 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [9000/10308 (87%)] Loss: 7.637859
2024-02-24 23:31:38,305 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 11 [9900/10308 (96%)] Loss: 7.015586
2024-02-24 23:34:32,377 - PROT.PROT.base.base_trainer - INFO - epoch          : 11
2024-02-24 23:34:32,380 - PROT.PROT.base.base_trainer - INFO - loss           : 7.142890603292944
2024-02-24 23:34:32,387 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7371686299641927
2024-02-24 23:34:32,395 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8575151364008585
2024-02-24 23:34:32,402 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.586901975174745
2024-02-24 23:34:32,409 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.0121361188745747
2024-02-24 23:34:32,416 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.7998228271802267
2024-02-24 23:34:32,424 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8148300796747208
2024-02-24 23:34:32,431 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 18.007967233657837
2024-02-24 23:34:32,437 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 25.489217201868694
2024-02-24 23:34:32,445 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.146126583493504
2024-02-24 23:34:32,451 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7565511770793872
2024-02-24 23:34:32,456 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8665703758322445
2024-02-24 23:34:32,462 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6101522549731114
2024-02-24 23:34:32,469 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.010507282032244719
2024-02-24 23:34:32,474 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8058205919951971
2024-02-24 23:34:32,480 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8198402911094722
2024-02-24 23:34:32,485 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.66631536202237
2024-02-24 23:34:32,491 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.582556884667092
2024-02-24 23:34:56,472 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/esm2/0224-141348/checkpoints/checkpoint-epoch11.pth ...
2024-02-24 23:34:58,930 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [0/10308 (0%)] Loss: 6.744005
2024-02-24 23:38:40,761 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [900/10308 (9%)] Loss: 6.263574
2024-02-24 23:42:44,829 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [1800/10308 (17%)] Loss: 6.800450
2024-02-24 23:46:40,147 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [2700/10308 (26%)] Loss: 6.893493
2024-02-24 23:50:37,443 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [3600/10308 (35%)] Loss: 6.529437
2024-02-24 23:54:33,393 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [4500/10308 (44%)] Loss: 6.482057
2024-02-24 23:58:21,746 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [5400/10308 (52%)] Loss: 7.052196
2024-02-25 00:02:19,215 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [6300/10308 (61%)] Loss: 7.622387
2024-02-25 00:06:12,754 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [7200/10308 (70%)] Loss: 6.563409
2024-02-25 00:10:07,433 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [8100/10308 (79%)] Loss: 6.626728
2024-02-25 00:14:02,007 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [9000/10308 (87%)] Loss: 7.649254
2024-02-25 00:17:52,876 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 12 [9900/10308 (96%)] Loss: 8.755409
2024-02-25 00:20:41,808 - PROT.PROT.base.base_trainer - INFO - epoch          : 12
2024-02-25 00:20:41,812 - PROT.PROT.base.base_trainer - INFO - loss           : 7.138028837143276
2024-02-25 00:20:41,814 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7561140209436417
2024-02-25 00:20:41,817 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8700909266869227
2024-02-25 00:20:41,820 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.598797446427246
2024-02-25 00:20:41,825 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.011842868494568393
2024-02-25 00:20:41,828 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8009106814861298
2024-02-25 00:20:41,831 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8164268235365549
2024-02-25 00:20:41,834 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 16.27207310994466
2024-02-25 00:20:41,836 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 22.861183643341064
2024-02-25 00:20:41,838 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.154700841411014
2024-02-25 00:20:41,841 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7580896358648349
2024-02-25 00:20:41,845 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8677797657317341
2024-02-25 00:20:41,848 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6014139240812097
2024-02-25 00:20:41,851 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.012445716419436786
2024-02-25 00:20:41,853 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8064289162299729
2024-02-25 00:20:41,856 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.819934331183064
2024-02-25 00:20:41,860 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.687592291743993
2024-02-25 00:20:41,864 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.587042588589377
2024-02-25 00:21:00,562 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/esm2/0224-141348/checkpoints/checkpoint-epoch12.pth ...
2024-02-25 00:21:02,476 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [0/10308 (0%)] Loss: 7.292678
2024-02-25 00:24:54,038 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [900/10308 (9%)] Loss: 6.754226
2024-02-25 00:28:44,247 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [1800/10308 (17%)] Loss: 7.088209
2024-02-25 00:32:33,690 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [2700/10308 (26%)] Loss: 7.307934
2024-02-25 00:36:23,862 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [3600/10308 (35%)] Loss: 7.127949
2024-02-25 00:40:22,702 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [4500/10308 (44%)] Loss: 7.538525
2024-02-25 00:44:13,989 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [5400/10308 (52%)] Loss: 6.840621
2024-02-25 00:48:16,918 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [6300/10308 (61%)] Loss: 6.360968
2024-02-25 00:52:14,781 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [7200/10308 (70%)] Loss: 8.102211
2024-02-25 00:56:10,994 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [8100/10308 (79%)] Loss: 6.437016
2024-02-25 01:00:05,679 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [9000/10308 (87%)] Loss: 7.388878
2024-02-25 01:04:04,367 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 13 [9900/10308 (96%)] Loss: 6.299106
2024-02-25 01:06:54,188 - PROT.PROT.base.base_trainer - INFO - epoch          : 13
2024-02-25 01:06:54,189 - PROT.PROT.base.base_trainer - INFO - loss           : 7.138790554336823
2024-02-25 01:06:54,191 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7692925383647283
2024-02-25 01:06:54,192 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8739237934350967
2024-02-25 01:06:54,194 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.5258155291279157
2024-02-25 01:06:54,196 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.011309205195478475
2024-02-25 01:06:54,197 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.7897026439507803
2024-02-25 01:06:54,198 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8090554773807526
2024-02-25 01:06:54,199 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 16.806174357732136
2024-02-25 01:06:54,200 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 23.106478373209637
2024-02-25 01:06:54,202 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.150990204617546
2024-02-25 01:06:54,203 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7569827725526591
2024-02-25 01:06:54,204 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.867684928253568
2024-02-25 01:06:54,205 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.609043421205617
2024-02-25 01:06:54,206 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.015071747325703577
2024-02-25 01:06:54,207 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8064065516214969
2024-02-25 01:06:54,209 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8197826037107798
2024-02-25 01:06:54,210 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.835228115870063
2024-02-25 01:06:54,211 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.609820154760158
2024-02-25 01:07:12,043 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/esm2/0224-141348/checkpoints/checkpoint-epoch13.pth ...
2024-02-25 01:07:13,960 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [0/10308 (0%)] Loss: 8.272753
2024-02-25 01:11:04,854 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [900/10308 (9%)] Loss: 7.463419
2024-02-25 01:14:54,491 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [1800/10308 (17%)] Loss: 6.445384
2024-02-25 01:19:03,664 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [2700/10308 (26%)] Loss: 6.828660
2024-02-25 01:23:07,889 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [3600/10308 (35%)] Loss: 6.756047
2024-02-25 01:26:53,248 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [4500/10308 (44%)] Loss: 7.344326
2024-02-25 01:30:47,588 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [5400/10308 (52%)] Loss: 7.116775
2024-02-25 01:34:39,143 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [6300/10308 (61%)] Loss: 6.786030
2024-02-25 01:38:38,317 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [7200/10308 (70%)] Loss: 7.029696
2024-02-25 01:42:26,912 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [8100/10308 (79%)] Loss: 6.267576
2024-02-25 01:46:22,672 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [9000/10308 (87%)] Loss: 7.105925
2024-02-25 01:50:11,745 - PROT.PROT.trainer.trainer - INFO - Train Epoch: 14 [9900/10308 (96%)] Loss: 6.577394
2024-02-25 01:52:59,493 - PROT.PROT.base.base_trainer - INFO - epoch          : 14
2024-02-25 01:52:59,497 - PROT.PROT.base.base_trainer - INFO - loss           : 7.130496340725183
2024-02-25 01:52:59,503 - PROT.PROT.base.base_trainer - INFO - metric_ss8     : 0.7715437809626261
2024-02-25 01:52:59,511 - PROT.PROT.base.base_trainer - INFO - metric_ss3     : 0.8814374208450317
2024-02-25 01:52:59,518 - PROT.PROT.base.base_trainer - INFO - metric_dis_mcc : 0.6372526362538338
2024-02-25 01:52:59,526 - PROT.PROT.base.base_trainer - INFO - metric_dis_fnr : 0.011104571177080894
2024-02-25 01:52:59,533 - PROT.PROT.base.base_trainer - INFO - metric_rsa     : 0.8246941516796747
2024-02-25 01:52:59,539 - PROT.PROT.base.base_trainer - INFO - metric_asa     : 0.8339541306098303
2024-02-25 01:52:59,544 - PROT.PROT.base.base_trainer - INFO - metric_phi     : 16.2511146068573
2024-02-25 01:52:59,549 - PROT.PROT.base.base_trainer - INFO - metric_psi     : 21.829354921976726
2024-02-25 01:52:59,554 - PROT.PROT.base.base_trainer - INFO - val_loss       : 7.177780472484462
2024-02-25 01:52:59,559 - PROT.PROT.base.base_trainer - INFO - val_metric_ss8 : 0.7559843254705196
2024-02-25 01:52:59,564 - PROT.PROT.base.base_trainer - INFO - val_metric_ss3 : 0.8666058273992855
2024-02-25 01:52:59,570 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_mcc: 0.6117194192345792
2024-02-25 01:52:59,574 - PROT.PROT.base.base_trainer - INFO - val_metric_dis_fnr: 0.01480069584365697
2024-02-25 01:52:59,579 - PROT.PROT.base.base_trainer - INFO - val_metric_rsa : 0.8045626413558242
2024-02-25 01:52:59,582 - PROT.PROT.base.base_trainer - INFO - val_metric_asa : 0.8180055401641945
2024-02-25 01:52:59,584 - PROT.PROT.base.base_trainer - INFO - val_metric_phi : 16.757822346423385
2024-02-25 01:52:59,586 - PROT.PROT.base.base_trainer - INFO - val_metric_psi : 23.759063414542
2024-02-25 01:53:22,343 - PROT.PROT.base.base_trainer - INFO - Saving checkpoint: saved/esm2/0224-141348/checkpoints/checkpoint-epoch14.pth ...
2024-02-25 01:53:23,156 - PROT.PROT.main - INFO - Initialising evaluation
2024-02-25 01:53:24,425 - PROT.PROT.base.base_eval - INFO - Starting evaluating...
2024-02-25 01:53:28,787 - PROT.PROT.base.base_eval - INFO - metric_ss8: 0.6664552944047111
2024-02-25 01:53:28,788 - PROT.PROT.base.base_eval - INFO - metric_ss3: 0.7774924550737653
2024-02-25 01:53:28,794 - PROT.PROT.base.base_eval - INFO - metric_dis_mcc: 0.5593646381582532
2024-02-25 01:53:28,800 - PROT.PROT.base.base_eval - INFO - metric_dis_fnr: 0.021287707412349328
2024-02-25 01:53:28,805 - PROT.PROT.base.base_eval - INFO - metric_rsa: 0.7072873967034476
2024-02-25 01:53:28,810 - PROT.PROT.base.base_eval - INFO - metric_asa: 0.7236969470977783
2024-02-25 01:53:28,816 - PROT.PROT.base.base_eval - INFO - metric_phi: 20.60733331952776
2024-02-25 01:53:28,821 - PROT.PROT.base.base_eval - INFO - metric_psi: 32.501014709472656
2024-02-25 01:53:30,216 - PROT.PROT.base.base_eval - INFO - Starting evaluating...
2024-02-25 01:54:27,619 - PROT.PROT.base.base_eval - INFO - metric_ss8: 0.7240939499341954
2024-02-25 01:54:27,622 - PROT.PROT.base.base_eval - INFO - metric_ss3: 0.8586717837038096
2024-02-25 01:54:27,628 - PROT.PROT.base.base_eval - INFO - metric_dis_mcc: 0.060304974070425295
2024-02-25 01:54:27,634 - PROT.PROT.base.base_eval - INFO - metric_dis_fnr: 0.012193897804408743
2024-02-25 01:54:27,640 - PROT.PROT.base.base_eval - INFO - metric_rsa: 0.8032443044129868
2024-02-25 01:54:27,646 - PROT.PROT.base.base_eval - INFO - metric_asa: 0.817485297283931
2024-02-25 01:54:27,651 - PROT.PROT.base.base_eval - INFO - metric_phi: 19.341488944159615
2024-02-25 01:54:27,658 - PROT.PROT.base.base_eval - INFO - metric_psi: 26.604242118478517
2024-02-25 01:54:28,995 - PROT.PROT.base.base_eval - INFO - Starting evaluating...
2024-02-25 01:54:43,531 - PROT.PROT.base.base_eval - INFO - metric_ss8: 0.752636846770411
2024-02-25 01:54:43,532 - PROT.PROT.base.base_eval - INFO - metric_ss3: 0.8579935172329778
2024-02-25 01:54:43,537 - PROT.PROT.base.base_eval - INFO - metric_dis_mcc: 0.6459913005485483
2024-02-25 01:54:43,543 - PROT.PROT.base.base_eval - INFO - metric_dis_fnr: 0.015862754439813612
2024-02-25 01:54:43,548 - PROT.PROT.base.base_eval - INFO - metric_rsa: 0.7853043094925258
2024-02-25 01:54:43,554 - PROT.PROT.base.base_eval - INFO - metric_asa: 0.8052197803621707
2024-02-25 01:54:43,560 - PROT.PROT.base.base_eval - INFO - metric_phi: 16.75584180251412
2024-02-25 01:54:43,565 - PROT.PROT.base.base_eval - INFO - metric_psi: 24.418053328472634
2024-02-25 01:54:43,589 - PROT.PROT.main - INFO - Finished!
done
