name: ESM2_aggregation
save_dir: saved/
seed: 1234
target_devices: [0]

arch:
  type: ESM2_aggregation
  args:
    init_n_channels: 1280
    embedding_pretrained: "pretrained/esm2_t33_650M_UR50D.pt"
    finetuning: 'LoRA'
    gradient_checkpointing: 2

data_loader:
  type: NSPDataLoader
  args:
    train_path: [/home/deag/multitask/data/aggregation/Train_agg.npz]
    test_path: [/home/deag/multitask/data/aggregation/CASP12_HHblits_extended.npz,
    /home/deag/multitask/data/aggregation/CB513_HHblits_extended.npz,
    /home/deag/multitask/data/aggregation/TS115_HHblits_extended.npz]
    dataset_loader: NSPDataOnlyEncoding
    batch_size: 5
    nworkers: 2
    shuffle: true
    validation_split: 0.05

loss: loss_aggregation

n_outputs: 1

augmentation:
  type: sparse_token
  args: {}

metrics:
  metric_aggregation: 0

optimizer:
  type: Adam
  args:
    lr: 0.00005
    weight_decay: 0.005

lr_scheduler:
  type: null

training:
  early_stop: 50
  epochs: 50
  gradient_accumulation: 6
  monitor: min val_loss
  save_period: 1
  tensorboard: true
