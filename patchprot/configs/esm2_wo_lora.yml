name: esm2_wo_lora
save_dir: saved/
seed: 1234
target_devices: [0]

arch:
  type: ESM2_multitask
  args:
    init_n_channels: 1280
    embedding_pretrained: "/scistor/informatica/dgi460/patchprot/PROT/pretrained/esm2_t33_650M_UR50D.pt"
    finetuning: 'no finetuning'
   

data_loader:
  type: NSPDataLoader
  args:
    train_path: [/scistor/informatica/dgi460/patchprot/data/Train_HHblits.npz]
    test_path: [/scistor/informatica/dgi460/patchprot/data/CASP12_HHblits.npz,
    /scistor/informatica/dgi460/patchprot/data/CB513_HHblits.npz,
    /scistor/informatica/dgi460/patchprot/data/TS115_HHblits.npz]
    dataset_loader: NSPDataOnlyEncoding
    batch_size: 18
    nworkers: 2
    shuffle: true
    validation_split: 0.05

loss: multi_task_loss_ss

n_outputs: 8

augmentation:
  type: sparse_token
  args: {}

metrics:
  metric_ss8: 0
  metric_ss3: 1
  metric_dis_mcc: 2
  metric_dis_fnr: 2
  metric_rsa: 3
  metric_asa: 3
  metric_phi: 4
  metric_psi: 5

optimizer:
  type: Adam
  args:
    lr: 0.00005
    weight_decay: 0.005

lr_scheduler:
  type: null

training:
  early_stop: 15
  epochs: 15
  monitor: min val_loss
  save_period: 1
  tensorboard: true
