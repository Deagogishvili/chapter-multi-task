name: patches
save_dir: saved/
seed: 1234
target_devices: [0]

arch:
  type: ESM2_extended_multitask
  args:
    init_n_channels: 1280
    embedding_pretrained: "/scistor/informatica/dgi460/patchprot/PROT/pretrained/esm2_t33_650M_UR50D.pt"
    finetuning: 'LoRA'
    gradient_checkpointing: 2

data_loader:
  type: NSPDataLoader
  args:
    train_path: [/scistor/informatica/dgi460/patchprot/data/extended/Train_LHP.npz]
    test_path: [/scistor/informatica/dgi460/patchprot/data/extended/CASP12_HHblits_extended.npz,
    /scistor/informatica/dgi460/patchprot/data/extended/CB513_HHblits_extended.npz,
    /scistor/informatica/dgi460/patchprot/data/extended/TS115_HHblits_extended.npz]
    dataset_loader: NSPDataOnlyEncoding
    batch_size: 3
    nworkers: 2
    shuffle: true
    validation_split: 0.05

loss: loss_patches

n_outputs: 3

augmentation:
  type: sparse_token
  args: {}

metrics:
  metric_lhp: 8
  metric_hp_loc_mcc: 9
  metric_hp_loc_fnr: 9
  metric_lhp_loc_mcc: 10
  metric_lhp_loc_fnr: 10

optimizer:
  type: Adam
  args:
    lr: 0.00005
    weight_decay: 0.005

lr_scheduler:
  type: null

training:
  early_stop: 15
  epochs: 15
  gradient_accumulation: 6
  monitor: min val_loss
  save_period: 1
  tensorboard: true
